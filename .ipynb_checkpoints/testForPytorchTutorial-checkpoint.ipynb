{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "######################################################################\n",
    "# Loading data files\n",
    "# ==================\n",
    "#\n",
    "# The data for this project is a set of many thousands of English to\n",
    "# French translation pairs.\n",
    "#\n",
    "# `This question on Open Data Stack\n",
    "# Exchange <http://opendata.stackexchange.com/questions/3888/dataset-of-sentences-translated-into-many-languages>`__\n",
    "# pointed me to the open translation site http://tatoeba.org/ which has\n",
    "# downloads available at http://tatoeba.org/eng/downloads - and better\n",
    "# yet, someone did the extra work of splitting language pairs into\n",
    "# individual text files here: http://www.manythings.org/anki/\n",
    "#\n",
    "# The English to French pairs are too big to include in the repo, so\n",
    "# download to ``data/eng-fra.txt`` before continuing. The file is a tab\n",
    "# separated list of translation pairs:\n",
    "#\n",
    "# ::\n",
    "#\n",
    "#     I am cold.    Je suis froid.\n",
    "#\n",
    "# .. Note::\n",
    "#    Download the data from\n",
    "#    `here <https://download.pytorch.org/tutorial/data.zip>`_\n",
    "#    and extract it to the current directory.\n",
    "\n",
    "######################################################################\n",
    "# Similar to the character encoding used in the character-level RNN\n",
    "# tutorials, we will be representing each word in a language as a one-hot\n",
    "# vector, or giant vector of zeros except for a single one (at the index\n",
    "# of the word). Compared to the dozens of characters that might exist in a\n",
    "# language, there are many many more words, so the encoding vector is much\n",
    "# larger. We will however cheat a bit and trim the data to only use a few\n",
    "# thousand words per language.\n",
    "#\n",
    "# .. figure:: /_static/img/seq-seq-images/word-encoding.png\n",
    "#    :alt:\n",
    "#\n",
    "#\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# We'll need a unique index per word to use as the inputs and targets of\n",
    "# the networks later. To keep track of all this we will use a helper class\n",
    "# called ``Lang`` which has word → index (``word2index``) and index → word\n",
    "# (``index2word``) dictionaries, as well as a count of each word\n",
    "# ``word2count`` to use to later replace rare words.\n",
    "#\n",
    "\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "#这个类通过不断输入sentence，构建词与下标的对应（词典），方便制作one-hot。\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# The files are all in Unicode, to simplify we will turn Unicode\n",
    "# characters to ASCII, make everything lowercase, and trim most\n",
    "# punctuation.\n",
    "#\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# To read the data file we will split the file into lines, and then split\n",
    "# lines into pairs. The files are all English → Other Language, so if we\n",
    "# want to translate from Other Language → English I added the ``reverse``\n",
    "# flag to reverse the pairs.\n",
    "#\n",
    "\n",
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# Since there are a *lot* of example sentences and we want to train\n",
    "# something quickly, we'll trim the data set to only relatively short and\n",
    "# simple sentences. Here the maximum length is 10 words (that includes\n",
    "# ending punctuation) and we're filtering to sentences that translate to\n",
    "# the form \"I am\" or \"He is\" etc. (accounting for apostrophes replaced\n",
    "# earlier).\n",
    "#\n",
    "\n",
    "MAX_LENGTH = 10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s\",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "        p[1].startswith(eng_prefixes)\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# The full process for preparing the data is:\n",
    "#\n",
    "# -  Read text file and split into lines, split lines into pairs\n",
    "# -  Normalize text, filter by length and content\n",
    "# -  Make word lists from sentences in pairs\n",
    "#\n",
    "\n",
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
    "print(random.choice(pairs))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
