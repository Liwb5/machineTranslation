{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "import jieba\n",
    "import h5py\n",
    "\n",
    "SOS_token = 0\n",
    "EOS_token = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    #通过不断输入sentence（字符串的格式），构建词与下标的对应（词典），方便制作one-hot。\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "    def save(self, path):\n",
    "        with open(path,'wb') as f:\n",
    "            pickle.dump([self.name,self.word2index, self.word2count, self.index2word, self.n_words],f)\n",
    "    \n",
    "    def load(self,path):\n",
    "        with open(path,'rb') as f:\n",
    "            name, self.word2index, self.word2count, self.index2word, self.n_words = pickle.load(f)\n",
    "        if self.name != name:\n",
    "            print('error: Name error------------------------------!')\n",
    "            \n",
    "            \n",
    "##################################################################\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "def normalizeChinese(s):\n",
    "    try:\n",
    "        s.encode(\"gb2312\")\n",
    "    except UnicodeEncodeError:\n",
    "        return ' '\n",
    "    s = re.sub(r\"[~!@#$%^&* ]+\",r' ', s)\n",
    "    return s\n",
    "\n",
    "\n",
    "#lang1 = 'zh'  lang2 = 'en'\n",
    "#默认英文到中文\n",
    "def readTrainLangs(lang1, lang2, reverse=True,fenci = False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    zh_lines = open('../data/train.%s'% lang1).read().strip().split('\\n')\n",
    "    #zh_lines = zh_lines[0:20]  #for test\n",
    "\n",
    "    zh_data_list = []\n",
    "    if fenci:\n",
    "        #jieba 分词\n",
    "        for line in zh_lines:\n",
    "            seg_line = jieba.cut(line,cut_all=False)\n",
    "            #dic = [seg for seg in seg_line]\n",
    "            dic = ' '.join(seg_line)\n",
    "            tmp = ' '\n",
    "            for char in dic.split(' '):\n",
    "                val = normalizeChinese(char)\n",
    "                tmp += val+' '\n",
    "            zh_data_list.append(tmp)\n",
    "    else: #用空格按字分开\n",
    "        for line in zh_lines:\n",
    "            dic = ' '.join(line)\n",
    "            tmp = ' '\n",
    "            for char in dic.split(' '):\n",
    "                val = normalizeChinese(char)##去除生僻词\n",
    "                tmp += val+' '\n",
    "            zh_data_list.append(tmp)\n",
    "\n",
    "    en_lines = open('../data/train.%s'% lang2).read().strip().split('\\n')\n",
    "    #en_lines = en_lines[0:20]  #for test\n",
    "    \n",
    "    # Split every line into pairs and normalize\n",
    "    #去掉一些标点符号\n",
    "    en_data_list = [[normalizeString(s) for s in l.split('\\t')] for l in en_lines]\n",
    "    pairs = []\n",
    "    if reverse:\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "        for en,zh in zip(en_data_list,zh_data_list):\n",
    "            input_lang.addSentence(en[0])\n",
    "            output_lang.addSentence(zh)\n",
    "            pairs.append([en[0].encode('utf-8'),zh.encode('gb2312')])\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "        for en,zh in zip(en_data_list,zh_data_list):\n",
    "            input_lang.addSentence(zh)\n",
    "            output_lang.addSentence(en[0])\n",
    "            pairs.append([zh.encode('gb2312'), en[0].encode('utf-8')])\n",
    "            \n",
    "    return input_lang, output_lang, pairs\n",
    "##################################################\n",
    "\n",
    "#这部分就是对数据进行处理的函数了，上面写的函数都会在这里被调用\n",
    "#最后得到三个变量input_lang，output_lang分别是源语言和目标语言的类，包含它们各自的词典。\n",
    "#pairs是一个列表，列表的元素是一个二元tuple，tuple里面的内容是一句源语言字符串，一句目标语言字符串。\n",
    "def prepareData(lang1, lang2, reverse=True, fenci=False):\n",
    "    input_lang, output_lang, pairs = readTrainLangs(lang1, lang2, reverse, fenci)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    print(pairs[0][0].decode('utf-8'),pairs[0][1].decode('gb2312'))\n",
    "    #pairs = filterPairs(pairs)\n",
    "    #print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "#     for pair in pairs:\n",
    "#         input_lang.addSentence(pair[0].decode('utf-8'))\n",
    "#         output_lang.addSentence(pair[1].decode('gb2312'))\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inputLang, outputLang, pairs = prepareData('zh','en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputLang.save('../data/en_train.pkl')\n",
    "outputLang.save('../data/zh_train.pkl')\n",
    "\n",
    "h5 = h5py.File('../data/train_afterProcess.h5py','w')\n",
    "h5.create_dataset('pairs',data=pairs,dtype = 'S400')\n",
    "h5.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a pair of red crowned cranes have staked out their nesting territory\n",
      " 一 对 丹 顶 鹤 正 监 视 着 它 们 的 筑 巢 领 地 \n",
      "error: Name error------------------------------!\n",
      "error: Name error------------------------------!\n",
      "en 388091\n",
      "zh 6754\n"
     ]
    }
   ],
   "source": [
    "import dataProcess as dp\n",
    "h5py_file = h5py.File('../data/train_afterProcess.h5py','r')\n",
    "pairs = h5py_file['pairs']\n",
    "print(pairs[0][0].decode('utf-8'))\n",
    "print(pairs[0][1].decode('gb2312'))\n",
    "\n",
    "inputlang = dp.Lang('en')\n",
    "outputlang = dp.Lang('zh')\n",
    "inputlang.load('../data/en_train.pkl')\n",
    "outputlang.load('../data/zh_train.pkl')\n",
    "\n",
    "print(inputlang.name,inputlang.n_words)\n",
    "print(outputlang.name,outputlang.n_words)\n",
    "#h5py_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "tt = ['我是中国人'.encode('gb2312'),'sdfeow dsf df dsf '.encode('utf-8')]\n",
    "h5 = h5py.File('../data/test1.h5py','w')\n",
    "h5.create_dataset('pairs',data=tt,dtype = 'S400')\n",
    "h5.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input english word count: 388091\n",
      "word count larger than 100: 82103\n",
      "word count Less than 10: 677382\n",
      "rate of word count less than 10: 0.005871136314350413\n"
     ]
    }
   ],
   "source": [
    "count = inputlang.word2count\n",
    "wordFreqLess10 = 0#保存词频小于某个阈值的词\n",
    "wordFreqMore100 = 0\n",
    "all_word_count=0\n",
    "print('input english word count:' , inputlang.n_words)\n",
    "for word in count:\n",
    "    all_word_count += count[word]\n",
    "    if count[word] <= 10:\n",
    "        wordFreqLess10 +=count[word]\n",
    "    if count[word] <= 10:\n",
    "        wordFreqMore100 +=1\n",
    "print('word count larger than 100:',inputlang.n_words - wordFreqMore100)\n",
    "print('word count Less than 10:',wordFreqLess10)\n",
    "print('rate of word count less than 10:', float(wordFreqLess10)/float(all_word_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "388089\n",
      "388089\n",
      "388091\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "388089\n",
      "388089\n",
      "388091\n",
      "82101\n",
      "82101\n",
      "82103\n"
     ]
    }
   ],
   "source": [
    "print(len(inputlang.word2count))\n",
    "print(len(inputlang.word2index))\n",
    "print(len(inputlang.index2word))\n",
    "all_en_words = inputlang.word2count.copy()\n",
    "for word in all_en_words:\n",
    "    if  all_en_words[word] <= 10:\n",
    "        inputlang.word2count.pop(word)\n",
    "        index = inputlang.word2index[word]\n",
    "        inputlang.word2index.pop(word)\n",
    "        inputlang.index2word.pop(index)\n",
    "        \n",
    "print(len(inputlang.word2count))\n",
    "print(len(inputlang.word2index))\n",
    "print(len(inputlang.index2word))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'd', 34: 'f', 22: 't'}\n",
      "{1: 'd', 22: 't'}\n",
      "{'b': 2, 'c': 4, 'a': 23}\n",
      "{'c': 4, 'a': 23}\n"
     ]
    }
   ],
   "source": [
    "testdict = {'a':23,'b':2,'c':4}\n",
    "test2 = {1:'d',34:'f',22:'t'}\n",
    "print(test2)\n",
    "test2.pop(34)\n",
    "print(test2)\n",
    "a = testdict.copy()\n",
    "print(a)\n",
    "for word in testdict:\n",
    "    if testdict[word] < 3:\n",
    "        a.pop(word)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 匹配生僻字与非法字符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def is_rare_name(string):\n",
    "    pattern = re.compile(u\"[~!@#$%^&* ]\")\n",
    "    match = pattern.search(string)\n",
    "    if match:\n",
    "        return True\n",
    "    try:\n",
    "        string.encode(\"gb2312\")\n",
    "    except UnicodeEncodeError:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def normalizeChinese(s):\n",
    "    try:\n",
    "        s.encode(\"gb2312\")\n",
    "    except UnicodeEncodeError:\n",
    "        return ' '\n",
    "    s = re.sub(r\"[~!@#$%^&* ]+\",r' ', s)\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "str1 = '我 是 @ 中 国 人 ， 我 # 不 才 囧 怪。'\n",
    "#str1 = 'I an # a boy'\n",
    "l = [normalizeChinese(s) for s in str1.split(' ')]\n",
    "print(l)\n",
    "tmp = ' '\n",
    "for s in str1.split(' '):\n",
    "    val = normalizeChinese(s)\n",
    "    tmp += val+' '\n",
    "print(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = re.search(r'.','I love FishC.com!')\n",
    "print(result.group())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = '我 是 @ 中 国 人 ， 我 # 不 囧 才 怪。'\n",
    "s = re.sub(r\"[~!@#$%^&* ]+\",r' ', s)\n",
    "print (s)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
