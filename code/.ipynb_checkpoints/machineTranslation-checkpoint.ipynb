{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "import jieba\n",
    "import h5py\n",
    "\n",
    "SOS_token = 0\n",
    "EOS_token = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    #通过不断输入sentence（字符串的格式），构建词与下标的对应（词典），方便制作one-hot。\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "    def save(self, path):\n",
    "        with open(path,'wb') as f:\n",
    "            pickle.dump([self.name,self.word2index, self.word2count, self.index2word, self.n_words],f)\n",
    "    \n",
    "    def load(self,path):\n",
    "        with open(path,'rb') as f:\n",
    "            name, self.word2index, self.word2count, self.index2word, self.n_words = pickle.load(f)\n",
    "        if self.name != name:\n",
    "            print('error: Name error------------------------------!')\n",
    "            \n",
    "            \n",
    "##################################################################\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "def normalizeChinese(s):\n",
    "    try:\n",
    "        s.encode(\"gb2312\")\n",
    "    except UnicodeEncodeError:\n",
    "        return ' '\n",
    "    s = re.sub(r\"[~!@#$%^&* ]+\",r' ', s)\n",
    "    return s\n",
    "\n",
    "\n",
    "#lang1 = 'zh'  lang2 = 'en'\n",
    "#默认英文到中文\n",
    "def readTrainLangs(lang1, lang2, reverse=True,fenci = False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    zh_lines = open('../data/train.%s'% lang1).read().strip().split('\\n')\n",
    "    #zh_lines = zh_lines[0:20]  #for test\n",
    "\n",
    "    zh_data_list = []\n",
    "    if fenci:\n",
    "        #jieba 分词\n",
    "        for line in zh_lines:\n",
    "            seg_line = jieba.cut(line,cut_all=False)\n",
    "            #dic = [seg for seg in seg_line]\n",
    "            dic = ' '.join(seg_line)\n",
    "            tmp = ' '\n",
    "            for char in dic.split(' '):\n",
    "                val = normalizeChinese(char)\n",
    "                tmp += val+' '\n",
    "            zh_data_list.append(tmp)\n",
    "    else: #用空格按字分开\n",
    "        for line in zh_lines:\n",
    "            dic = ' '.join(line)\n",
    "            tmp = ' '\n",
    "            for char in dic.split(' '):\n",
    "                val = normalizeChinese(char)##去除生僻词\n",
    "                tmp += val+' '\n",
    "            zh_data_list.append(tmp)\n",
    "\n",
    "    en_lines = open('../data/train.%s'% lang2).read().strip().split('\\n')\n",
    "    #en_lines = en_lines[0:20]  #for test\n",
    "    \n",
    "    # Split every line into pairs and normalize\n",
    "    #去掉一些标点符号\n",
    "    en_data_list = [[normalizeString(s) for s in l.split('\\t')] for l in en_lines]\n",
    "    pairs = []\n",
    "    if reverse:\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "        for en,zh in zip(en_data_list,zh_data_list):\n",
    "            input_lang.addSentence(en[0])\n",
    "            output_lang.addSentence(zh)\n",
    "            pairs.append([en[0].encode('utf-8'),zh.encode('gb2312')])\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "        for en,zh in zip(en_data_list,zh_data_list):\n",
    "            input_lang.addSentence(zh)\n",
    "            output_lang.addSentence(en[0])\n",
    "            pairs.append([zh.encode('gb2312'), en[0].encode('utf-8')])\n",
    "            \n",
    "    return input_lang, output_lang, pairs\n",
    "##################################################\n",
    "\n",
    "#这部分就是对数据进行处理的函数了，上面写的函数都会在这里被调用\n",
    "#最后得到三个变量input_lang，output_lang分别是源语言和目标语言的类，包含它们各自的词典。\n",
    "#pairs是一个列表，列表的元素是一个二元tuple，tuple里面的内容是一句源语言字符串，一句目标语言字符串。\n",
    "def prepareData(lang1, lang2, reverse=True, fenci=False):\n",
    "    input_lang, output_lang, pairs = readTrainLangs(lang1, lang2, reverse, fenci)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    print(pairs[0][0].decode('utf-8'),pairs[0][1].decode('gb2312'))\n",
    "    #pairs = filterPairs(pairs)\n",
    "    #print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "#     for pair in pairs:\n",
    "#         input_lang.addSentence(pair[0].decode('utf-8'))\n",
    "#         output_lang.addSentence(pair[1].decode('gb2312'))\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputLang, outputLang, pairs = prepareData('zh','en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputLang.save('../data/en_train.pkl')\n",
    "outputLang.save('../data/zh_train.pkl')\n",
    "\n",
    "h5 = h5py.File('../data/train_afterProcess.h5py','w')\n",
    "h5.create_dataset('pairs',data=pairs,dtype = 'S400')\n",
    "h5.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a pair of red crowned cranes have staked out their nesting territory\n",
      " 一 对 丹 顶 鹤 正 监 视 着 它 们 的 筑 巢 领 地 \n",
      "en 370\n",
      "zh 454\n"
     ]
    }
   ],
   "source": [
    "import dataProcess as dp\n",
    "h5py_file = h5py.File('../data/train_afterProcess.h5py','r')\n",
    "pairs = h5py_file['pairs']\n",
    "\n",
    "pairs = pairs[0:100]\n",
    "print(pairs[0][0].decode('utf-8'))\n",
    "print(pairs[0][1].decode('gb2312'))\n",
    "\n",
    "inputlang = dp.Lang('en')\n",
    "outputlang = dp.Lang('zh')\n",
    "# inputlang.load('../data/en_train.pkl')\n",
    "# outputlang.load('../data/zh_train.pkl')\n",
    "\n",
    "####测试用，上面两行注释的语句在真正运行的时候要用到的##################################################################\n",
    "for pair in pairs:\n",
    "    inputlang.addSentence(pair[0].decode('utf-8'))\n",
    "    outputlang.addSentence(pair[1].decode('gb2312'))\n",
    "    \n",
    "print(inputlang.name,inputlang.n_words)\n",
    "print(outputlang.name,outputlang.n_words)\n",
    "#h5py_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter -- remove words that appear less than 10 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "388089\n",
      "388089\n",
      "388091\n",
      "82101\n",
      "82101\n",
      "82103\n"
     ]
    }
   ],
   "source": [
    "print(len(inputlang.word2count))\n",
    "print(len(inputlang.word2index))\n",
    "print(len(inputlang.index2word))\n",
    "all_en_words = inputlang.word2count.copy()\n",
    "for word in all_en_words:\n",
    "    if  all_en_words[word] <= 10:\n",
    "        inputlang.word2count.pop(word)\n",
    "        index = inputlang.word2index[word]\n",
    "        inputlang.word2index.pop(word)\n",
    "        inputlang.index2word.pop(index)\n",
    "        \n",
    "print(len(inputlang.word2count))\n",
    "print(len(inputlang.word2index))\n",
    "print(len(inputlang.index2word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from hyperboard import Agent\n",
    "\n",
    "agent = Agent(address='127.0.0.1',port=5000)\n",
    "#agent = Agent(address='172.18.216.69',port=5000)\n",
    "hyperparameters = {'learning rate':0.01}\n",
    "name = agent.register(hyperparameters, 'loss',overwrite=True)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "MAX_LENGTH = 30\n",
    "teacher_forcing_ratio = 0.5  #在训练时解码器使用labels（平行预料）进行训练的概率\n",
    "LEARNING_RATE = 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    #input_size是指词典的大小(毕竟要建立embedding)，hidden_size是hidden_state的维度\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    #input是一个句子(这个句子已经通过数据处理的类转换成下标，这样可以对应一个embedded)\n",
    "    #hidden 是上一个迭代中的hidden，即pre_hidden\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        #这个n_layers==1其实就是只相当于一个cell，对一个input(单词)和上一个hidden state\n",
    "        #这里做了一个gru操作。n_layers大于1则是对同一个东西迭代多次，也许效果会好。\n",
    "        for i in range(self.n_layers):\n",
    "            output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result\n",
    "        \n",
    "#hidden_size都是说hidden_state的维度，要和encoder一致。\n",
    "#output_size是目标语言的词典大小，因为输出的是所有词的概率\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, n_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        for i in range(self.n_layers):\n",
    "            output = F.relu(output)\n",
    "            output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result\n",
    "        \n",
    "#hidden_size都是说hidden_state的维度，要和encoder一致。\n",
    "#output_size是目标语言的词典大小，因为输出的是所有词的概率\n",
    "#max_length是句子的最大长度(之前被限制了，以后看看能否不要这个限制)\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, n_layers=1, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    #input是一个目标句子中的某个词(这个词已经通过数据处理的类转换成下标，这样可以对应一个embedded)。\n",
    "    #当然，在进行预测的时候就不会是输入目标句子的词了。而是它预测出来的词\n",
    "    #hidden 是上一个迭代中的hidden，即pre_hidden\n",
    "    #encoder_output是encoder最后一个输出，不过在这里它并没有被使用到\n",
    "    #encoder_outputs是encoder每次输出(y1,y2,...,yn)的组成tensor，格式跟input一样，只不过它是句子，而不是某个词。\n",
    "    def forward(self, input, hidden, encoder_output, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)))\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        for i in range(self.n_layers):\n",
    "            output = F.relu(output)\n",
    "            output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]))#每个词的概率\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将数据进行处理成pytorch变量，方便转换成embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#########################training#####################################\n",
    "#返回词对应的下标\n",
    "def indexesFromSentence(lang, sentence):\n",
    "    #return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "    result = []\n",
    "    all_lang_keys = lang.word2index.keys()\n",
    "    for word in sentence.split(' '):\n",
    "        if word in all_lang_keys:#判断词是否在词典中，因为词典中有些出现次数太少的词被删掉了\n",
    "            result.append(lang.word2index[word])\n",
    "    return result\n",
    "\n",
    "#将词转换成variable\n",
    "def variableFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    result = Variable(torch.LongTensor(indexes).view(-1, 1))\n",
    "    if use_cuda:\n",
    "        return result.cuda()\n",
    "    else:\n",
    "        return result\n",
    "\n",
    "    \n",
    "def variablesFromPair(pair,input_lang, output_lang):\n",
    "    #注意这里要先解码，因为保存到h5py里面的时候要编码，所以现在要解码\n",
    "    input_variable = variableFromSentence(input_lang, pair[0].decode('utf-8'))\n",
    "    target_variable = variableFromSentence(output_lang, pair[1].decode('gb2312'))\n",
    "    return (input_variable, target_variable)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义训练所需要的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(input_variable, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    #input_variable已经在函数外变成了tensor了，tensor的元素是词的下标\n",
    "    input_length = input_variable.size()[0]#source sentence 的长度\n",
    "    target_length = target_variable.size()[0]#目标句子的长度\n",
    "\n",
    "    encoder_outputs = Variable(torch.zeros(max_length, encoder.hidden_size))#max_length=10，也就是句子的最长长度，hidden_size是256，所以encoder_outputs是矩阵10X256\n",
    "    encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):#句子有多长就迭代多少次\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_variable[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0][0]#将每个词encoder的output记录下来\n",
    "\n",
    "    decoder_input = Variable(torch.LongTensor([[SOS_token]]))\n",
    "    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "    decoder_hidden = encoder_hidden#encoder最后一层的hidden_state传给decoder作为decoder的第一个hidden_state\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "        \t#encoder_outputs作为decoder的输入，是为了改变attention。\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_output, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_variable[di])#这两个变量是什么形式\n",
    "            decoder_input = target_variable[di]  # Teacher forcing这个是直接给答案，也就是一个单词，进入decoder里面再变成词向量\n",
    "\n",
    "    else:#这边是不直接给答案，而是每次output那里选择概率最大的作为下一个输入\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_output, encoder_outputs)\n",
    "            #从decoder的类中可以知道，decoder_output是softmax出来的，即所有词的概率。\n",
    "            #topk函数是查找最大的K 个数，这里参数是1，topv就是value，topi是index，也就是词对应的下标\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            ni = topi[0][0]\n",
    "\n",
    "            decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "            decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "            loss += criterion(decoder_output, target_variable[di])\n",
    "            if ni == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()#如何理解这一步反向梯度对encoder和decoder都有效\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.data[0] / target_length\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# This is a helper function to print time elapsed and estimated time\n",
    "# remaining given the current time and progress %.\n",
    "#\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent) #总时间\n",
    "    rs = es - s        #总时间减去已经运行的时间等于还剩下的时间\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "######################################################################\n",
    "# The whole training process looks like this:\n",
    "#\n",
    "# -  Start a timer\n",
    "# -  Initialize optimizers and criterion\n",
    "# -  Create set of training pairs\n",
    "# -  Start empty losses array for plotting\n",
    "#\n",
    "# Then we call ``train`` many times and occasionally print the progress (%\n",
    "# of examples, time so far, estimated time) and average loss.\n",
    "#\n",
    "\n",
    "def trainIters(encoder, decoder, inputlang, outputlang, n_iters, print_every=1000, plot_every=100, learning_rate=0.01, save_model_every=10000):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    #考虑改成其他的优化器\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    #这里的training_pairs经过variableFromPair处理后，每个元素已经是一个tensor了，并且是单词所在的下标，为了可以和embedd匹配。\n",
    "    #training_pairs = [variablesFromPair(random.choice(pairs))\n",
    "    #                 for i in range(n_iters)]\n",
    "    #print(random.choice(training_pairs)[0].data)\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        #training_pair = training_pairs[iter - 1]\n",
    "        #################@#%…………&&&\n",
    "        \n",
    "        training_pair = variablesFromPair(random.choice(pairs),inputlang,outputlang)\n",
    "        input_variable = training_pair[0]\n",
    "        target_variable = training_pair[1]\n",
    "\n",
    "        loss = train(input_variable, target_variable, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            agent.append(name, iter, plot_loss_avg)\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "            \n",
    "        if iter % save_model_every == 0:\n",
    "            torch.save(encoder.state_dict(),'../models/firstModelforTest/encoder.model{0}'.format(iter))\n",
    "            torch.save(decoder.state_dict(),'../models/firstModelforTest/decoder.model{0}'.format(iter))\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### real training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 1s (- 2m 2s) (10 1%) 5.1803\n",
      "0m 2s (- 1m 45s) (20 2%) 4.2406\n",
      "0m 2s (- 1m 34s) (30 3%) 2.7209\n",
      "0m 3s (- 1m 33s) (40 4%) 3.8582\n",
      "0m 4s (- 1m 33s) (50 5%) 3.1568\n",
      "0m 5s (- 1m 31s) (60 6%) 2.9410\n",
      "0m 6s (- 1m 29s) (70 7%) 2.8581\n",
      "0m 7s (- 1m 27s) (80 8%) 3.0708\n",
      "0m 8s (- 1m 24s) (90 9%) 3.1120\n",
      "0m 9s (- 1m 22s) (100 10%) 2.8838\n",
      "0m 9s (- 1m 19s) (110 11%) 2.1233\n",
      "0m 10s (- 1m 17s) (120 12%) 2.5620\n",
      "0m 11s (- 1m 14s) (130 13%) 2.4656\n",
      "0m 11s (- 1m 11s) (140 14%) 2.8532\n",
      "0m 12s (- 1m 9s) (150 15%) 3.1096\n",
      "0m 12s (- 1m 7s) (160 16%) 2.6291\n",
      "0m 13s (- 1m 6s) (170 17%) 2.6315\n",
      "0m 14s (- 1m 5s) (180 18%) 3.6557\n",
      "0m 14s (- 1m 3s) (190 19%) 2.7477\n",
      "0m 15s (- 1m 2s) (200 20%) 2.6909\n",
      "0m 16s (- 1m 2s) (210 21%) 2.9392\n",
      "0m 17s (- 1m 3s) (220 22%) 3.1193\n",
      "0m 18s (- 1m 3s) (230 23%) 3.0570\n",
      "0m 19s (- 1m 2s) (240 24%) 3.0176\n",
      "0m 20s (- 1m 2s) (250 25%) 3.0326\n",
      "0m 21s (- 1m 0s) (260 26%) 2.8846\n",
      "0m 22s (- 0m 59s) (270 27%) 2.7951\n",
      "0m 23s (- 1m 0s) (280 28%) 3.1883\n",
      "0m 24s (- 1m 1s) (290 28%) 2.2723\n",
      "0m 26s (- 1m 2s) (300 30%) 2.7882\n",
      "0m 27s (- 1m 1s) (310 31%) 2.2495\n",
      "0m 28s (- 1m 0s) (320 32%) 3.4943\n",
      "0m 29s (- 0m 59s) (330 33%) 3.2738\n",
      "0m 30s (- 0m 59s) (340 34%) 3.0020\n",
      "0m 31s (- 0m 58s) (350 35%) 2.7473\n",
      "0m 32s (- 0m 57s) (360 36%) 2.6020\n",
      "0m 32s (- 0m 56s) (370 37%) 2.7755\n",
      "0m 33s (- 0m 55s) (380 38%) 2.9459\n",
      "0m 34s (- 0m 54s) (390 39%) 2.9152\n",
      "0m 35s (- 0m 53s) (400 40%) 2.6662\n",
      "0m 36s (- 0m 52s) (410 41%) 2.8634\n",
      "0m 36s (- 0m 50s) (420 42%) 2.9281\n",
      "0m 37s (- 0m 50s) (430 43%) 2.8234\n",
      "0m 38s (- 0m 49s) (440 44%) 3.1614\n",
      "0m 39s (- 0m 48s) (450 45%) 2.5566\n",
      "0m 40s (- 0m 47s) (460 46%) 2.8352\n",
      "0m 41s (- 0m 47s) (470 47%) 2.6783\n",
      "0m 43s (- 0m 46s) (480 48%) 2.6444\n",
      "0m 43s (- 0m 45s) (490 49%) 2.6777\n",
      "0m 45s (- 0m 45s) (500 50%) 3.0674\n",
      "0m 46s (- 0m 44s) (510 51%) 2.4689\n",
      "0m 46s (- 0m 43s) (520 52%) 2.8571\n",
      "0m 47s (- 0m 42s) (530 53%) 1.8603\n",
      "0m 48s (- 0m 40s) (540 54%) 2.1054\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 64\n",
    "encoder1 = EncoderRNN(inputlang.n_words, hidden_size)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, outputlang.n_words,\n",
    "                               1, dropout_p=0.1)\n",
    "\n",
    "if use_cuda:\n",
    "    encoder1 = encoder1.cuda()\n",
    "    attn_decoder1 = attn_decoder1.cuda()\n",
    "\n",
    "trainIters(encoder1, attn_decoder1, inputlang, outputlang, 1000, print_every=10, save_model_every=4000)\n",
    "\n",
    "######################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "######################################################################\n",
    "# Evaluation\n",
    "# ==========\n",
    "#\n",
    "# Evaluation is mostly the same as training, but there are no targets so\n",
    "# we simply feed the decoder's predictions back to itself for each step.\n",
    "# Every time it predicts a word we add it to the output string, and if it\n",
    "# predicts the EOS token we stop there. We also store the decoder's\n",
    "# attention outputs for display later.\n",
    "#\n",
    "\n",
    "def evaluate(encoder, decoder, sentence, input_lang, max_length=MAX_LENGTH):\n",
    "    input_variable = variableFromSentence(input_lang, sentence)\n",
    "    input_length = input_variable.size()[0]\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_outputs = Variable(torch.zeros(max_length, encoder.hidden_size))\n",
    "    encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_variable[ei],\n",
    "                                                 encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_outputs[ei] + encoder_output[0][0]\n",
    "\n",
    "    decoder_input = Variable(torch.LongTensor([[SOS_token]]))  # SOS\n",
    "    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    decoded_words = []\n",
    "    decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "    for di in range(max_length):#注意这里跟训练的时候不一样，训练的时候用的是target_length。这里因为要输出句子，而代码限定了句子的最大长度。\n",
    "        decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "            decoder_input, decoder_hidden, encoder_output, encoder_outputs)\n",
    "        decoder_attentions[di] = decoder_attention.data\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        ni = topi[0][0]\n",
    "        if ni == EOS_token:\n",
    "            decoded_words.append('<EOS>')  #检测到结束符就停止\n",
    "            break\n",
    "        else:\n",
    "            decoded_words.append(output_lang.index2word[ni])\n",
    "\n",
    "        decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "        decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "    return decoded_words, decoder_attentions[:di + 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# We can evaluate random sentences from the training set and print out the\n",
    "# input, target, and output to make some subjective quality judgements:\n",
    "#\n",
    "\n",
    "def evaluateRandomly(encoder, decoder, n=100):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0].decode('utf-8'))\n",
    "        print('=', pair[1].decode('gb2312'))\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0].decode('utf-8'))\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
