{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "import jieba\n",
    "import h5py\n",
    "\n",
    "SOS_token = 0\n",
    "EOS_token = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    #通过不断输入sentence（字符串的格式），构建词与下标的对应（词典），方便制作one-hot。\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "    def save(self, path):\n",
    "        with open(path,'wb') as f:\n",
    "            pickle.dump([self.name,self.word2index, self.word2count, self.index2word, self.n_words],f)\n",
    "    \n",
    "    def load(self,path):\n",
    "        with open(path,'rb') as f:\n",
    "            name, self.word2index, self.word2count, self.index2word, self.n_words = pickle.load(f)\n",
    "        if self.name != name:\n",
    "            print('error: Name error------------------------------!')\n",
    "            \n",
    "            \n",
    "##################################################################\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "def normalizeChinese(s):\n",
    "    try:\n",
    "        s.encode(\"gb2312\")\n",
    "    except UnicodeEncodeError:\n",
    "        return ' '\n",
    "    s = re.sub(r\"[~!@#$%^&* ]+\",r' ', s)\n",
    "    return s\n",
    "\n",
    "\n",
    "#lang1 = 'zh'  lang2 = 'en'\n",
    "#默认英文到中文\n",
    "def readTrainLangs(lang1, lang2, reverse=True,fenci = False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    zh_lines = open('../data/train.%s'% lang1).read().strip().split('\\n')\n",
    "    #zh_lines = zh_lines[0:20]  #for test\n",
    "\n",
    "    zh_data_list = []\n",
    "    if fenci:\n",
    "        #jieba 分词\n",
    "        for line in zh_lines:\n",
    "            seg_line = jieba.cut(line,cut_all=False)\n",
    "            #dic = [seg for seg in seg_line]\n",
    "            dic = ' '.join(seg_line)\n",
    "            tmp = ' '\n",
    "            for char in dic.split(' '):\n",
    "                val = normalizeChinese(char)\n",
    "                tmp += val+' '\n",
    "            zh_data_list.append(tmp)\n",
    "    else: #用空格按字分开\n",
    "        for line in zh_lines:\n",
    "            dic = ' '.join(line)\n",
    "            tmp = ' '\n",
    "            for char in dic.split(' '):\n",
    "                val = normalizeChinese(char)##去除生僻词\n",
    "                tmp += val+' '\n",
    "            zh_data_list.append(tmp)\n",
    "\n",
    "    en_lines = open('../data/train.%s'% lang2).read().strip().split('\\n')\n",
    "    #en_lines = en_lines[0:20]  #for test\n",
    "    \n",
    "    # Split every line into pairs and normalize\n",
    "    #去掉一些标点符号\n",
    "    en_data_list = [[normalizeString(s) for s in l.split('\\t')] for l in en_lines]\n",
    "    pairs = []\n",
    "    if reverse:\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "        for en,zh in zip(en_data_list,zh_data_list):\n",
    "            input_lang.addSentence(en[0])\n",
    "            output_lang.addSentence(zh)\n",
    "            pairs.append([en[0].encode('utf-8'),zh.encode('gb2312')])\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "        for en,zh in zip(en_data_list,zh_data_list):\n",
    "            input_lang.addSentence(zh)\n",
    "            output_lang.addSentence(en[0])\n",
    "            pairs.append([zh.encode('gb2312'), en[0].encode('utf-8')])\n",
    "            \n",
    "    return input_lang, output_lang, pairs\n",
    "##################################################\n",
    "\n",
    "#这部分就是对数据进行处理的函数了，上面写的函数都会在这里被调用\n",
    "#最后得到三个变量input_lang，output_lang分别是源语言和目标语言的类，包含它们各自的词典。\n",
    "#pairs是一个列表，列表的元素是一个二元tuple，tuple里面的内容是一句源语言字符串，一句目标语言字符串。\n",
    "def prepareData(lang1, lang2, reverse=True, fenci=False):\n",
    "    input_lang, output_lang, pairs = readTrainLangs(lang1, lang2, reverse, fenci)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    print(pairs[0][0].decode('utf-8'),pairs[0][1].decode('gb2312'))\n",
    "    #pairs = filterPairs(pairs)\n",
    "    #print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "#     for pair in pairs:\n",
    "#         input_lang.addSentence(pair[0].decode('utf-8'))\n",
    "#         output_lang.addSentence(pair[1].decode('gb2312'))\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputLang, outputLang, pairs = prepareData('zh','en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputLang.save('../data/en_train.pkl')\n",
    "outputLang.save('../data/zh_train.pkl')\n",
    "\n",
    "h5 = h5py.File('../data/train_afterProcess.h5py','w')\n",
    "h5.create_dataset('pairs',data=pairs,dtype = 'S400')\n",
    "h5.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a pair of red crowned cranes have staked out their nesting territory\n",
      " 一 对 丹 顶 鹤 正 监 视 着 它 们 的 筑 巢 领 地 \n",
      "en 1842\n",
      "zh 1408\n"
     ]
    }
   ],
   "source": [
    "import dataProcess as dp\n",
    "h5py_file = h5py.File('../data/train_afterProcess.h5py','r')\n",
    "pairs = h5py_file['pairs']\n",
    "\n",
    "pairs = pairs[0:1000]\n",
    "print(pairs[0][0].decode('utf-8'))\n",
    "print(pairs[0][1].decode('gb2312'))\n",
    "\n",
    "inputlang = dp.Lang('en')\n",
    "outputlang = dp.Lang('zh')\n",
    "# inputlang.load('../data/en_train.pkl')\n",
    "# outputlang.load('../data/zh_train.pkl')\n",
    "\n",
    "####测试用，上面两行注释的语句在真正运行的时候要用到的##################################################################\n",
    "for pair in pairs:\n",
    "    inputlang.addSentence(pair[0].decode('utf-8'))\n",
    "    outputlang.addSentence(pair[1].decode('gb2312'))\n",
    "    \n",
    "print(inputlang.name,inputlang.n_words)\n",
    "print(outputlang.name,outputlang.n_words)\n",
    "#h5py_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter -- remove words that appear less than 10 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "388089\n",
      "388089\n",
      "388091\n",
      "82101\n",
      "82101\n",
      "82103\n"
     ]
    }
   ],
   "source": [
    "print(len(inputlang.word2count))\n",
    "print(len(inputlang.word2index))\n",
    "print(len(inputlang.index2word))\n",
    "all_en_words = inputlang.word2count.copy()\n",
    "for word in all_en_words:\n",
    "    if  all_en_words[word] <= 10:\n",
    "        inputlang.word2count.pop(word)\n",
    "        index = inputlang.word2index[word]\n",
    "        inputlang.word2index.pop(word)\n",
    "        inputlang.index2word.pop(index)\n",
    "        \n",
    "print(len(inputlang.word2count))\n",
    "print(len(inputlang.word2index))\n",
    "print(len(inputlang.index2word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from hyperboard import Agent\n",
    "\n",
    "agent = Agent(address='127.0.0.1',port=5000)\n",
    "#agent = Agent(address='172.18.216.69',port=5000)\n",
    "hyperparameters = {'learning rate':0.01}\n",
    "name = agent.register(hyperparameters, 'loss',overwrite=True)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "MAX_LENGTH = 30\n",
    "teacher_forcing_ratio = 0.5  #在训练时解码器使用labels（平行预料）进行训练的概率\n",
    "LEARNING_RATE = 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    #input_size是指词典的大小(毕竟要建立embedding)，hidden_size是hidden_state的维度\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    #input是一个句子(这个句子已经通过数据处理的类转换成下标，这样可以对应一个embedded)\n",
    "    #hidden 是上一个迭代中的hidden，即pre_hidden\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        #这个n_layers==1其实就是只相当于一个cell，对一个input(单词)和上一个hidden state\n",
    "        #这里做了一个gru操作。n_layers大于1则是对同一个东西迭代多次，也许效果会好。\n",
    "        for i in range(self.n_layers):\n",
    "            output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result\n",
    "        \n",
    "#hidden_size都是说hidden_state的维度，要和encoder一致。\n",
    "#output_size是目标语言的词典大小，因为输出的是所有词的概率\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, n_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        for i in range(self.n_layers):\n",
    "            output = F.relu(output)\n",
    "            output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result\n",
    "        \n",
    "#hidden_size都是说hidden_state的维度，要和encoder一致。\n",
    "#output_size是目标语言的词典大小，因为输出的是所有词的概率\n",
    "#max_length是句子的最大长度(之前被限制了，以后看看能否不要这个限制)\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, n_layers=1, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    #input是一个目标句子中的某个词(这个词已经通过数据处理的类转换成下标，这样可以对应一个embedded)。\n",
    "    #当然，在进行预测的时候就不会是输入目标句子的词了。而是它预测出来的词\n",
    "    #hidden 是上一个迭代中的hidden，即pre_hidden\n",
    "    #encoder_output是encoder最后一个输出，不过在这里它并没有被使用到\n",
    "    #encoder_outputs是encoder每次输出(y1,y2,...,yn)的组成tensor，格式跟input一样，只不过它是句子，而不是某个词。\n",
    "    def forward(self, input, hidden, encoder_output, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)))\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        for i in range(self.n_layers):\n",
    "            output = F.relu(output)\n",
    "            output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]))#每个词的概率\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将数据进行处理成pytorch变量，方便转换成embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#########################training#####################################\n",
    "#返回词对应的下标\n",
    "def indexesFromSentence(lang, sentence):\n",
    "    #return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "    result = []\n",
    "    all_lang_keys = lang.word2index.keys()\n",
    "    for word in sentence.split(' '):\n",
    "        if word in all_lang_keys:#判断词是否在词典中，因为词典中有些出现次数太少的词被删掉了\n",
    "            result.append(lang.word2index[word])\n",
    "    return result\n",
    "\n",
    "#将词转换成variable\n",
    "def variableFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    result = Variable(torch.LongTensor(indexes).view(-1, 1))\n",
    "    if use_cuda:\n",
    "        return result.cuda()\n",
    "    else:\n",
    "        return result\n",
    "\n",
    "    \n",
    "def variablesFromPair(pair,input_lang, output_lang):\n",
    "    #注意这里要先解码，因为保存到h5py里面的时候要编码，所以现在要解码\n",
    "    input_variable = variableFromSentence(input_lang, pair[0].decode('utf-8'))\n",
    "    target_variable = variableFromSentence(output_lang, pair[1].decode('gb2312'))\n",
    "    return (input_variable, target_variable)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义训练所需要的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(input_variable, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    #input_variable已经在函数外变成了tensor了，tensor的元素是词的下标\n",
    "    input_length = input_variable.size()[0]#source sentence 的长度\n",
    "    target_length = target_variable.size()[0]#目标句子的长度\n",
    "\n",
    "    encoder_outputs = Variable(torch.zeros(max_length, encoder.hidden_size))#max_length=10，也就是句子的最长长度，hidden_size是256，所以encoder_outputs是矩阵10X256\n",
    "    encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):#句子有多长就迭代多少次\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_variable[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0][0]#将每个词encoder的output记录下来\n",
    "\n",
    "    decoder_input = Variable(torch.LongTensor([[SOS_token]]))\n",
    "    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "    decoder_hidden = encoder_hidden#encoder最后一层的hidden_state传给decoder作为decoder的第一个hidden_state\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "        \t#encoder_outputs作为decoder的输入，是为了改变attention。\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_output, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_variable[di])#这两个变量是什么形式\n",
    "            decoder_input = target_variable[di]  # Teacher forcing这个是直接给答案，也就是一个单词，进入decoder里面再变成词向量\n",
    "\n",
    "    else:#这边是不直接给答案，而是每次output那里选择概率最大的作为下一个输入\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_output, encoder_outputs)\n",
    "            #从decoder的类中可以知道，decoder_output是softmax出来的，即所有词的概率。\n",
    "            #topk函数是查找最大的K 个数，这里参数是1，topv就是value，topi是index，也就是词对应的下标\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            ni = topi[0][0]\n",
    "\n",
    "            decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "            decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "            loss += criterion(decoder_output, target_variable[di])\n",
    "            if ni == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()#如何理解这一步反向梯度对encoder和decoder都有效\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.data[0] / target_length\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# This is a helper function to print time elapsed and estimated time\n",
    "# remaining given the current time and progress %.\n",
    "#\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent) #总时间\n",
    "    rs = es - s        #总时间减去已经运行的时间等于还剩下的时间\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "######################################################################\n",
    "# The whole training process looks like this:\n",
    "#\n",
    "# -  Start a timer\n",
    "# -  Initialize optimizers and criterion\n",
    "# -  Create set of training pairs\n",
    "# -  Start empty losses array for plotting\n",
    "#\n",
    "# Then we call ``train`` many times and occasionally print the progress (%\n",
    "# of examples, time so far, estimated time) and average loss.\n",
    "#\n",
    "\n",
    "def trainIters(encoder, decoder, inputlang, outputlang, n_iters, print_every=1000, plot_every=100, learning_rate=0.01, save_model_every=10000):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    #考虑改成其他的优化器\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    #这里的training_pairs经过variableFromPair处理后，每个元素已经是一个tensor了，并且是单词所在的下标，为了可以和embedd匹配。\n",
    "    #training_pairs = [variablesFromPair(random.choice(pairs))\n",
    "    #                 for i in range(n_iters)]\n",
    "    #print(random.choice(training_pairs)[0].data)\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        #training_pair = training_pairs[iter - 1]\n",
    "        #################@#%…………&&&\n",
    "        \n",
    "        training_pair = variablesFromPair(random.choice(pairs),inputlang,outputlang)\n",
    "        input_variable = training_pair[0]\n",
    "        target_variable = training_pair[1]\n",
    "\n",
    "        loss = train(input_variable, target_variable, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            agent.append(name, iter, plot_loss_avg)\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "            \n",
    "        if iter % save_model_every == 0:\n",
    "            torch.save(encoder.state_dict(),'../models/encoder.model{0}'.format(iter))\n",
    "            torch.save(decoder.state_dict(),'../models/decoder.model{0}'.format(iter))\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### real training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 18s (- 12m 15s) (100 2%) 4.0702\n",
      "0m 28s (- 8m 53s) (200 5%) 3.1481\n",
      "0m 37s (- 7m 42s) (300 7%) 3.4844\n",
      "0m 44s (- 6m 41s) (400 10%) 3.3048\n",
      "0m 52s (- 6m 7s) (500 12%) 3.3797\n",
      "1m 3s (- 6m 0s) (600 15%) 3.3205\n",
      "1m 14s (- 5m 49s) (700 17%) 3.2064\n",
      "1m 23s (- 5m 35s) (800 20%) 3.2238\n",
      "1m 33s (- 5m 23s) (900 22%) 3.2745\n",
      "1m 43s (- 5m 9s) (1000 25%) 3.2137\n",
      "1m 51s (- 4m 52s) (1100 27%) 3.0761\n",
      "1m 57s (- 4m 35s) (1200 30%) 3.1396\n",
      "2m 5s (- 4m 21s) (1300 32%) 3.2696\n",
      "2m 14s (- 4m 9s) (1400 35%) 3.2522\n",
      "2m 21s (- 3m 56s) (1500 37%) 3.2117\n",
      "2m 28s (- 3m 43s) (1600 40%) 3.1307\n",
      "2m 35s (- 3m 30s) (1700 42%) 2.9380\n",
      "2m 42s (- 3m 18s) (1800 45%) 3.1566\n",
      "2m 48s (- 3m 6s) (1900 47%) 3.1065\n",
      "2m 54s (- 2m 54s) (2000 50%) 2.8038\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../models/firstModelforTest/encoder.model2000'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-5266cddd535c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mattn_decoder1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_decoder1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtrainIters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_decoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_model_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m######################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-9be09888edee>\u001b[0m in \u001b[0;36mtrainIters\u001b[0;34m(encoder, decoder, inputlang, outputlang, n_iters, print_every, plot_every, learning_rate, save_model_every)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0miter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msave_model_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'../models/firstModelforTest/encoder.model{0}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'../models/firstModelforTest/decoder.model{0}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/liwb/anaconda3/lib/python3.5/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../models/firstModelforTest/encoder.model2000'"
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "encoder1 = EncoderRNN(inputlang.n_words, hidden_size)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, outputlang.n_words,\n",
    "                               1, dropout_p=0.1)\n",
    "\n",
    "if use_cuda:\n",
    "    encoder1 = encoder1.cuda()\n",
    "    attn_decoder1 = attn_decoder1.cuda()\n",
    "\n",
    "trainIters(encoder1, attn_decoder1, inputlang, outputlang, 4000, print_every=100, save_model_every=2000)\n",
    "\n",
    "######################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "######################################################################\n",
    "# Evaluation\n",
    "# ==========\n",
    "#\n",
    "# Evaluation is mostly the same as training, but there are no targets so\n",
    "# we simply feed the decoder's predictions back to itself for each step.\n",
    "# Every time it predicts a word we add it to the output string, and if it\n",
    "# predicts the EOS token we stop there. We also store the decoder's\n",
    "# attention outputs for display later.\n",
    "#\n",
    "\n",
    "def evaluate(encoder, decoder, sentence, input_lang, output_lang, max_length=MAX_LENGTH):\n",
    "    input_variable = variableFromSentence(input_lang, sentence)\n",
    "    input_length = input_variable.size()[0]\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_outputs = Variable(torch.zeros(max_length, encoder.hidden_size))\n",
    "    encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_variable[ei],\n",
    "                                                 encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_outputs[ei] + encoder_output[0][0]\n",
    "\n",
    "    decoder_input = Variable(torch.LongTensor([[SOS_token]]))  # SOS\n",
    "    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    decoded_words = []\n",
    "    decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "    for di in range(max_length):#注意这里跟训练的时候不一样，训练的时候用的是target_length。这里因为要输出句子，而代码限定了句子的最大长度。\n",
    "        decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "            decoder_input, decoder_hidden, encoder_output, encoder_outputs)\n",
    "        decoder_attentions[di] = decoder_attention.data\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        ni = topi[0][0]\n",
    "        if ni == EOS_token:\n",
    "            decoded_words.append('<EOS>')  #检测到结束符就停止\n",
    "            break\n",
    "        else:\n",
    "            decoded_words.append(output_lang.index2word[ni])\n",
    "\n",
    "        decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "        decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "    return decoded_words, decoder_attentions[:di + 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# We can evaluate random sentences from the training set and print out the\n",
    "# input, target, and output to make some subjective quality judgements:\n",
    "#\n",
    "\n",
    "def evaluateRandomly(encoder, decoder, inputlang, outputlang, n=100):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0].decode('utf-8'))\n",
    "        print('=', pair[1].decode('gb2312'))\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0].decode('utf-8'),inputlang, outputlang)\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> a letter that miss have sham received minutes before the wedding .\n",
      "=  一 封 富 人 小 姐 在 婚 礼 前 的 二 十 分 钟 前 收 到 的 。 \n",
      "<  一 小 时 内 有 的  <EOS>\n",
      "\n",
      "> one against .\n",
      "=  一 对 五 百 诶 。 \n",
      "<  一 对 夫 妇 的  <EOS>\n",
      "\n",
      "> five on one . five on one . yeah not the greatest odds .\n",
      "=  一 对 五 。 一 对 五 。 胜 算 不 大 啊 。 \n",
      "<  一 对 夫 妇 。  <EOS>\n",
      "\n",
      "> we took her off an hour ago .\n",
      "=  我 们 一 小 时 之 前 就 拿 开 了 。 \n",
      "<  一 小 时 前 我 就 在 我 们 。  <EOS>\n",
      "\n",
      "> they re just a nice happy normal couple .\n",
      "=  一 对 美 满 幸 福 普 通 的 夫 妻 。 \n",
      "<  一 对 夫 妇 的  <EOS>\n",
      "\n",
      "> i m resigning in an hour \n",
      "=  一 小 时 内 我 就 辞 职 ， \n",
      "<  一 小 时 内 ，  <EOS>\n",
      "\n",
      "> damaged goods .\n",
      "=  一 对 烂 货 ！ \n",
      "<  一 对 夫 妇 。  <EOS>\n",
      "\n",
      "> phoned it in about an hour ago .\n",
      "=  一 小 时 前 打 电 话 报 的 警 。 \n",
      "<  一 小 时 前 我 在 我 们 。  <EOS>\n",
      "\n",
      "> be back in one hour . and call me as soon as the deal is done .\n",
      "=  一 小 时 内 回 来 。 交 易 一 结 束 就 马 上 打 电 话 告 诉 我 。 \n",
      "<  一 小 时 内 我 们 。  <EOS>\n",
      "\n",
      "> they could be herein an hour .\n",
      "=  他 们 一 小 时 之 内 就 能 到 达 。 \n",
      "<  一 小 时 内 我 就 。  <EOS>\n",
      "\n",
      "> a couple of kids fishing poles .\n",
      "=  一 对 儿 童 鱼 竿 。 \n",
      "<  一 对 夫 妇  <EOS>\n",
      "\n",
      "> i can t tell you everything in a letter \n",
      "=  一 封 信 里 无 法 说 清 我 想 说 的 话 ， \n",
      "<  一 封 块 ，  <EOS>\n",
      "\n",
      "> a couple getting married .\n",
      "=  一 对 新 人 。 \n",
      "<  一 对 夫 妇  <EOS>\n",
      "\n",
      "> two bees have a baby they have a bee .\n",
      "=  一 对 蜜 蜂 呢 会 生 一 只 小 蜜 蜂 。 \n",
      "<  一 对 夫 妇 的  <EOS>\n",
      "\n",
      "> may have been still clinging on in asia\n",
      "=  一 小 撮 直 立 人 可 能 还 \n",
      "<  一 小 时 前 我 们  <EOS>\n",
      "\n",
      "> he was backing me up an hour ago .\n",
      "=  一 小 时 前 他 还 替 我 上 场 的 。 \n",
      "<  一 小 时 前 我 在 我 们 。  <EOS>\n",
      "\n",
      "> an hour later the director phoned me and put early retirement on the table .\n",
      "=  一 小 时 以 后 ， 主 管 就 给 我 打 了 电 话 让 我 提 早 退 休 。 \n",
      "<  一 小 时 内 我 们  <EOS>\n",
      "\n",
      "> we ll know more in an hour .\n",
      "=  一 小 时 内 会 有 消 息 的 。 \n",
      "<  一 小 时 内 我 们 。  <EOS>\n",
      "\n",
      "> a voice mail .\n",
      "=  一 封 语 音 邮 件 。 \n",
      "<  一 封 信 ，  <EOS>\n",
      "\n",
      "> he s being released into witness protection within the hour .\n",
      "=  一 小 时 内 他 会 被 交 给 证 人 保 护 。 \n",
      "<  一 小 时 内 我 们 。  <EOS>\n",
      "\n",
      "> a couple of windows but uh \n",
      "=  一 对 窗 户 ， 但 是 ， 呃 ， \n",
      "<  一 封 块 ，  <EOS>\n",
      "\n",
      "> he ll have a thousand dates in an hour .\n",
      "=  一 小 时 内 就 能 有 上 千 女 人 蜂 拥 而 至 。 \n",
      "<  一 小 时 内 我 们 。  <EOS>\n",
      "\n",
      "> ah ! couple of wusses .\n",
      "=  一 对 儿 窝 囊 废 。 \n",
      "<  一 对 夫 妇 。  <EOS>\n",
      "\n",
      "> a nice naked hippie couple gave me some brownies .\n",
      "=  一 对 裸 体 嬉 皮 情 侣 给 了 我 一 些 点 心 。 \n",
      "<  一 对 夫 妇 的  <EOS>\n",
      "\n",
      "> in the hour mind or lose it .\n",
      "=  一 小 时 内 ， 记 着 ， 否 则 位 置 就 没 了 。 \n",
      "<  一 小 时 内 我 就 。  <EOS>\n",
      "\n",
      "> a letter from chicago .\n",
      "=  一 封 来 自 芝 加 哥 的 。 \n",
      "<  一 封 信 。  <EOS>\n",
      "\n",
      "> twin girls .\n",
      "=  一 对 双 胞 胎 女 孩 。 \n",
      "<  一 封 块 。  <EOS>\n",
      "\n",
      "> a breeding pair .\n",
      "=  一 对 能 繁 殖 的 浣 熊 。 \n",
      "<  一 封 块 。  <EOS>\n",
      "\n",
      "> a nibble s not enough .\n",
      "=  一 小 口 不 够 的 。 \n",
      "<  一 对 夫 妇 。  <EOS>\n",
      "\n",
      "> a pair of pied kingfishers are trying to find\n",
      "=  一 对 斑 驳 的 翠 鸟 也 在 食 蜂 鸟 属 地 里 努 力 寻 找 \n",
      "<  一 封 信 ？  <EOS>\n",
      "\n",
      "> was alone in the vehicle when this accident happened just an hour ago .\n",
      "=  一 小 时 前 发 生 意 外 时 。 \n",
      "<  一 小 时 前 我 们 就 能 。  <EOS>\n",
      "\n",
      "> pair of kings three of a kind wins .\n",
      "=  一 对 王 ， 三 张 一 类 的 获 胜 。 \n",
      "<  一 对 夫 妇 的  <EOS>\n",
      "\n",
      "> meet me in the office in an hour .\n",
      "=  一 小 时 内 在 办 公 室 等 我 。 \n",
      "<  一 小 时 内 我 们 。  <EOS>\n",
      "\n",
      "> and a couple really big ones .\n",
      "=  一 对 夫 妇 一 个 天 大 的 错 误 。 \n",
      "<  一 对 夫 妇 的  <EOS>\n",
      "\n",
      "> and exactly one hour later \n",
      "=  一 小 时 之 后 ， \n",
      "<  一 小 时 前 ，  <EOS>\n",
      "\n",
      "> because you didn t care .\n",
      "=  一 封 都 没 有 回 复 。 \n",
      "<  一 对 夫 妇 的  <EOS>\n",
      "\n",
      "> violet won t be home for an hour .\n",
      "=  一 小 时 以 内 瓦 奥 莱 特 不 会 回 家 。 \n",
      "<  一 小 时 内 我 们 。  <EOS>\n",
      "\n",
      "> before a father and a daughter turn up dead\n",
      "=  一 对 父 女 就 会 在 某 个 地 方 丧 命 \n",
      "<  一 对 夫 妇 的  <EOS>\n",
      "\n",
      "> what have you found ?\n",
      "=  你 发 现 了 什 么 ？ \n",
      "<  一 封 信 ？  <EOS>\n",
      "\n",
      "> because you didn t care .\n",
      "=  一 封 都 没 有 回 复 。 \n",
      "<  一 对 夫 妇 的  <EOS>\n",
      "\n",
      "> a young couple with a lack of family warmth .\n",
      "=  一 对 少 年 男 女 在 缺 乏 家 庭 温 暖 之 下 。 \n",
      "<  一 对 夫 妇 的  <EOS>\n",
      "\n",
      "> within one hour you ll each have dates .\n",
      "=  一 小 时 内 有 2 0 个 对 象 。 \n",
      "<  一 小 时 内 我 们 。  <EOS>\n",
      "\n",
      "> your husband was spotted at a convenience store not far from here about an hour ago .\n",
      "=  一 小 时 前 有 人 发 现 你 丈 夫 出 现 在 附 近 的 便 利 店 。 \n",
      "<  一 小 时 前 我 们  <EOS>\n",
      "\n",
      "> they ended an hour ago . now shut the hell up !\n",
      "=  一 小 时 前 就 结 束 了 。 现 在 把 嘴 闭 上 ！ \n",
      "<  一 小 时 前 我 就 在 了 。  <EOS>\n",
      "\n",
      "> a couple of rich jerks they fly their private jet \n",
      "=  一 对 有 钱 的 变 态 ， 开 着 自 己 的 私 人 飞 机 ， \n",
      "<  一 封 信 ，  <EOS>\n",
      "\n",
      "> could asked me that an hour ago .\n",
      "=  一 小 时 前 你 就 该 问 我 了 。 \n",
      "<  一 小 时 前 我 在 我 们 。  <EOS>\n",
      "\n",
      "> i got a call from the bank about an hour ago .\n",
      "=  一 小 时 前 我 接 到 银 行 的 电 话 。 \n",
      "<  一 小 时 前 我 们 就 。  <EOS>\n",
      "\n",
      "> a couple s son died in a crash \n",
      "=  一 对 夫 妻 的 儿 子 死 于 车 祸 ， \n",
      "<  一 封 信 ，  <EOS>\n",
      "\n",
      "> an urgent registered letter ?\n",
      "=  一 封 加 急 挂 号 信 ？ \n",
      "<  一 封 信 ？  <EOS>\n",
      "\n",
      "> a male and female .\n",
      "=  一 对 男 女 。 \n",
      "<  一 对 夫 妇 的  <EOS>\n",
      "\n",
      "> and we re back \n",
      "=  一 小 时 1 2 分 钟 之 后 ， \n",
      "<  一 小 时 前 ，  <EOS>\n",
      "\n",
      "> be back in one hour . and call me as soon as the deal is done .\n",
      "=  一 小 时 内 回 来 。 交 易 一 结 束 就 马 上 打 电 话 告 诉 我 。 \n",
      "<  一 小 时 内 我 就 能 。  <EOS>\n",
      "\n",
      "> school started almost an hour ago .\n",
      "=  一 小 时 前 就 开 课 了 。 \n",
      "<  一 小 时 前 我 就 在 了 。  <EOS>\n",
      "\n",
      "> an e mail infected with a worm that could bore\n",
      "=  一 封 带 病 毒 的 \n",
      "<  一 对 夫 妇 的  <EOS>\n",
      "\n",
      "> and a maternal match .\n",
      "=  一 对 母 女 关 系 。 \n",
      "<  一 对 夫 妇 。  <EOS>\n",
      "\n",
      "> you should have them within the hour .\n",
      "=  一 小 时 之 内 应 该 就 能 拿 到 。 \n",
      "<  一 小 时 内 我 就 。  <EOS>\n",
      "\n",
      "> wow . couples coming in to have their plastic surgery reversed .\n",
      "=  一 对 夫 妻 来 想 毁 掉 以 前 的 整 容 成 果 。 \n",
      "<  一 封 块 儿 。  <EOS>\n",
      "\n",
      "> a letter of recommendation from a science teacher\n",
      "=  一 封 由 理 科 老 师 写 的 推 荐 信 \n",
      "<  一 封 块 儿 ？  <EOS>\n",
      "\n",
      "> a good bride and groom that s it . you re done .\n",
      "=  一 对 好 的 新 娘 新 郎 -   到 此 为 止 。 你 该 说 完 了 。 \n",
      "<  一 对 夫 妇 的  <EOS>\n",
      "\n",
      "> an hour ago i was here asleep \n",
      "=  一 小 时 前 我 在 这 里 睡 觉 ， \n",
      "<  一 小 时 前 ，  <EOS>\n",
      "\n",
      "> a letter won t stop us from winning this war !\n",
      "=  一 封 信 不 会 阻 止 我 们 取 得 战 争 的 胜 利 ！ \n",
      "<  一 封 块 儿 ？  <EOS>\n",
      "\n",
      "> we contacted the mother about an hour ago and she refused to come down .\n",
      "=  一 小 时 前 我 们 联 系 了 母 亲 但 她 拒 绝 到 访 。 \n",
      "<  一 小 时 前 我 们 我 们 。  <EOS>\n",
      "\n",
      "> you had a milk shake an hour ago .\n",
      "=  一 小 时 前 你 刚 喝 了 奶 昔 。 \n",
      "<  一 小 时 前 我 们 我 们 。  <EOS>\n",
      "\n",
      "> had mine an hour ago . hour ago huh ?\n",
      "=  一 小 时 前 就 吃 了 ， 是 吗 ？ \n",
      "<  一 小 时 前 我 在 我 们 了 。  <EOS>\n",
      "\n",
      "> a photo journey of a special couple on their special day .\n",
      "=  一 对 特 别 的 夫 妇 在 他 们 特 别 的 日 子 里 的 相 片 之 旅 。 \n",
      "<  一 对 夫 妇 的  <EOS>\n",
      "\n",
      "> eight bucks an hour .\n",
      "=  一 小 时 8 美 元 。 \n",
      "<  一 小 时 内 我 们 。  <EOS>\n",
      "\n",
      "> and like an hour ago \n",
      "=  一 小 时 前 ， \n",
      "<  一 小 时 前 ，  <EOS>\n",
      "\n",
      "> and i ll be there within the hour\n",
      "=  一 小 时 内 我 就 会 赶 到 \n",
      "<  一 小 时 内 我 们 。  <EOS>\n",
      "\n",
      "> i ll be gone in an hour .\n",
      "=  一 小 时 之 内 我 就 走 。 \n",
      "<  一 小 时 内 我 就 。  <EOS>\n",
      "\n",
      "> phoned it in about an hour ago .\n",
      "=  一 小 时 前 打 电 话 报 的 警 。 \n",
      "<  一 小 时 前 我 在 我 们 。  <EOS>\n",
      "\n",
      "> cheating spouses \n",
      "=  一 对 狗 男 女 ， \n",
      "<  一 封 信 ，  <EOS>\n",
      "\n",
      "> and exactly one hour later \n",
      "=  一 小 时 之 后 ， \n",
      "<  一 小 时 前 ，  <EOS>\n",
      "\n",
      "> they found a couple burned in their home .\n",
      "=  一 对 夫 妇 被 发 现 烧 死 在 家 中 。 \n",
      "<  一 对 夫 妇 的  <EOS>\n",
      "\n",
      "> of two men who love each other \n",
      "=  一 对 男 同 性 恋 ， \n",
      "<  一 对 夫 妇 的  <EOS>\n",
      "\n",
      "> a iittie area ?\n",
      "=  一 小 块 地 方 ？ \n",
      "<  一 小 块 ？  <EOS>\n",
      "\n",
      "> yeah get back to me within the hour and i will hold the story .\n",
      "=  一 小 时 内 给 我 答 复 , 这 期 间 我 暂 时 不 发 稿 。 \n",
      "<  一 小 时 内 我 们 我 们 。  <EOS>\n",
      "\n",
      "> plane leaves in an hour . jonas and charlie are already down there .\n",
      "=  一 小 时 内 出 发 。 乔 纳 斯 和 查 理 已 经 在 那 了 。 \n",
      "<  一 小 时 内 我  <EOS>\n",
      "\n",
      "> since about an hour ago when the earth opened up and tried to swallow los angeles whole .\n",
      "=  一 小 时 前 大 地 张 开 血 盆 大 口 想 要 将 洛 杉 矶 整 个 吞 进 腹 中 。 \n",
      "<  一 小 时 前 我 们 我 们 。  <EOS>\n",
      "\n",
      "> and in about an hour . i ii have absoiutely think that no friends ieft .\n",
      "=  一 小 时 内 ， 我 将 一 个 朋 友 也 没 有 。 \n",
      "<  一 小 时 前 我 在 我 们 。  <EOS>\n",
      "\n",
      "> you have a doctor s appointment in an hour .\n",
      "=  一 小 时 内 你 去 预 约 医 生 。 \n",
      "<  一 小 时 内 我 们 。  <EOS>\n",
      "\n",
      "> pair of underachievers .\n",
      "=  一 对 后 进 生 。 \n",
      "<  一 对 夫 妇 。  <EOS>\n",
      "\n",
      "> a couple of fat pervert losers .\n",
      "=  一 对 肥 佬 性 变 态 衰 人 。 \n",
      "<  一 对 夫 妇  <EOS>\n",
      "\n",
      "> an interracial couple .\n",
      "=  一 对 黑 白 配 夫 妇 。 \n",
      "<  一 对 夫 妇 的  <EOS>\n",
      "\n",
      "> general . highness .\n",
      "=  将 军 。 殿 下 。 \n",
      "<  一 对 夫 妇 的  <EOS>\n",
      "\n",
      "> an incredibly emotional fight between sisters \n",
      "=  一 对 五 年 没 见 过 的 姐 妹 一 场 激 烈 的 争 吵 ？ \n",
      "<  一 对 夫 妇 的  <EOS>\n",
      "\n",
      "> he was backing me up an hour ago .\n",
      "=  一 小 时 前 他 还 替 我 上 场 的 。 \n",
      "<  一 小 时 前 我 在 我 们 。  <EOS>\n",
      "\n",
      "> pinch of h heat the foil inhale the fumes .\n",
      "=  一 小 撮 海 洛 因 ， 加 热 铝 箔 ， 吸 气 。 \n",
      "<  一 小 时 前 有 的  <EOS>\n",
      "\n",
      "> he s just fired his sixth attorney an hour ago\n",
      "=  一 小 时 前 他 刚 炒 掉 他 的 第 6 名 律 师 \n",
      "<  一 小 时 前 我 们  <EOS>\n",
      "\n",
      "> a couple of damn kids .\n",
      "=  一 对 该 死 的 孩 子 。 \n",
      "<  一 对 夫 妇  <EOS>\n",
      "\n",
      "> i thought we were eating dinner an hour ago .\n",
      "=  一 小 时 前 我 们 本 该 一 起 吃 晚 饭 的 。 \n",
      "<  一 小 时 前 我 们 我 们 。  <EOS>\n",
      "\n",
      "> one second . hopefully .\n",
      "=  一 小 会 儿 。 但 愿 。 \n",
      "<  一 封 信 ？  <EOS>\n",
      "\n",
      "> should been in an o . r . an hour ago .\n",
      "=  一 小 时 前 就 该 进 手 术 室 了 。 \n",
      "<  一 小 时 前 我 们 。  <EOS>\n",
      "\n",
      "> it s a letter to the king .\n",
      "=  一 封 给 国 王 的 信 。 \n",
      "<  一 对 块 人 。  <EOS>\n",
      "\n",
      "> a pair is enough .\n",
      "=  一 对 就 够 了 。 \n",
      "<  一 对 夫 妇  <EOS>\n",
      "\n",
      "> at washington central station within the hour .\n",
      "=  一 小 时 内 袭 击 华 盛 顿 中 心 车 站 。 \n",
      "<  一 小 时 内 我 就 。  <EOS>\n",
      "\n",
      "> i am returning to vulcan within the hour .\n",
      "=  一 小 时 内 我 就 要 回 凡 根 星 球 了 我 想 向 你 道 别 一 声 。 \n",
      "<  一 小 时 内 我 们 。  <EOS>\n",
      "\n",
      "> two young man and a woman meet at a ball .\n",
      "=  一 对 年 轻 男 女 在 舞 会 上 相 遇 . \n",
      "<  一 对 夫 妇 的  <EOS>\n",
      "\n",
      "> a couple is hardly allowed a moment alone together\n",
      "=  一 对 夫 妇 步 入 婚 姻 殿 堂 之 前 \n",
      "<  一 对 夫 妇 的  <EOS>\n",
      "\n",
      "> a postcard from my passionate abandoned inamorata .\n",
      "=  一 封 来 自 我 梦 中 情 人 的 明 信 片 。 \n",
      "<  一 小 时 前 ，  <EOS>\n",
      "\n",
      "> a nibble s not enough .\n",
      "=  一 小 口 不 够 的 。 \n",
      "<  一 对 夫 妇  <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder1,attn_decoder1,inputlang,outputlang)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
