{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "import jieba\n",
    "import h5py\n",
    "\n",
    "SOS_token = 0\n",
    "EOS_token = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a pair of red crowned cranes have staked out their nesting territory\n",
      " 一 对 丹 顶 鹤 正 监 视 着 它 们 的 筑 巢 领 地 \n",
      "en 1842\n",
      "zh 1408\n"
     ]
    }
   ],
   "source": [
    "import dataProcess as dp\n",
    "h5py_file = h5py.File('../data/train_afterProcess.h5py','r')\n",
    "pairs = h5py_file['pairs']\n",
    "\n",
    "print(pairs[0][0].decode('utf-8'))\n",
    "print(pairs[0][1].decode('gb2312'))\n",
    "\n",
    "inputlang = dp.Lang('en')\n",
    "outputlang = dp.Lang('zh')\n",
    "inputlang.load('../data/en_train.pkl')\n",
    "outputlang.load('../data/zh_train.pkl')\n",
    "\n",
    "####测试用，上面两行注释的语句在真正运行的时候要用到的##################################################################\n",
    "# pairs = pairs[0:1000]\n",
    "# for pair in pairs:\n",
    "#     inputlang.addSentence(pair[0].decode('utf-8'))\n",
    "#     outputlang.addSentence(pair[1].decode('gb2312'))\n",
    "    \n",
    "print(inputlang.name,inputlang.n_words)\n",
    "print(outputlang.name,outputlang.n_words)\n",
    "#h5py_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cut some unfrequency words\n",
    "print(len(inputlang.word2count))\n",
    "print(len(inputlang.word2index))\n",
    "print(len(inputlang.index2word))\n",
    "all_en_words = inputlang.word2count.copy()\n",
    "for word in all_en_words:\n",
    "    if  all_en_words[word] <= 10:\n",
    "        inputlang.word2count.pop(word)\n",
    "        index = inputlang.word2index[word]\n",
    "        inputlang.word2index.pop(word)\n",
    "        inputlang.index2word.pop(index)\n",
    "        \n",
    "print(len(inputlang.word2count))\n",
    "print(len(inputlang.word2index))\n",
    "print(len(inputlang.index2word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dataProcess as dp\n",
    "h5py_file = h5py.File('../data/train_afterProcess.h5py','r')\n",
    "pairs = h5py_file['pairs']\n",
    "\n",
    "print(pairs[0][0].decode('utf-8'))\n",
    "print(pairs[0][1].decode('gb2312'))\n",
    "\n",
    "inputlang = dp.Lang('en')\n",
    "outputlang = dp.Lang('zh')\n",
    "# inputlang.load('../data/en_train.pkl')\n",
    "# outputlang.load('../data/zh_train.pkl')\n",
    "\n",
    "####测试用，上面两行注释的语句在真正运行的时候要用到的##################################################################\n",
    "pairs = pairs[0:1000]\n",
    "for pair in pairs:\n",
    "    inputlang.addSentence(pair[0].decode('utf-8'))\n",
    "    outputlang.addSentence(pair[1].decode('gb2312'))\n",
    "    \n",
    "print(inputlang.name,inputlang.n_words)\n",
    "print(outputlang.name,outputlang.n_words)\n",
    "#h5py_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "388089\n",
      "388089\n",
      "388091\n",
      "82101\n",
      "82101\n",
      "82103\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1m 38s (- 130m 13s) (1000 1%) 3.7346\n",
      "3m 0s (- 117m 24s) (2000 2%) 3.3245\n",
      "4m 18s (- 110m 39s) (3000 3%) 2.8743\n",
      "5m 38s (- 107m 2s) (4000 5%) 2.5834\n",
      "7m 1s (- 105m 28s) (5000 6%) 2.3345\n",
      "8m 28s (- 104m 35s) (6000 7%) 2.1141\n",
      "10m 11s (- 106m 21s) (7000 8%) 1.8685\n",
      "11m 53s (- 106m 58s) (8000 10%) 1.7289\n",
      "13m 35s (- 107m 16s) (9000 11%) 1.4711\n",
      "15m 14s (- 106m 40s) (10000 12%) 1.3665\n",
      "16m 35s (- 104m 2s) (11000 13%) 1.2610\n",
      "17m 55s (- 101m 35s) (12000 15%) 1.0647\n",
      "19m 4s (- 98m 16s) (13000 16%) 0.9653\n",
      "20m 15s (- 95m 28s) (14000 17%) 0.9568\n",
      "21m 33s (- 93m 24s) (15000 18%) 0.8217\n",
      "22m 52s (- 91m 28s) (16000 20%) 0.8044\n",
      "24m 8s (- 89m 29s) (17000 21%) 0.5796\n",
      "25m 49s (- 88m 56s) (18000 22%) 0.5767\n",
      "27m 34s (- 88m 31s) (19000 23%) 0.5180\n",
      "29m 6s (- 87m 18s) (20000 25%) 0.4798\n",
      "30m 26s (- 85m 31s) (21000 26%) 0.3960\n",
      "31m 48s (- 83m 51s) (22000 27%) 0.3309\n",
      "33m 18s (- 82m 32s) (23000 28%) 0.2961\n",
      "34m 56s (- 81m 30s) (24000 30%) 0.2452\n",
      "36m 40s (- 80m 41s) (25000 31%) 0.2579\n",
      "38m 23s (- 79m 44s) (26000 32%) 0.2516\n",
      "40m 6s (- 78m 43s) (27000 33%) 0.1556\n",
      "41m 48s (- 77m 38s) (28000 35%) 0.2075\n",
      "43m 16s (- 76m 6s) (29000 36%) 0.2231\n",
      "44m 44s (- 74m 33s) (30000 37%) 0.1652\n",
      "46m 25s (- 73m 22s) (31000 38%) 0.1402\n",
      "48m 3s (- 72m 5s) (32000 40%) 0.1429\n",
      "49m 44s (- 70m 50s) (33000 41%) 0.1053\n",
      "51m 25s (- 69m 34s) (34000 42%) 0.0976\n",
      "52m 56s (- 68m 4s) (35000 43%) 0.0938\n",
      "54m 40s (- 66m 49s) (36000 45%) 0.0886\n",
      "56m 15s (- 65m 22s) (37000 46%) 0.0747\n",
      "57m 50s (- 63m 55s) (38000 47%) 0.0510\n",
      "59m 27s (- 62m 30s) (39000 48%) 0.0377\n",
      "61m 3s (- 61m 3s) (40000 50%) 0.0455\n",
      "62m 31s (- 59m 28s) (41000 51%) 0.0316\n",
      "64m 0s (- 57m 54s) (42000 52%) 0.0252\n",
      "65m 22s (- 56m 15s) (43000 53%) 0.0170\n",
      "66m 40s (- 54m 33s) (44000 55%) 0.0200\n",
      "68m 0s (- 52m 53s) (45000 56%) 0.0222\n",
      "69m 9s (- 51m 6s) (46000 57%) 0.0207\n",
      "70m 49s (- 49m 43s) (47000 58%) 0.0352\n",
      "72m 8s (- 48m 5s) (48000 60%) 0.0155\n",
      "73m 17s (- 46m 22s) (49000 61%) 0.0267\n",
      "74m 33s (- 44m 44s) (50000 62%) 0.0148\n",
      "76m 4s (- 43m 15s) (51000 63%) 0.0237\n",
      "77m 25s (- 41m 41s) (52000 65%) 0.0140\n",
      "78m 46s (- 40m 7s) (53000 66%) 0.0137\n",
      "80m 4s (- 38m 33s) (54000 67%) 0.0095\n",
      "81m 19s (- 36m 57s) (55000 68%) 0.0090\n",
      "83m 5s (- 35m 36s) (56000 70%) 0.0083\n",
      "84m 42s (- 34m 11s) (57000 71%) 0.0174\n",
      "86m 20s (- 32m 45s) (58000 72%) 0.0094\n",
      "87m 59s (- 31m 19s) (59000 73%) 0.0075\n",
      "89m 42s (- 29m 54s) (60000 75%) 0.0099\n",
      "91m 29s (- 28m 29s) (61000 76%) 0.0134\n",
      "93m 11s (- 27m 3s) (62000 77%) 0.0119\n",
      "94m 34s (- 25m 31s) (63000 78%) 0.0066\n",
      "97m 43s (- 24m 25s) (64000 80%) 0.0075\n",
      "103m 18s (- 23m 50s) (65000 81%) 0.0111\n",
      "106m 51s (- 22m 40s) (66000 82%) 0.0144\n",
      "108m 19s (- 21m 1s) (67000 83%) 0.0078\n",
      "109m 29s (- 19m 19s) (68000 85%) 0.0232\n",
      "110m 54s (- 17m 40s) (69000 86%) 0.0094\n",
      "112m 22s (- 16m 3s) (70000 87%) 0.0076\n",
      "114m 23s (- 14m 29s) (71000 88%) 0.0118\n",
      "118m 19s (- 13m 8s) (72000 90%) 0.0075\n",
      "120m 1s (- 11m 30s) (73000 91%) 0.0088\n",
      "122m 22s (- 9m 55s) (74000 92%) 0.0069\n",
      "123m 59s (- 8m 15s) (75000 93%) 0.0099\n",
      "125m 27s (- 6m 36s) (76000 95%) 0.0072\n",
      "127m 8s (- 4m 57s) (77000 96%) 0.0049\n",
      "128m 45s (- 3m 18s) (78000 97%) 0.0097\n",
      "130m 12s (- 1m 38s) (79000 98%) 0.0090\n",
      "131m 38s (- 0m 0s) (80000 100%) 0.0057\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import models as m\n",
    "\n",
    "hidden_size = 256\n",
    "encoder1 = m.EncoderRNN(inputlang.n_words, hidden_size)\n",
    "attn_decoder1 = m.AttnDecoderRNN(hidden_size, outputlang.n_words,\n",
    "                               1, dropout_p=0.1)\n",
    "\n",
    "if m.use_cuda:\n",
    "    encoder1 = encoder1.cuda()\n",
    "    attn_decoder1 = attn_decoder1.cuda()\n",
    "\n",
    "m.trainIters(encoder1, attn_decoder1, inputlang, outputlang,pairs, n_iters = 8000000, \\\n",
    "             plot_every = 100, print_every=1000, save_model_every=400000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import models as m\n",
    "\n",
    "hidden_size = 256\n",
    "encoder1 = m.EncoderRNN(inputlang.n_words, hidden_size)\n",
    "attn_decoder1 = m.AttnDecoderRNN(hidden_size, outputlang.n_words,\n",
    "                               1, dropout_p=0.1)\n",
    "\n",
    "if m.use_cuda:\n",
    "    encoder1 = encoder1.cuda()\n",
    "    attn_decoder1 = attn_decoder1.cuda()\n",
    "\n",
    "m.trainIters(encoder1, attn_decoder1, inputlang, outputlang,pairs, n_iters = 80000, \\\n",
    "             plot_every=100, print_every=1000, save_model_every=40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> a couple of what ?\n",
      "=  一 对 什 么 ？ \n",
      "<  一 对 什 么 ？  <EOS>\n",
      "\n",
      "> a little garlic some paprika\n",
      "=  一 小 块 大 蒜 一 些 辣 椒 \n",
      "<  一 小 块 大 蒜 一 些 辣 椒  <EOS>\n",
      "\n",
      "> i took one in your driveway half an hour ago .\n",
      "=  半 个 小 时 前 我 开 车 过 来 的 还 吃 了 一 颗 。 \n",
      "<  半 个 小 时 前 我 开 车 过 来 的 还 吃 了 一 颗 。  <EOS>\n",
      "\n",
      "> painter s due in an hour .\n",
      "=  一 小 时 内 油 漆 匠 就 能 做 完 。 \n",
      "<  一 小 时 内 油 漆 匠 就 能 做 完 。  <EOS>\n",
      "\n",
      "> one second . wait . i ii be right back !\n",
      "=  一 小 会 儿 。 稍 等 。 我 马 上 回 来 ！ \n",
      "<  一 小 会 儿 。 稍 等 。 我 马 上 回 来 ！  <EOS>\n",
      "\n",
      "> teenagers fornicating in a liverpool basement .\n",
      "=  一 对 在 利 物 浦 地 下 室 私 通 的 青 少 年 。 \n",
      "<  一 对 在 利 物 浦 地 下 室 私 通 的 青 少 年 。  <EOS>\n",
      "\n",
      "> couple of people ? no a couple of antelope .\n",
      "=  一 对 夫 妇 ？ 不 ， 是 一 对 羚 羊 。 \n",
      "<  一 对 夫 妇 ？ 不 ， 是 一 对 羚 羊 。  <EOS>\n",
      "\n",
      "> couple of pretty ladies both trained assassins .\n",
      "=  一 对 漂 亮 女 性 ， 都 受 训 成 为 刺 客 。 \n",
      "<  一 对 漂 亮 女 性 ， 都 受 训 成 为 刺 客 。  <EOS>\n",
      "\n",
      "> a loving young couple cleaved together in terror \n",
      "=  一 对 年 轻 爱 人 紧 抱 在 一 起 陷 入 恐 惧 ， \n",
      "<  一 对 年 轻 爱 人 紧 抱 在 一 起 陷 入 恐 惧 ，  <EOS>\n",
      "\n",
      "> one that you wrote to santa claus when you were a little boy .\n",
      "=  一 封 你 小 时 候 写 给 圣 诞 老 人 的 信 。 \n",
      "<  一 封 你 小 时 候 写 给 圣 诞 老 人 的 信 。  <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "m.evaluateRandomly(encoder1,attn_decoder1,inputlang,outputlang,pairs,n = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load model and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import models as m\n",
    "#filter some Pair for test\n",
    "hidden_size = 256\n",
    "# encoder2 = m.EncoderRNN(inputlang.n_words, hidden_size)\n",
    "# attn_decoder2 = m.AttnDecoderRNN(hidden_size, outputlang.n_words,\n",
    "#                                1, dropout_p=0.1)\n",
    "# encoder2.load_state_dict(torch.load('../models/encoder.model8000'))\n",
    "# attn_decoder2.load_state_dict(torch.load('../models/decoder.model8000'))\n",
    "\n",
    "encoder2 = torch.load('../models/encoder.model800')\n",
    "deconder2 = torch.load('../models/decoder.model800')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> the dynamic duo will seal the deal tonight .\n",
      "=  一 对 俪 人 今 天 晚 上 就 会 敲 定 了 。 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/liwb/Documents/projects/machineTranslation/code/models.py:48: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greately increasing memory usage. To compact weights again call flatten_parameters().\n",
      "  output, hidden = self.gru(output, hidden)\n",
      "/data/liwb/Documents/projects/machineTranslation/code/models.py:124: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greately increasing memory usage. To compact weights again call flatten_parameters().\n",
      "  output, hidden = self.gru(output, hidden)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> ah ! couple of wusses .\n",
      "=  一 对 儿 窝 囊 废 。 \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> for a successful relationship . i fear for you child .\n",
      "=  为 了 成 功 的 关 系 。 我 为 你 担 心 ， 小 朋 友 。 \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> shot to death in their house .\n",
      "=  一 对 夫 妇 在 自 家 遇 害 。 \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> and a couple who have decided to separate\n",
      "=  一 对 准 备 分 居 的 夫 妻 \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> so does a couple that wants a large family .\n",
      "=  一 对 夫 妇 也 选 了 他 他 们 想 要 个 大 家 庭 。 \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> that man and woman so clearly made for each other \n",
      "=  天 造 地 设 的 一 对 ， \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> a pair of russian mortars and the crates look like american rifles .\n",
      "=  一 对 俄 国 迫 击 炮 ， 那 箱 子 看 起 来 像 是 美 国 的 步 枪 。 \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> one against .\n",
      "=  一 对 五 百 诶 。 \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> a pair of nines ? you pushed in with a pair of nines ?\n",
      "=  一 对 九 ？ 一 对 九 你 就 全 下 注 了 ？ \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> husband and wife .\n",
      "=  一 对 夫 妇 。 \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> one couple had to put their rottweiler down because\n",
      "=  一 对 夫 妇 不 得 不 杀 掉 他 们 的 罗 特 韦 尔 犬 \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> two lovers last seen with a dead husband ?\n",
      "=  一 对 偷 情 男 女 最 后 见 过 死 去 的 丈 夫 ？ \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> a couple who is willing to go along with an open adoption .\n",
      "=  一 对 同 意 开 放 领 养 的 夫 妇 。 \n",
      "<  一 对 垃 圾 箱 箱 子   <EOS>\n",
      "\n",
      "> twin hooker alibis .\n",
      "=  一 对 双 胞 胎 妓 女 给 他 做 的 证 。 \n",
      "<  一 对 垃 圾 箱 箱 子   <EOS>\n",
      "\n",
      "> a pair of star crossed lovers take their life \n",
      "=  一 对 命 运 不 佳 的 爱 人 失 去 了 生 命 ， \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> a gay dad wants to adopt a crack baby .\n",
      "=  一 对 同 性 恋 父 亲 打 算 收 养 一 个 在 妊 娠 期 摄 取 过 可 卡 因 的 婴 儿 。 \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> a pair of star crossed lovers take their life \n",
      "=  一 对 命 运 不 佳 的 爱 人 失 去 了 生 命 ， \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> a couple of coeds ?\n",
      "=  一 对 儿 老 同 学 ？ \n",
      "<  一 对 垃 圾 箱 箱 子   <EOS>\n",
      "\n",
      "> a couple ? what the fuck does that mean ?\n",
      "=  一 对 ？ 他 妈 的 什 么 意 思 ？ \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> a couple is hardly allowed a moment alone together\n",
      "=  一 对 夫 妇 步 入 婚 姻 殿 堂 之 前 \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> married couple and years old .\n",
      "=  一 对 夫 妇 ， 一 个 7 4 岁 一 个 7 7 岁 。 \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> a bunch of fellas sitting in the middle of this field .\n",
      "=  一 对 伙 计 们 坐 在 战 场 上 。 \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> pair of fives the winner .\n",
      "=  一 对 五 胜 。 \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> a couple ? like two people ? like one two people ?\n",
      "=  一 对 夫 妇 ？ 就 像 两 个 人 ？ 就 像 一 个 ， 两 个 人 ？ \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> ah ! couple of wusses .\n",
      "=  一 对 儿 窝 囊 废 。 \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> husband and wife .\n",
      "=  一 对 夫 妇 。 \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> into a convincing married couple .\n",
      "=  一 对 令 人 信 服 的 已 婚 夫 妇 。 \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> a pair of wives ?\n",
      "=  一 对 太 太 ？ \n",
      "<  一 对 垃 圾 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> a couple of kids fishing poles .\n",
      "=  一 对 儿 童 鱼 竿 。 \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> pair of fives the winner .\n",
      "=  一 对 五 胜 。 \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> a couple called . they found a body in the street .\n",
      "=  一 对 夫 妇 打 了 9 1 1 。 说 他 们 发 现 一 具 尸 体 在 路 上 。 \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> a blue balls on the rocks .\n",
      "=  一 对 加 冰 块 而 不 加 水 的 下 流 睾 丸 。 \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> a couple of coeds ?\n",
      "=  一 对 儿 老 同 学 ？ \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> a renowned synchronized swimming duo\n",
      "=  一 对 出 名 的 同 游 表 演 者 \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> about this rich couple in this townhouse \n",
      "=  一 对 住 在 别 墅 里 的 有 钱 夫 妇 ， \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> for a successful relationship . i fear for you child .\n",
      "=  为 了 成 功 的 关 系 。 我 为 你 担 心 ， 小 朋 友 。 \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> of a brother and sister who .\n",
      "=  一 对 兄 妹 . \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> twin hooker alibis .\n",
      "=  一 对 双 胞 胎 妓 女 给 他 做 的 证 。 \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> a renowned synchronized swimming duo\n",
      "=  一 对 出 名 的 同 游 表 演 者 \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> so does a couple that wants a large family .\n",
      "=  一 对 夫 妇 也 选 了 他 他 们 想 要 个 大 家 庭 。 \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> into a convincing married couple .\n",
      "=  一 对 令 人 信 服 的 已 婚 夫 妇 。 \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> america s brother and sister darlings \n",
      "=  一 对 亲 密 的 美 国 兄 妹 ， \n",
      "<  一 对 垃 圾 箱 箱 子   <EOS>\n",
      "\n",
      "> a couple of coeds ?\n",
      "=  一 对 儿 老 同 学 ？ \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> a pair of russian mortars nand the crates look like american rifles .\n",
      "=  一 对 俄 国 迫 击 炮 ， 那 箱 子 看 起 来 像 是 美 国 的 步 枪 。 \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> couple that couldn t be more mismatched . i m gonna guess that this guy s loaded .\n",
      "=  一 对 史 上 最 不 般 配 的 情 侣 。 我 猜 这 个 男 人 很 富 有 。 \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> two members of a gene pair when they segregate into the gametes \n",
      "=  一 对 基 因 的 两 个 成 员 ， 当 它 们 分 离 进 入 配 子 时 ， \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> into a convincing married couple .\n",
      "=  一 对 令 人 信 服 的 已 婚 夫 妇 。 \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> a husband and wife team are through to the next round .\n",
      "=  一 对 夫 妇 晋 级 到 了 下 一 轮 。 \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> couple of drug crazy maniacs .\n",
      "=  一 对 嗑 了 毒 品 的 疯 子 。 \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> i do it a couple times a year .\n",
      "=  一 对 夫 妇 一 年 预 定 的 我 做 它 的 。 \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> a couple of dumpster divers . dumpster divers .\n",
      "=  一 对 垃 圾 箱 \"   潜 水 \"   情 侣 。 垃 圾 箱 潜 水 员 。 \n",
      "<  一 对 垃 圾 箱 箱 子   <EOS>\n",
      "\n",
      "> about this rich couple in this townhouse \n",
      "=  一 对 住 在 别 墅 里 的 有 钱 夫 妇 ， \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> into a convincing married couple .\n",
      "=  一 对 令 人 信 服 的 已 婚 夫 妇 。 \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> couples freely dance together under the apple tree of temptation and around a bagpipe\n",
      "=  一 对 夫 妇 在 苹 果 树 下 伴 随 风 笛 自 由 跳 舞 \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> five on one . five on one . yeah not the greatest odds .\n",
      "=  一 对 五 。 一 对 五 。 胜 算 不 大 啊 。 \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> a renowned synchronized swimming duo\n",
      "=  一 对 出 名 的 同 游 表 演 者 \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> the proof of a fake married couple\n",
      "=  一 对 假 结 婚 的 夫 妇 \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> that man and woman so clearly made for each other \n",
      "=  天 造 地 设 的 一 对 ， \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> how a couple would begin to hate each other .\n",
      "=  一 对 夫 妇 如 何 开 始 讨 厌 对 方 。 \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> a bunch of fellas sitting in the middle of this field .\n",
      "=  一 对 伙 计 们 坐 在 战 场 上 。 \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> a brother and sister . when i search this vessel i won t find them will i ?\n",
      "=  一 对 兄 妹 。   在 我 搜 查 这 条 船 之 时 我 不 会 发 现 他 们 的 ， 对 吧 ？ \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> a pair of wives ?\n",
      "=  一 对 太 太 ？ \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> couple that couldn t be more mismatched . i m gonna guess that this guy s loaded .\n",
      "=  一 对 史 上 最 不 般 配 的 情 侣 。 我 猜 这 个 男 人 很 富 有 。 \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> a couple who is willing to go along with an open adoption .\n",
      "=  一 对 同 意 开 放 领 养 的 夫 妇 。 \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> a binary star one blue one yellow . lapis and gold .\n",
      "=  一 对 双 子 星 ， 一 个 是 蓝 色 的 ， 一 个 是 黄 色 的 。 石 头 和 黄 金 。 \n",
      "<  一 对 垃 圾 箱 箱 子   <EOS>\n",
      "\n",
      "> two fine young people starting out on the road of life .\n",
      "=  一 对 儿 年 轻 男 女 将 要 携 手 共 渡 人 生 旅 程 。 \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> a pair of brothers who have done bad things .\n",
      "=  一 对 兄 弟 犯 了 戒 。 \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> a couple of fishermen found him washed up on the beach .\n",
      "=  一 对 儿 渔 民 夫 妇 发 现 他 被 冲 到 了 沙 滩 上 。 \n",
      "<  一 对 垃 圾 箱 箱 子   <EOS>\n",
      "\n",
      "> you got all them dimples .\n",
      "=  一 对 可 爱 的 小 酒 窝 。 \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> one against .\n",
      "=  一 对 五 百 诶 。 \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> pair of underachievers .\n",
      "=  一 对 后 进 生 。 \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> a pair of russian mortars and the crates look like american rifles .\n",
      "=  一 对 俄 国 迫 击 炮 ， 那 箱 子 看 起 来 像 是 美 国 的 步 枪 。 \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> the twins played so nicely while they waited for their parents to get home .\n",
      "=  一 对 双 胞 胎 正 等 待 父 母 回 家 她 们 玩 得 很 融 洽 。 \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> two members of a gene pair when they segregate into the gametes \n",
      "=  一 对 基 因 的 两 个 成 员 ， 当 它 们 分 离 进 入 配 子 时 ， \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> couple of nice juicy boars !\n",
      "=  一 对 多 汁 的 野 猪 ！ \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> couple of what ? who s buddy ?\n",
      "=  一 对 什 么 ？ 哥 们 儿 是 谁 ？ \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> a couple who is willing to go along with an open adoption .\n",
      "=  一 对 同 意 开 放 领 养 的 夫 妇 。 \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> a pair of nines ? you pushed in with a pair of nines ?\n",
      "=  一 对 九 ？ 一 对 九 你 就 全 下 注 了 ？ \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> a blue balls on the rocks .\n",
      "=  一 对 加 冰 块 而 不 加 水 的 下 流 睾 丸 。 \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> an incredibly emotional fight between sisters \n",
      "=  一 对 五 年 没 见 过 的 姐 妹 一 场 激 烈 的 争 吵 ？ \n",
      "<  一 对 垃 圾 箱 箱 \"    情 侣 。  <EOS>\n",
      "\n",
      "> a couple ? like two people ? like one two people ?\n",
      "=  一 对 夫 妇 ？ 就 像 两 个 人 ？ 就 像 一 个 ， 两 个 人 ？ \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> and a couple who have decided to separate\n",
      "=  一 对 准 备 分 居 的 夫 妻 \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> a pair of wives ?\n",
      "=  一 对 太 太 ？ \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> a couple of dumpster divers .\n",
      "=  一 对 垃 圾 鸳 鸯 。 \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> a couple lost their adult son\n",
      "=  一 对 夫 妇 失 去 他 们 的 儿 子 \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> how a couple would begin to hate each other .\n",
      "=  一 对 夫 妇 如 何 开 始 讨 厌 对 方 。 \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> couple of drug crazy maniacs .\n",
      "=  一 对 嗑 了 毒 品 的 疯 子 。 \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> deuces the winner .\n",
      "=  一 对 二 胜 。 \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> you got all them dimples .\n",
      "=  一 对 可 爱 的 小 酒 窝 。 \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> a couple ? like two people ? like one two people ?\n",
      "=  一 对 夫 妇 ？ 就 像 两 个 人 ？ 就 像 一 个 ， 两 个 人 ？ \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> so does a couple that wants a large family .\n",
      "=  一 对 夫 妇 也 选 了 他 他 们 想 要 个 大 家 庭 。 \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> a couple witnesses saw a tall white male get in a vehicle and leave the scene\n",
      "=  一 对 夫 妇 看 见 一 个 高 个 儿 白 种 男 性 钻 进 车 子 离 开 了 现 场 \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> shot to death in their house .\n",
      "=  一 对 夫 妇 在 自 家 遇 害 。 \n",
      "<  一 对 垃 圾 箱 箱 子   <EOS>\n",
      "\n",
      "> you know a couple was drowning .\n",
      "=  你 懂 的 ， 一 对 夫 妇 溺 水 了 。 \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> a gay dad wants to adopt a crack baby .\n",
      "=  一 对 同 性 恋 父 亲 打 算 收 养 一 个 在 妊 娠 期 摄 取 过 可 卡 因 的 婴 儿 。 \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> how a couple would begin to hate each other .\n",
      "=  一 对 夫 妇 如 何 开 始 讨 厌 对 方 。 \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> for a single mother and daughter .\n",
      "=  一 对 单 身 的 母 女 。 \n",
      "<  一 对 垃 圾 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> pair of underachievers .\n",
      "=  一 对 后 进 生 。 \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n",
      "> a couple witnesses saw a tall white male get in a vehicle and leave the scene\n",
      "=  一 对 夫 妇 看 见 一 个 高 个 儿 白 种 男 性 钻 进 车 子 离 开 了 现 场 \n",
      "<  一 对 垃 圾 箱 箱   潜 水 员 。  <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "m.evaluateRandomly(encoder2,deconder2,inputlang,outputlang,pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load valid data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a pair of red crowned cranes have staked out their nesting territory\n",
      " 一 对 丹 顶 鹤 正 监 视 着 它 们 的 筑 巢 领 地 \n",
      "error: Name error------------------------------!\n",
      "error: Name error------------------------------!\n",
      "en 388091\n",
      "zh 6754\n"
     ]
    }
   ],
   "source": [
    "import validDataProcess as vdp\n",
    "h5py_file = h5py.File('../data/valid_data.h5py','r')\n",
    "valid_pairs = h5py_file['pairs']\n",
    "print(len(pairs))\n",
    "print(pairs[0][0].decode('utf-8'))\n",
    "print(pairs[0][1].decode('gb2312'))\n",
    "\n",
    "valid_inputlang = vdp.Lang('en')\n",
    "valid_outputlang = vdp.Lang('zh')\n",
    "valid_inputlang.load('../data/en_valid1.pkl')\n",
    "valid_outputlang.load('../data/zh_valid1.pkl')\n",
    "\n",
    "####测试用，上面两行注释的语句在真正运行的时候要用到的##################################################################\n",
    "# pairs = pairs[0:1000]\n",
    "# for pair in pairs:\n",
    "#     inputlang.addSentence(pair[0].decode('utf-8'))\n",
    "#     outputlang.addSentence(pair[1].decode('gb2312'))\n",
    "\n",
    "print(valid_inputlang.name,valid_inputlang.n_words)\n",
    "print(valid_outputlang.name,valid_outputlang.n_words)\n",
    "#h5py_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it ll be just like we planned .\n"
     ]
    }
   ],
   "source": [
    "print(pairs[90987][0].decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now if you ll just take a seat we can begin .\n",
      " 好 ， 请 坐 吧 ， 这 样 我 们 就 可 以 开 始 了 。 \n"
     ]
    }
   ],
   "source": [
    "pair = random.choice(pairs)\n",
    "print(pair[0].decode('utf-8'))\n",
    "print(pair[1].decode('gb2312'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "tt = ['我是中国人'.encode('gb2312'),'sdfeow dsf df dsf '.encode('utf-8')]\n",
    "h5 = h5py.File('../data/test1.h5py','w')\n",
    "h5.create_dataset('pairs',data=tt,dtype = 'S400')\n",
    "h5.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count = inputlang.word2count\n",
    "wordFreqLess10 = 0#保存词频小于某个阈值的词\n",
    "wordFreqMore100 = 0\n",
    "all_word_count=0\n",
    "print('input english word count:' , inputlang.n_words)\n",
    "for word in count:\n",
    "    all_word_count += count[word]\n",
    "    if count[word] <= 10:\n",
    "        wordFreqLess10 +=count[word]\n",
    "    if count[word] <= 10:\n",
    "        wordFreqMore100 +=1\n",
    "print('word count larger than 100:',inputlang.n_words - wordFreqMore100)\n",
    "print('word count Less than 10:',wordFreqLess10)\n",
    "print('rate of word count less than 10:', float(wordFreqLess10)/float(all_word_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## filter input language data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testdict = {'a':23,'b':2,'c':4}\n",
    "test2 = {1:'d',34:'f',22:'t'}\n",
    "print(test2)\n",
    "test2.pop(34)\n",
    "print(test2)\n",
    "a = testdict.copy()\n",
    "print(a)\n",
    "for word in testdict:\n",
    "    if testdict[word] < 3:\n",
    "        a.pop(word)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 匹配生僻字与非法字符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def is_rare_name(string):\n",
    "    pattern = re.compile(u\"[~!@#$%^&* ]\")\n",
    "    match = pattern.search(string)\n",
    "    if match:\n",
    "        return True\n",
    "    try:\n",
    "        string.encode(\"gb2312\")\n",
    "    except UnicodeEncodeError:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def normalizeChinese(s):\n",
    "    try:\n",
    "        s.encode(\"gb2312\")\n",
    "    except UnicodeEncodeError:\n",
    "        return ' '\n",
    "    s = re.sub(r\"[~!@#$%^&* ]+\",r' ', s)\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "str1 = '我 是 @ 中 国 人 ， 我 # 不 才 囧 怪。'\n",
    "#str1 = 'I an # a boy'\n",
    "l = [normalizeChinese(s) for s in str1.split(' ')]\n",
    "print(l)\n",
    "tmp = ' '\n",
    "for s in str1.split(' '):\n",
    "    val = normalizeChinese(s)\n",
    "    tmp += val+' '\n",
    "print(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = re.search(r'.','I love FishC.com!')\n",
    "print(result.group())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = '我 是 @ 中 国 人 ， 我 # 不 囧 才 怪。'\n",
    "s = re.sub(r\"[~!@#$%^&* ]+\",r' ', s)\n",
    "print (s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from hyperboard import Agent\n",
    "\n",
    "agent = Agent(port=5100)\n",
    "#agent = Agent(address='172.18.216.69',port=5000)\n",
    "hyperparameters = {'test':0.1}\n",
    "name = agent.register(hyperparameters, 'loss')\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "MAX_LENGTH = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test filter data to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#返回词对应的下标\n",
    "def indexesFromSentence(lang, sentence):\n",
    "    #return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "    result = []\n",
    "    all_lang_keys = lang.word2index.keys()\n",
    "    for word in sentence.split(' '):\n",
    "        if word in all_lang_keys:\n",
    "            result.append(lang.word2index[word])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test12 = 'a boy is watching tree.'\n",
    "test11 = indexesFromSentence(inputlang, test12)\n",
    "print(test11)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testdict = {'a':1,'b':23,'c':4}\n",
    "keys = testdict.keys()\n",
    "list_result = []\n",
    "for word in ['a','f','c','d']:\n",
    "    if word in keys:\n",
    "        list_result.append(testdict[word])\n",
    "print(list_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# process valid data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "import jieba\n",
    "import h5py\n",
    "\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    #通过不断输入sentence（字符串的格式），构建词与下标的对应（词典），方便制作one-hot。\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "    def save(self, path):\n",
    "        with open(path,'wb') as f:\n",
    "            pickle.dump([self.name,self.word2index, self.word2count, self.index2word, self.n_words],f)\n",
    "    \n",
    "    def load(self,path):\n",
    "        with open(path,'rb') as f:\n",
    "            name, self.word2index, self.word2count, self.index2word, self.n_words = pickle.load(f)\n",
    "        if self.name != name:\n",
    "            print('error: Name error------------------------------!')\n",
    "            \n",
    "    def indexesFromSentence(self, sentence):\n",
    "        indexes = []\n",
    "        all_lang_keys = self.word2index.keys()\n",
    "        for word in sentence.split(' '):\n",
    "            if word in all_lang_keys:\n",
    "                indexes.append(self.word2index[word])\n",
    "        indexes.append(EOS_token)\n",
    "        return indexes\n",
    "            \n",
    "##################################################################\n",
    "\n",
    "def readFromFile(path):\n",
    "    file = open(path)\n",
    "    pattern = re.compile('<seg id=\".*?\"> (.*?) </seg>')\n",
    "    result=[]\n",
    "    for line in file.readlines():\n",
    "        #print(line)\n",
    "        item = re.findall(pattern,line)\n",
    "        if item:\n",
    "            #print(item[0])\n",
    "            result.append(item[0])\n",
    "    print('the number of all sentences is ', len(result))\n",
    "    return result\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "def normalizeChinese(s):\n",
    "    try:\n",
    "        s.encode(\"gb2312\")\n",
    "    except UnicodeEncodeError:\n",
    "        return ' '\n",
    "    s = re.sub(r\"[~!@#$%^&* ]+\",r' ', s)\n",
    "    return s\n",
    "\n",
    "\n",
    "#lang1 = 'zh'  lang2 = 'en'\n",
    "#默认英文到中文\n",
    "def readValidLangs(lang1, lang2, reverse=True, fenci=False):\n",
    "    print(\"Reading lines...\")\n",
    "    zh_lines = readFromFile('../data/valid.en-zh.%s.sgm'% lang1)\n",
    "    #zh_lines = zh_lines[0:20]  #for test\n",
    "    \n",
    "    zh_data_list = []\n",
    "    if fenci:\n",
    "        #jieba 分词\n",
    "        for line in zh_lines:\n",
    "            seg_line = jieba.cut(line,cut_all=False)\n",
    "            #dic = [seg for seg in seg_line]\n",
    "            dic = ' '.join(seg_line)\n",
    "            tmp = ' '\n",
    "            for char in dic.split(' '):\n",
    "                val = normalizeChinese(char)\n",
    "                tmp += val+' '\n",
    "            zh_data_list.append(tmp)\n",
    "    else: #用空格按字分开\n",
    "        for line in zh_lines:\n",
    "            dic = ' '.join(line)\n",
    "            tmp = ' '\n",
    "            for char in dic.split(' '):\n",
    "                val = normalizeChinese(char)##去除生僻词\n",
    "                tmp += val+' '\n",
    "            zh_data_list.append(tmp)\n",
    "            \n",
    "    en_lines = readFromFile('../data/valid.en-zh.%s.sgm'% lang2)\n",
    "    #en_lines = en_lines[0:20]  #for test\n",
    "    \n",
    "    # Split every line into pairs and normalize\n",
    "    #去掉一些标点符号\n",
    "    en_data_list = [[normalizeString(s) for s in l.split('\\t')] for l in en_lines]\n",
    "    pairs = []\n",
    "    if reverse:\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "        for en,zh in zip(en_data_list,zh_data_list):\n",
    "            input_lang.addSentence(en[0])\n",
    "            output_lang.addSentence(zh)\n",
    "            pairs.append([en[0].encode('utf-8'),zh.encode('gb2312')])\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "        for en,zh in zip(en_data_list,zh_data_list):\n",
    "            input_lang.addSentence(zh)\n",
    "            output_lang.addSentence(en[0])\n",
    "            pairs.append([zh.encode('gb2312'), en[0].encode('utf-8')])\n",
    "            \n",
    "    return input_lang, output_lang, pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "the number of all sentences is  8000\n",
      "the number of all sentences is  8000\n"
     ]
    }
   ],
   "source": [
    "#read data and process data\n",
    "input_valid, output_valid, pairs = readValidLangs(lang1='zh', lang2='en')\n",
    "\n",
    "#save data\n",
    "input_valid.save('../data/en_valid1.pkl')\n",
    "output_valid.save('../data/zh_valid1.pkl')\n",
    "\n",
    "h5 = h5py.File('../data/valid_data.h5py','w')\n",
    "h5.create_dataset('pairs',data=pairs,dtype = 'S400')\n",
    "h5.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\n",
      "do you think we look young enough to blend in at a high school ?\n",
      " 你 们 觉 得 我 们 看 起 来 够 年 轻 溜 进 高 中 吗 ？ \n",
      "en 8501\n",
      "zh 2956\n"
     ]
    }
   ],
   "source": [
    "import validDataProcess as vdp\n",
    "h5py_file = h5py.File('../data/valid_data.h5py','r')\n",
    "pairs = h5py_file['pairs']\n",
    "print(len(pairs))\n",
    "print(pairs[0][0].decode('utf-8'))\n",
    "print(pairs[0][1].decode('gb2312'))\n",
    "\n",
    "inputlang = vdp.Lang('en')\n",
    "outputlang = vdp.Lang('zh')\n",
    "inputlang.load('../data/en_valid1.pkl')\n",
    "outputlang.load('../data/zh_valid1.pkl')\n",
    "\n",
    "####测试用，上面两行注释的语句在真正运行的时候要用到的##################################################################\n",
    "# pairs = pairs[0:1000]\n",
    "# for pair in pairs:\n",
    "#     inputlang.addSentence(pair[0].decode('utf-8'))\n",
    "#     outputlang.addSentence(pair[1].decode('gb2312'))\n",
    "    \n",
    "print(inputlang.name,inputlang.n_words)\n",
    "print(outputlang.name,outputlang.n_words)\n",
    "#h5py_file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# process test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "import jieba\n",
    "import h5py\n",
    "\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    #通过不断输入sentence（字符串的格式），构建词与下标的对应（词典），方便制作one-hot。\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "    def save(self, path):\n",
    "        with open(path,'wb') as f:\n",
    "            pickle.dump([self.name,self.word2index, self.word2count, self.index2word, self.n_words],f)\n",
    "    \n",
    "    def load(self,path):\n",
    "        with open(path,'rb') as f:\n",
    "            name, self.word2index, self.word2count, self.index2word, self.n_words = pickle.load(f)\n",
    "        if self.name != name:\n",
    "            print('error: Name error------------------------------!')\n",
    "            \n",
    "    def indexesFromSentence(self, sentence):\n",
    "        indexes = []\n",
    "        all_lang_keys = self.word2index.keys()\n",
    "        for word in sentence.split(' '):\n",
    "            if word in all_lang_keys:\n",
    "                indexes.append(self.word2index[word])\n",
    "        indexes.append(EOS_token)\n",
    "        return indexes\n",
    "            \n",
    "##################################################################\n",
    "\n",
    "def readFromFile(path):\n",
    "    file = open(path)\n",
    "    pattern = re.compile('<seg id=\".*?\"> (.*?) </seg>')\n",
    "    result=[]\n",
    "    for line in file.readlines():\n",
    "        #print(line)\n",
    "        item = re.findall(pattern,line)\n",
    "        if item:\n",
    "            #print(item[0])\n",
    "            result.append(item[0])\n",
    "    print('the number of all sentences is ', len(result))\n",
    "    return result\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "def normalizeChinese(s):\n",
    "    try:\n",
    "        s.encode(\"gb2312\")\n",
    "    except UnicodeEncodeError:\n",
    "        return ' '\n",
    "    s = re.sub(r\"[~!@#$%^&* ]+\",r' ', s)\n",
    "    return s\n",
    "\n",
    "\n",
    "#lang1 = 'zh'  lang2 = 'en'\n",
    "#默认英文到中文\n",
    "def readValidLangs(path):\n",
    "    print(\"Reading lines...\")\n",
    "    en_lines = readFromFile(path)\n",
    "    \n",
    "    # Split every line into pairs and normalize\n",
    "    #去掉一些标点符号\n",
    "    en_data_list = [[normalizeString(s) for s in l.split('\\t')] for l in en_lines]\n",
    "    pairs = []\n",
    "    input_lang = Lang('en')\n",
    "    for en in en_data_list:\n",
    "        input_lang.addSentence(en[0])\n",
    "        pairs.append(en[0].encode('utf-8'))\n",
    "        \n",
    "    return input_lang, pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# calculate bleu score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['我', '也', '为', '你', '骄', '傲', '因', '为', '你', '尝', '试', '对', '这', '样', '的', '话', '题', '感', '兴', '趣', '。']\n",
      "['and', 'i', 'm', 'proud', 'of', 'you', 'for', 'trying', 'to', 'be', 'interested', '.']\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "pair =  random.choice(pairs)\n",
    "label_words = nltk.word_tokenize(pair[1].decode('gb2312'))\n",
    "output_words = nltk.word_tokenize(pair[0].decode('utf-8'))\n",
    "print(label_words)\n",
    "print(output_words)\n",
    "belu_score = nltk.translate.bleu_score.sentence_bleu([label_words],label_words)\n",
    "print(belu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "def evaluateValidData(encoder, decoder, inputlang, outputlang, pairs):\n",
    "    predict_words = []\n",
    "    bleu_score = 0.0\n",
    "    for pair in pairs:\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0].decode('utf-8'),inputlang, outputlang)\n",
    "        #output_sentence = ' '.join(output_words)\n",
    "        label_words = nltk.word_tokenize(pair[1].decode('gb2312'))\n",
    "        bleu_score += nltk.translate.bleu_score.sentence_bleu([label_words],output_words)\n",
    "    bleu_score = bleu_score/len(pairs)\n",
    "    return bleu_score\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
