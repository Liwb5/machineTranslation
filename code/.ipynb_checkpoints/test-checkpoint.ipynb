{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "import jieba\n",
    "import h5py\n",
    "\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    #通过不断输入sentence（字符串的格式），构建词与下标的对应（词典），方便制作one-hot。\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "    def save(self, path):\n",
    "        with open(path,'wb') as f:\n",
    "            pickle.dump([self.name,self.word2index, self.word2count, self.index2word, self.n_words],f)\n",
    "    \n",
    "    def load(self,path):\n",
    "        with open(path,'rb') as f:\n",
    "            name, self.word2index, self.word2count, self.index2word, self.n_words = pickle.load(f)\n",
    "        if self.name != name:\n",
    "            print('error: Name error------------------------------!')\n",
    "##################################################################\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "#lang1 = 'zh'  lang2 = 'en'\n",
    "#默认英文到中文\n",
    "def readTrainLangs(lang1, lang2, reverse=True,fenci = False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    zh_lines = open('../data/train.%s'% lang1, encoding='utf-8').read().strip().split('\\n')\n",
    "    zh_lines = zh_lines[0:20]  #for test\n",
    "    zh_data_list = []\n",
    "    if fenci:\n",
    "        #jieba 分词\n",
    "        for line in zh_lines:\n",
    "            seg_line = jieba.cut(line,cut_all=False)\n",
    "            #dic = [seg for seg in seg_line]\n",
    "            dic = ' '.join(seg_line)\n",
    "            zh_data_list.append(dic)\n",
    "    else: #用空格按字分开\n",
    "        for line in zh_lines:\n",
    "            dic = ' '.join(line)\n",
    "            zh_data_list.append(dic)\n",
    "\n",
    "    en_lines = open('../data/train.%s'% lang2, encoding='utf-8').read().strip().split('\\n')\n",
    "    en_lines = en_lines[0:20]  #for test\n",
    "    \n",
    "    # Split every line into pairs and normalize\n",
    "    #去掉一些标点符号\n",
    "    en_data_list = [[normalizeString(s) for s in l.split('\\t')] for l in en_lines]\n",
    "\n",
    "    pairs = []\n",
    "    if reverse:\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "        for en,zh in zip(en_data_list,zh_data_list):\n",
    "            print(zh)\n",
    "            pairs.append([en[0],zh])\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "        for en,zh in zip(en_data_list,zh_data_list):\n",
    "            pairs.append([zh, en[0]])\n",
    "            \n",
    "    return input_lang, output_lang, pairs\n",
    "##################################################\n",
    "\n",
    "#这部分就是对数据进行处理的函数了，上面写的函数都会在这里被调用\n",
    "#最后得到三个变量input_lang，output_lang分别是源语言和目标语言的类，包含它们各自的词典。\n",
    "#pairs是一个列表，列表的元素是一个二元tuple，tuple里面的内容是一句源语言字符串，一句目标语言字符串。\n",
    "def prepareData(lang1, lang2, reverse=True, fenci=False):\n",
    "    input_lang, output_lang, pairs = readTrainLangs(lang1, lang2, reverse, fenci)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    print(pairs[0])\n",
    "    #pairs = filterPairs(pairs)\n",
    "    #print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "一 对 丹 顶 鹤 正 监 视 着 它 们 的 筑 巢 领 地\n",
      "一 对 乌 鸦 飞 到 我 们 屋 顶 上 的 巢 里 ， 它 们 好 像 专 门 为 拉 木 而 来 的 。\n",
      "一 对 乖 乖 仔 开 着 老 爸 的 车 子 。\n",
      "一 对 九 ？ 一 对 九 你 就 全 下 注 了 ？\n",
      "一 对 二 总 不 是 好 事 ，\n",
      "一 对 二 ， 沃 克 传 给 波 顿 。\n",
      "一 对 二 胜 。\n",
      "一 对 五 。 一 对 五 。 胜 算 不 大 啊 。\n",
      "一 对 五 年 没 见 过 的 姐 妹 一 场 激 烈 的 争 吵 ？\n",
      "一 对 五 百 诶 。\n",
      "一 对 五 胜 。\n",
      "一 对 亲 密 的 美 国 兄 妹 ，\n",
      "一 对 人 用 致 命 的 春 药 满 足 欲 望 。\n",
      "一 对 什 么 ？\n",
      "一 对 什 么 ？ 哥 们 儿 是 谁 ？\n",
      "一 对 什 么 ， 我 并 不 知 道 ！\n",
      "一 对 ？ 他 妈 的 什 么 意 思 ？\n",
      "一 对 令 人 信 服 的 已 婚 夫 妇 。\n",
      "一 对 伙 计 们 坐 在 战 场 上 。\n",
      "一 对 住 在 别 墅 里 的 有 钱 夫 妇 ，\n",
      "Read 20 sentence pairs\n",
      "['a pair of red crowned cranes have staked out their nesting territory', b'\\xe4\\xb8\\x80 \\xe5\\xaf\\xb9 \\xe4\\xb8\\xb9 \\xe9\\xa1\\xb6 \\xe9\\xb9\\xa4 \\xe6\\xad\\xa3 \\xe7\\x9b\\x91 \\xe8\\xa7\\x86 \\xe7\\x9d\\x80 \\xe5\\xae\\x83 \\xe4\\xbb\\xac \\xe7\\x9a\\x84 \\xe7\\xad\\x91 \\xe5\\xb7\\xa2 \\xe9\\xa2\\x86 \\xe5\\x9c\\xb0']\n",
      "Counting words...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "a bytes-like object is required, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-915a1806a936>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minputLang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputLang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepareData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'zh'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-43-ed1d7b6b5e42>\u001b[0m in \u001b[0;36mprepareData\u001b[0;34m(lang1, lang2, reverse, fenci)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpair\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0minput_lang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0moutput_lang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Counted words:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_lang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_lang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-43-ed1d7b6b5e42>\u001b[0m in \u001b[0;36maddSentence\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m#通过不断输入sentence（字符串的格式），构建词与下标的对应（词典），方便制作one-hot。\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0maddSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddWord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: a bytes-like object is required, not 'str'"
     ]
    }
   ],
   "source": [
    "inputLang, outputLang, pairs = prepareData('zh','en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "tt = ['thishi sadif dsaf '.encode('utf-8'),'sdfeow dsf df dsf '.encode('utf-8')]\n",
    "testpairs = np.array(pairs)\n",
    "arr  = np.arange(100)\n",
    "h5 = h5py.File('../data/test1.h5py','w')\n",
    "t = h5.create_dataset('pairs',data=tt,dtype = 'S400')\n",
    "h5.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Not a file id (Not a file id)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-d0535acd2798>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mh5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/data/liwb/anaconda3/lib/python3.5/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    285\u001b[0m             \u001b[0;31m# Close file-resident objects first, then the files.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0;31m# Otherwise we get errors in MPI mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m             \u001b[0mid_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_obj_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m~\u001b[0m\u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOBJ_FILE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m             \u001b[0mfile_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_obj_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOBJ_FILE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper (/home/ilan/minonda/conda-bld/h5py_1490027952255/work/h5py/_objects.c:2846)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper (/home/ilan/minonda/conda-bld/h5py_1490027952255/work/h5py/_objects.c:2804)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.get_obj_ids (/home/ilan/minonda/conda-bld/h5py_1490027952255/work/h5py/h5f.c:3425)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Not a file id (Not a file id)"
     ]
    }
   ],
   "source": [
    "h5.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "No conversion path for dtype: dtype('<U4')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-9a17d683ce16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mh5File\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'xxx.h5'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstrList\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'asas'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'asas'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'asas'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mh5File\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'xxx'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'S10'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstrList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mh5File\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/liwb/anaconda3/lib/python3.5/site-packages/h5py/_hl/group.py\u001b[0m in \u001b[0;36mcreate_dataset\u001b[0;34m(self, name, shape, dtype, data, **kwds)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mphil\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             \u001b[0mdsid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_new_dset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m             \u001b[0mdset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/liwb/anaconda3/lib/python3.5/site-packages/h5py/_hl/dataset.py\u001b[0m in \u001b[0;36mmake_new_dset\u001b[0;34m(parent, shape, dtype, data, chunks, compression, shuffle, fletcher32, maxshape, compression_opts, fillvalue, scaleoffset, track_times)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mdset_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh5s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mALL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mALL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdset_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper (/home/ilan/minonda/conda-bld/h5py_1490027952255/work/h5py/_objects.c:2846)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper (/home/ilan/minonda/conda-bld/h5py_1490027952255/work/h5py/_objects.c:2804)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5d.pyx\u001b[0m in \u001b[0;36mh5py.h5d.DatasetID.write (/home/ilan/minonda/conda-bld/h5py_1490027952255/work/h5py/h5d.c:3609)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5t.pyx\u001b[0m in \u001b[0;36mh5py.h5t.py_create (/home/ilan/minonda/conda-bld/h5py_1490027952255/work/h5py/h5t.c:17938)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5t.pyx\u001b[0m in \u001b[0;36mh5py.h5t.py_create (/home/ilan/minonda/conda-bld/h5py_1490027952255/work/h5py/h5t.c:17897)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: No conversion path for dtype: dtype('<U4')"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "h5File=h5py.File('xxx.h5','w')\n",
    "strList=['asas','asas','asas']  \n",
    "h5File.create_dataset('xxx',(len(strList),1),'S10',strList)\n",
    " \n",
    "h5File.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h5File.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputLang.save('../data/test.pkl')\n",
    "\n",
    "lang = Lang('en')\n",
    "lang.load('../data/test.pkl')\n",
    "\n",
    "print(lang.word2count)\n",
    "\n",
    "print(outputLang.word2count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "pairs1 = {'我':1,'人':2}\n",
    "pairs2 = [1,2,3,4]\n",
    "pairs3 = 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle_file = open('../data/test.pkl','wb')\n",
    "pickle.dump([pairs1,pairs2,pairs3], pickle_file)\n",
    "pickle_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'人': 2, '我': 1} [1, 2, 3, 4] 90\n"
     ]
    }
   ],
   "source": [
    "f = open('../data/test.pkl','rb')\n",
    "a1,a2,a3 = pickle.load(f)\n",
    "print(a1,a2,a3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Dump cache file failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/liwb/anaconda3/lib/python3.5/site-packages/jieba/__init__.py\", line 152, in initialize\n",
      "    _replace_file(fpath, cache_file)\n",
      "PermissionError: [Errno 1] Operation not permitted: '/tmp/tmpoz_v2tcn' -> '/tmp/jieba.cache'\n",
      "Loading model cost 1.016 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "import dataProcess as dp\n",
    "pairs =  dp.readTrainLangs('zh','en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "pickle_file = open('../data/train_pairs.pkl','rb')\n",
    "pairs_pkl = pickle.load(pickle_file)\n",
    "\n",
    "print(pairs_pkl['a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['一对', '丹顶鹤', '正', '监视', '着', '它们', '的', '筑巢', '领地'],\n",
       "  'a pair of red crowned cranes have staked out their nesting territory'],\n",
       " [['一对',\n",
       "   '乌鸦',\n",
       "   '飞',\n",
       "   '到',\n",
       "   '我们',\n",
       "   '屋顶',\n",
       "   '上',\n",
       "   '的',\n",
       "   '巢里',\n",
       "   '，',\n",
       "   '它们',\n",
       "   '好像',\n",
       "   '专门',\n",
       "   '为拉木',\n",
       "   '而',\n",
       "   '来',\n",
       "   '的',\n",
       "   '。'],\n",
       "  'a pair of crows had come to nest on our roof as if they had come for lhamo .'],\n",
       " [['一对', '乖乖仔', '开着', '老爸', '的', '车子', '。'],\n",
       "  'a couple of boys driving around in daddy s car .'],\n",
       " [['一对', '九', '？', '一对', '九', '你', '就', '全', '下注', '了', '？'],\n",
       "  'a pair of nines ? you pushed in with a pair of nines ?'],\n",
       " [['一对二', '总', '不是', '好事', '，'], 'fighting two against one is never ideal '],\n",
       " [['一对二', '，', '沃克', '传给', '波顿', '。'],\n",
       "  'it s a neat one two . walker to burton .'],\n",
       " [['一对二', '胜', '。'], 'deuces the winner .'],\n",
       " [['一对', '五', '。', '一对', '五', '。', '胜算', '不大', '啊', '。'],\n",
       "  'five on one . five on one . yeah not the greatest odds .'],\n",
       " [['一对', '五年', '没见', '过', '的', '姐妹', '一场', '激烈', '的', '争吵', '？'],\n",
       "  'an incredibly emotional fight between sisters '],\n",
       " [['一对', '五百', '诶', '。'], 'one against .']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs_pkl[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
