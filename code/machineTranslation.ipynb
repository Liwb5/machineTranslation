{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "import jieba\n",
    "import h5py\n",
    "\n",
    "SOS_token = 0\n",
    "EOS_token = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    #通过不断输入sentence（字符串的格式），构建词与下标的对应（词典），方便制作one-hot。\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "    def save(self, path):\n",
    "        with open(path,'wb') as f:\n",
    "            pickle.dump([self.name,self.word2index, self.word2count, self.index2word, self.n_words],f)\n",
    "    \n",
    "    def load(self,path):\n",
    "        with open(path,'rb') as f:\n",
    "            name, self.word2index, self.word2count, self.index2word, self.n_words = pickle.load(f)\n",
    "        if self.name != name:\n",
    "            print('error: Name error------------------------------!')\n",
    "            \n",
    "            \n",
    "##################################################################\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "def normalizeChinese(s):\n",
    "    try:\n",
    "        s.encode(\"gb2312\")\n",
    "    except UnicodeEncodeError:\n",
    "        return ' '\n",
    "    s = re.sub(r\"[~!@#$%^&* ]+\",r' ', s)\n",
    "    return s\n",
    "\n",
    "\n",
    "#lang1 = 'zh'  lang2 = 'en'\n",
    "#默认英文到中文\n",
    "def readTrainLangs(lang1, lang2, reverse=True,fenci = False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    zh_lines = open('../data/train.%s'% lang1).read().strip().split('\\n')\n",
    "    #zh_lines = zh_lines[0:20]  #for test\n",
    "\n",
    "    zh_data_list = []\n",
    "    if fenci:\n",
    "        #jieba 分词\n",
    "        for line in zh_lines:\n",
    "            seg_line = jieba.cut(line,cut_all=False)\n",
    "            #dic = [seg for seg in seg_line]\n",
    "            dic = ' '.join(seg_line)\n",
    "            tmp = ' '\n",
    "            for char in dic.split(' '):\n",
    "                val = normalizeChinese(char)\n",
    "                tmp += val+' '\n",
    "            zh_data_list.append(tmp)\n",
    "    else: #用空格按字分开\n",
    "        for line in zh_lines:\n",
    "            dic = ' '.join(line)\n",
    "            tmp = ' '\n",
    "            for char in dic.split(' '):\n",
    "                val = normalizeChinese(char)##去除生僻词\n",
    "                tmp += val+' '\n",
    "            zh_data_list.append(tmp)\n",
    "\n",
    "    en_lines = open('../data/train.%s'% lang2).read().strip().split('\\n')\n",
    "    #en_lines = en_lines[0:20]  #for test\n",
    "    \n",
    "    # Split every line into pairs and normalize\n",
    "    #去掉一些标点符号\n",
    "    en_data_list = [[normalizeString(s) for s in l.split('\\t')] for l in en_lines]\n",
    "    pairs = []\n",
    "    if reverse:\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "        for en,zh in zip(en_data_list,zh_data_list):\n",
    "            input_lang.addSentence(en[0])\n",
    "            output_lang.addSentence(zh)\n",
    "            pairs.append([en[0].encode('utf-8'),zh.encode('gb2312')])\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "        for en,zh in zip(en_data_list,zh_data_list):\n",
    "            input_lang.addSentence(zh)\n",
    "            output_lang.addSentence(en[0])\n",
    "            pairs.append([zh.encode('gb2312'), en[0].encode('utf-8')])\n",
    "            \n",
    "    return input_lang, output_lang, pairs\n",
    "##################################################\n",
    "\n",
    "#这部分就是对数据进行处理的函数了，上面写的函数都会在这里被调用\n",
    "#最后得到三个变量input_lang，output_lang分别是源语言和目标语言的类，包含它们各自的词典。\n",
    "#pairs是一个列表，列表的元素是一个二元tuple，tuple里面的内容是一句源语言字符串，一句目标语言字符串。\n",
    "def prepareData(lang1, lang2, reverse=True, fenci=False):\n",
    "    input_lang, output_lang, pairs = readTrainLangs(lang1, lang2, reverse, fenci)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    print(pairs[0][0].decode('utf-8'),pairs[0][1].decode('gb2312'))\n",
    "    #pairs = filterPairs(pairs)\n",
    "    #print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "#     for pair in pairs:\n",
    "#         input_lang.addSentence(pair[0].decode('utf-8'))\n",
    "#         output_lang.addSentence(pair[1].decode('gb2312'))\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputLang, outputLang, pairs = prepareData('zh','en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputLang.save('../data/en_train.pkl')\n",
    "outputLang.save('../data/zh_train.pkl')\n",
    "\n",
    "h5 = h5py.File('../data/train_afterProcess.h5py','w')\n",
    "h5.create_dataset('pairs',data=pairs,dtype = 'S400')\n",
    "h5.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a pair of red crowned cranes have staked out their nesting territory\n",
      " 一 对 丹 顶 鹤 正 监 视 着 它 们 的 筑 巢 领 地 \n",
      "en 1842\n",
      "zh 1408\n"
     ]
    }
   ],
   "source": [
    "import dataProcess as dp\n",
    "h5py_file = h5py.File('../data/train_afterProcess.h5py','r')\n",
    "pairs = h5py_file['pairs']\n",
    "\n",
    "pairs = pairs[0:1000]\n",
    "print(pairs[0][0].decode('utf-8'))\n",
    "print(pairs[0][1].decode('gb2312'))\n",
    "\n",
    "inputlang = dp.Lang('en')\n",
    "outputlang = dp.Lang('zh')\n",
    "# inputlang.load('../data/en_train.pkl')\n",
    "# outputlang.load('../data/zh_train.pkl')\n",
    "\n",
    "####测试用，上面两行注释的语句在真正运行的时候要用到的##################################################################\n",
    "for pair in pairs:\n",
    "    inputlang.addSentence(pair[0].decode('utf-8'))\n",
    "    outputlang.addSentence(pair[1].decode('gb2312'))\n",
    "    \n",
    "print(inputlang.name,inputlang.n_words)\n",
    "print(outputlang.name,outputlang.n_words)\n",
    "#h5py_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter -- remove words that appear less than 10 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "388089\n",
      "388089\n",
      "388091\n",
      "82101\n",
      "82101\n",
      "82103\n"
     ]
    }
   ],
   "source": [
    "print(len(inputlang.word2count))\n",
    "print(len(inputlang.word2index))\n",
    "print(len(inputlang.index2word))\n",
    "all_en_words = inputlang.word2count.copy()\n",
    "for word in all_en_words:\n",
    "    if  all_en_words[word] <= 10:\n",
    "        inputlang.word2count.pop(word)\n",
    "        index = inputlang.word2index[word]\n",
    "        inputlang.word2index.pop(word)\n",
    "        inputlang.index2word.pop(index)\n",
    "        \n",
    "print(len(inputlang.word2count))\n",
    "print(len(inputlang.word2index))\n",
    "print(len(inputlang.index2word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from hyperboard import Agent\n",
    "\n",
    "agent = Agent(address='127.0.0.1',port=5000)\n",
    "#agent = Agent(address='172.18.216.69',port=5000)\n",
    "hyperparameters = {'learning rate':0.01}\n",
    "name = agent.register(hyperparameters, 'loss',overwrite=True)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "MAX_LENGTH = 30\n",
    "teacher_forcing_ratio = 1  #在训练时解码器使用labels（平行预料）进行训练的概率\n",
    "LEARNING_RATE = 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    #input_size是指词典的大小(毕竟要建立embedding)，hidden_size是hidden_state的维度\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    #input是一个句子(这个句子已经通过数据处理的类转换成下标，这样可以对应一个embedded)\n",
    "    #hidden 是上一个迭代中的hidden，即pre_hidden\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        #这个n_layers==1其实就是只相当于一个cell，对一个input(单词)和上一个hidden state\n",
    "        #这里做了一个gru操作。n_layers大于1则是对同一个东西迭代多次，也许效果会好。\n",
    "        for i in range(self.n_layers):\n",
    "            output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result\n",
    "        \n",
    "#hidden_size都是说hidden_state的维度，要和encoder一致。\n",
    "#output_size是目标语言的词典大小，因为输出的是所有词的概率\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, n_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        for i in range(self.n_layers):\n",
    "            output = F.relu(output)\n",
    "            output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result\n",
    "        \n",
    "#hidden_size都是说hidden_state的维度，要和encoder一致。\n",
    "#output_size是目标语言的词典大小，因为输出的是所有词的概率\n",
    "#max_length是句子的最大长度(之前被限制了，以后看看能否不要这个限制)\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, n_layers=1, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    #input是一个目标句子中的某个词(这个词已经通过数据处理的类转换成下标，这样可以对应一个embedded)。\n",
    "    #当然，在进行预测的时候就不会是输入目标句子的词了。而是它预测出来的词\n",
    "    #hidden 是上一个迭代中的hidden，即pre_hidden\n",
    "    #encoder_output是encoder最后一个输出，不过在这里它并没有被使用到\n",
    "    #encoder_outputs是encoder每次输出(y1,y2,...,yn)的组成tensor，格式跟input一样，只不过它是句子，而不是某个词。\n",
    "    def forward(self, input, hidden, encoder_output, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)))\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        for i in range(self.n_layers):\n",
    "            output = F.relu(output)\n",
    "            output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]))#每个词的概率\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将数据进行处理成pytorch变量，方便转换成embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#########################training#####################################\n",
    "#返回词对应的下标\n",
    "def indexesFromSentence(lang, sentence):\n",
    "    #return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "    result = []\n",
    "    all_lang_keys = lang.word2index.keys()\n",
    "    for word in sentence.split(' '):\n",
    "        if word in all_lang_keys:#判断词是否在词典中，因为词典中有些出现次数太少的词被删掉了\n",
    "            result.append(lang.word2index[word])\n",
    "    return result\n",
    "\n",
    "#将词转换成variable\n",
    "def variableFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    result = Variable(torch.LongTensor(indexes).view(-1, 1))\n",
    "    if use_cuda:\n",
    "        return result.cuda()\n",
    "    else:\n",
    "        return result\n",
    "\n",
    "    \n",
    "def variablesFromPair(pair,input_lang, output_lang):\n",
    "    #注意这里要先解码，因为保存到h5py里面的时候要编码，所以现在要解码\n",
    "    input_variable = variableFromSentence(input_lang, pair[0].decode('utf-8'))\n",
    "    target_variable = variableFromSentence(output_lang, pair[1].decode('gb2312'))\n",
    "    return (input_variable, target_variable)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义训练所需要的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(input_variable, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    #input_variable已经在函数外变成了tensor了，tensor的元素是词的下标\n",
    "    input_length = input_variable.size()[0]#source sentence 的长度\n",
    "    target_length = target_variable.size()[0]#目标句子的长度\n",
    "\n",
    "    encoder_outputs = Variable(torch.zeros(max_length, encoder.hidden_size))#max_length=10，也就是句子的最长长度，hidden_size是256，所以encoder_outputs是矩阵10X256\n",
    "    encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):#句子有多长就迭代多少次\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_variable[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0][0]#将每个词encoder的output记录下来\n",
    "\n",
    "    decoder_input = Variable(torch.LongTensor([[SOS_token]]))\n",
    "    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "    decoder_hidden = encoder_hidden#encoder最后一层的hidden_state传给decoder作为decoder的第一个hidden_state\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "        \t#encoder_outputs作为decoder的输入，是为了改变attention。\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_output, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_variable[di])#这两个变量是什么形式\n",
    "            decoder_input = target_variable[di]  # Teacher forcing这个是直接给答案，也就是一个单词，进入decoder里面再变成词向量\n",
    "\n",
    "    else:#这边是不直接给答案，而是每次output那里选择概率最大的作为下一个输入\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_output, encoder_outputs)\n",
    "            #从decoder的类中可以知道，decoder_output是softmax出来的，即所有词的概率。\n",
    "            #topk函数是查找最大的K 个数，这里参数是1，topv就是value，topi是index，也就是词对应的下标\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            ni = topi[0][0]\n",
    "\n",
    "            decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "            decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "            loss += criterion(decoder_output, target_variable[di])\n",
    "            if ni == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()#如何理解这一步反向梯度对encoder和decoder都有效\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.data[0] / target_length\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# This is a helper function to print time elapsed and estimated time\n",
    "# remaining given the current time and progress %.\n",
    "#\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent) #总时间\n",
    "    rs = es - s        #总时间减去已经运行的时间等于还剩下的时间\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "######################################################################\n",
    "# The whole training process looks like this:\n",
    "#\n",
    "# -  Start a timer\n",
    "# -  Initialize optimizers and criterion\n",
    "# -  Create set of training pairs\n",
    "# -  Start empty losses array for plotting\n",
    "#\n",
    "# Then we call ``train`` many times and occasionally print the progress (%\n",
    "# of examples, time so far, estimated time) and average loss.\n",
    "#\n",
    "\n",
    "def trainIters(encoder, decoder, inputlang, outputlang, n_iters, print_every=1000, plot_every=100, learning_rate=0.01, save_model_every=10000):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    #考虑改成其他的优化器\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    #这里的training_pairs经过variableFromPair处理后，每个元素已经是一个tensor了，并且是单词所在的下标，为了可以和embedd匹配。\n",
    "    #training_pairs = [variablesFromPair(random.choice(pairs))\n",
    "    #                 for i in range(n_iters)]\n",
    "    #print(random.choice(training_pairs)[0].data)\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        #training_pair = training_pairs[iter - 1]\n",
    "        #################@#%…………&&&\n",
    "        \n",
    "        training_pair = variablesFromPair(random.choice(pairs),inputlang,outputlang)\n",
    "        input_variable = training_pair[0]\n",
    "        target_variable = training_pair[1]\n",
    "\n",
    "        loss = train(input_variable, target_variable, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            agent.append(name, iter, plot_loss_avg)\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "            \n",
    "        if iter % save_model_every == 0:\n",
    "            torch.save(encoder.state_dict(),'../models/encoder.model{0}'.format(iter))\n",
    "            torch.save(decoder.state_dict(),'../models/decoder.model{0}'.format(iter))\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### real training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 6s (- 90m 12s) (100 0%) 4.6545\n",
      "0m 13s (- 89m 25s) (200 0%) 4.0698\n",
      "0m 21s (- 94m 39s) (300 0%) 3.8483\n",
      "0m 28s (- 96m 7s) (400 0%) 3.7090\n",
      "0m 37s (- 100m 30s) (500 0%) 3.6727\n",
      "0m 46s (- 103m 39s) (600 0%) 3.7431\n",
      "0m 54s (- 102m 55s) (700 0%) 3.5381\n",
      "1m 2s (- 102m 57s) (800 1%) 3.4063\n",
      "1m 10s (- 103m 4s) (900 1%) 3.3690\n",
      "1m 18s (- 102m 56s) (1000 1%) 3.3279\n",
      "1m 26s (- 102m 56s) (1100 1%) 3.2043\n",
      "1m 34s (- 103m 18s) (1200 1%) 3.2180\n",
      "1m 42s (- 103m 16s) (1300 1%) 3.3067\n",
      "1m 49s (- 102m 22s) (1400 1%) 3.1445\n",
      "1m 56s (- 101m 34s) (1500 1%) 3.1311\n",
      "2m 3s (- 101m 14s) (1600 2%) 3.1672\n",
      "2m 11s (- 100m 52s) (1700 2%) 2.9504\n",
      "2m 19s (- 101m 3s) (1800 2%) 3.1100\n",
      "2m 27s (- 101m 20s) (1900 2%) 2.9769\n",
      "2m 37s (- 102m 3s) (2000 2%) 3.1005\n",
      "2m 45s (- 102m 9s) (2100 2%) 2.8924\n",
      "2m 53s (- 102m 8s) (2200 2%) 2.8906\n",
      "3m 1s (- 101m 58s) (2300 2%) 2.8482\n",
      "3m 9s (- 102m 4s) (2400 3%) 2.9255\n",
      "3m 17s (- 101m 58s) (2500 3%) 2.8514\n",
      "3m 24s (- 101m 29s) (2600 3%) 2.6627\n",
      "3m 32s (- 101m 21s) (2700 3%) 2.6850\n",
      "3m 40s (- 101m 17s) (2800 3%) 2.7400\n",
      "3m 48s (- 101m 8s) (2900 3%) 2.8070\n",
      "3m 56s (- 101m 7s) (3000 3%) 2.5938\n",
      "4m 3s (- 100m 47s) (3100 3%) 2.6292\n",
      "4m 11s (- 100m 47s) (3200 4%) 2.5911\n",
      "4m 20s (- 100m 48s) (3300 4%) 2.4693\n",
      "4m 27s (- 100m 32s) (3400 4%) 2.3633\n",
      "4m 34s (- 100m 8s) (3500 4%) 2.3536\n",
      "4m 42s (- 99m 53s) (3600 4%) 2.3615\n",
      "4m 49s (- 99m 32s) (3700 4%) 2.3078\n",
      "4m 57s (- 99m 25s) (3800 4%) 2.4936\n",
      "5m 5s (- 99m 15s) (3900 4%) 2.1777\n",
      "5m 12s (- 98m 58s) (4000 5%) 2.1516\n",
      "5m 19s (- 98m 43s) (4100 5%) 2.0761\n",
      "5m 27s (- 98m 38s) (4200 5%) 2.2419\n",
      "5m 35s (- 98m 31s) (4300 5%) 2.0791\n",
      "5m 43s (- 98m 23s) (4400 5%) 2.1302\n",
      "5m 50s (- 98m 0s) (4500 5%) 2.0551\n",
      "5m 58s (- 97m 58s) (4600 5%) 2.2165\n",
      "6m 6s (- 97m 54s) (4700 5%) 1.9989\n",
      "6m 14s (- 97m 49s) (4800 6%) 2.0056\n",
      "6m 23s (- 97m 51s) (4900 6%) 2.0434\n",
      "6m 30s (- 97m 41s) (5000 6%) 1.9228\n",
      "6m 38s (- 97m 31s) (5100 6%) 1.9472\n",
      "6m 45s (- 97m 17s) (5200 6%) 1.7663\n",
      "6m 53s (- 97m 14s) (5300 6%) 1.9072\n",
      "7m 1s (- 97m 3s) (5400 6%) 1.9535\n",
      "7m 9s (- 96m 52s) (5500 6%) 1.8142\n",
      "7m 17s (- 96m 57s) (5600 7%) 1.8174\n",
      "7m 25s (- 96m 52s) (5700 7%) 1.7616\n",
      "7m 34s (- 96m 52s) (5800 7%) 1.6451\n",
      "7m 43s (- 96m 55s) (5900 7%) 1.3543\n",
      "7m 51s (- 96m 53s) (6000 7%) 1.7383\n",
      "7m 58s (- 96m 42s) (6100 7%) 1.6377\n",
      "8m 7s (- 96m 43s) (6200 7%) 1.4615\n",
      "8m 15s (- 96m 40s) (6300 7%) 1.5932\n",
      "8m 22s (- 96m 24s) (6400 8%) 1.5459\n",
      "8m 30s (- 96m 15s) (6500 8%) 1.6861\n",
      "8m 38s (- 96m 9s) (6600 8%) 1.4034\n",
      "8m 46s (- 95m 57s) (6700 8%) 1.4693\n",
      "8m 53s (- 95m 41s) (6800 8%) 1.4509\n",
      "9m 1s (- 95m 36s) (6900 8%) 1.4052\n",
      "9m 8s (- 95m 19s) (7000 8%) 1.5163\n",
      "9m 15s (- 95m 4s) (7100 8%) 1.3315\n",
      "9m 23s (- 94m 54s) (7200 9%) 1.3998\n",
      "9m 31s (- 94m 48s) (7300 9%) 1.3785\n",
      "9m 38s (- 94m 32s) (7400 9%) 1.2924\n",
      "9m 45s (- 94m 20s) (7500 9%) 1.3100\n",
      "9m 52s (- 94m 7s) (7600 9%) 1.2872\n",
      "10m 0s (- 93m 53s) (7700 9%) 1.2615\n",
      "10m 7s (- 93m 42s) (7800 9%) 1.1615\n",
      "10m 13s (- 93m 21s) (7900 9%) 1.2492\n",
      "10m 20s (- 93m 7s) (8000 10%) 1.2339\n",
      "10m 27s (- 92m 54s) (8100 10%) 1.1746\n",
      "10m 35s (- 92m 43s) (8200 10%) 1.2125\n",
      "10m 43s (- 92m 34s) (8300 10%) 1.1797\n",
      "10m 50s (- 92m 25s) (8400 10%) 1.0161\n",
      "10m 57s (- 92m 14s) (8500 10%) 1.0939\n",
      "11m 5s (- 92m 9s) (8600 10%) 0.9538\n",
      "11m 11s (- 91m 45s) (8700 10%) 1.0247\n",
      "11m 17s (- 91m 23s) (8800 11%) 1.1916\n",
      "11m 24s (- 91m 4s) (8900 11%) 1.0004\n",
      "11m 30s (- 90m 45s) (9000 11%) 0.9232\n",
      "11m 36s (- 90m 25s) (9100 11%) 0.9471\n",
      "11m 42s (- 90m 2s) (9200 11%) 0.9669\n",
      "11m 48s (- 89m 46s) (9300 11%) 0.9388\n",
      "11m 59s (- 90m 2s) (9400 11%) 0.9838\n",
      "12m 8s (- 90m 4s) (9500 11%) 0.9501\n",
      "12m 19s (- 90m 21s) (9600 12%) 1.0208\n",
      "12m 29s (- 90m 31s) (9700 12%) 0.9591\n",
      "12m 37s (- 90m 28s) (9800 12%) 0.9278\n",
      "12m 46s (- 90m 27s) (9900 12%) 0.8562\n",
      "12m 54s (- 90m 18s) (10000 12%) 0.9597\n",
      "13m 2s (- 90m 15s) (10100 12%) 0.9022\n",
      "13m 13s (- 90m 31s) (10200 12%) 0.8395\n",
      "13m 21s (- 90m 26s) (10300 12%) 0.8479\n",
      "13m 34s (- 90m 49s) (10400 13%) 0.7535\n",
      "13m 42s (- 90m 41s) (10500 13%) 0.8433\n",
      "13m 52s (- 90m 53s) (10600 13%) 0.8169\n",
      "14m 0s (- 90m 43s) (10700 13%) 0.7978\n",
      "14m 8s (- 90m 35s) (10800 13%) 0.7848\n",
      "14m 15s (- 90m 23s) (10900 13%) 0.7011\n",
      "14m 22s (- 90m 7s) (11000 13%) 0.6996\n",
      "14m 31s (- 90m 10s) (11100 13%) 0.7246\n",
      "14m 40s (- 90m 6s) (11200 14%) 0.7229\n",
      "14m 50s (- 90m 14s) (11300 14%) 0.7097\n",
      "14m 58s (- 90m 7s) (11400 14%) 0.6441\n",
      "15m 7s (- 90m 7s) (11500 14%) 0.7591\n",
      "15m 17s (- 90m 12s) (11600 14%) 0.6142\n",
      "15m 24s (- 89m 59s) (11700 14%) 0.7250\n",
      "15m 33s (- 89m 56s) (11800 14%) 0.6949\n",
      "15m 41s (- 89m 46s) (11900 14%) 0.6825\n",
      "15m 48s (- 89m 32s) (12000 15%) 0.7199\n",
      "15m 55s (- 89m 19s) (12100 15%) 0.6463\n",
      "16m 3s (- 89m 17s) (12200 15%) 0.5897\n",
      "16m 14s (- 89m 21s) (12300 15%) 0.5822\n",
      "16m 23s (- 89m 21s) (12400 15%) 0.6279\n",
      "16m 32s (- 89m 20s) (12500 15%) 0.5230\n",
      "16m 42s (- 89m 21s) (12600 15%) 0.6245\n",
      "16m 48s (- 89m 4s) (12700 15%) 0.5669\n",
      "16m 57s (- 89m 1s) (12800 16%) 0.6243\n",
      "17m 6s (- 88m 59s) (12900 16%) 0.5175\n",
      "17m 15s (- 88m 57s) (13000 16%) 0.4628\n",
      "17m 25s (- 89m 1s) (13100 16%) 0.5028\n",
      "17m 35s (- 89m 0s) (13200 16%) 0.5482\n",
      "17m 44s (- 88m 58s) (13300 16%) 0.5731\n",
      "17m 53s (- 88m 55s) (13400 16%) 0.5241\n",
      "18m 4s (- 89m 4s) (13500 16%) 0.4594\n",
      "18m 14s (- 89m 1s) (13600 17%) 0.5504\n",
      "18m 21s (- 88m 50s) (13700 17%) 0.5149\n",
      "18m 30s (- 88m 45s) (13800 17%) 0.5154\n",
      "18m 39s (- 88m 44s) (13900 17%) 0.4704\n",
      "18m 46s (- 88m 29s) (14000 17%) 0.4461\n",
      "18m 52s (- 88m 11s) (14100 17%) 0.3805\n",
      "18m 59s (- 88m 0s) (14200 17%) 0.4808\n",
      "19m 6s (- 87m 48s) (14300 17%) 0.4950\n",
      "19m 12s (- 87m 32s) (14400 18%) 0.3961\n",
      "19m 19s (- 87m 16s) (14500 18%) 0.4512\n",
      "19m 25s (- 87m 0s) (14600 18%) 0.3848\n",
      "19m 31s (- 86m 42s) (14700 18%) 0.4294\n",
      "19m 39s (- 86m 35s) (14800 18%) 0.4035\n",
      "19m 46s (- 86m 25s) (14900 18%) 0.3646\n",
      "19m 55s (- 86m 19s) (15000 18%) 0.4003\n",
      "20m 2s (- 86m 8s) (15100 18%) 0.4530\n",
      "20m 9s (- 85m 56s) (15200 19%) 0.3922\n",
      "20m 17s (- 85m 47s) (15300 19%) 0.3426\n",
      "20m 23s (- 85m 33s) (15400 19%) 0.3494\n",
      "20m 30s (- 85m 19s) (15500 19%) 0.3314\n",
      "20m 38s (- 85m 10s) (15600 19%) 0.3613\n",
      "20m 47s (- 85m 7s) (15700 19%) 0.3289\n",
      "20m 54s (- 84m 58s) (15800 19%) 0.3192\n",
      "21m 2s (- 84m 48s) (15900 19%) 0.3595\n",
      "21m 13s (- 84m 52s) (16000 20%) 0.3296\n",
      "21m 20s (- 84m 40s) (16100 20%) 0.3212\n",
      "21m 28s (- 84m 32s) (16200 20%) 0.2975\n",
      "21m 37s (- 84m 30s) (16300 20%) 0.3046\n",
      "21m 49s (- 84m 36s) (16400 20%) 0.3081\n",
      "21m 56s (- 84m 25s) (16500 20%) 0.3243\n",
      "22m 2s (- 84m 12s) (16600 20%) 0.3817\n",
      "22m 8s (- 83m 56s) (16700 20%) 0.3220\n",
      "22m 14s (- 83m 40s) (16800 21%) 0.2562\n",
      "22m 20s (- 83m 24s) (16900 21%) 0.2995\n",
      "22m 26s (- 83m 8s) (17000 21%) 0.2595\n",
      "22m 32s (- 82m 54s) (17100 21%) 0.2847\n",
      "22m 39s (- 82m 43s) (17200 21%) 0.3143\n",
      "22m 46s (- 82m 32s) (17300 21%) 0.2490\n",
      "22m 55s (- 82m 29s) (17400 21%) 0.2728\n",
      "23m 4s (- 82m 23s) (17500 21%) 0.3102\n",
      "23m 12s (- 82m 15s) (17600 22%) 0.2451\n",
      "23m 19s (- 82m 5s) (17700 22%) 0.3248\n",
      "23m 26s (- 81m 55s) (17800 22%) 0.2467\n",
      "23m 34s (- 81m 47s) (17900 22%) 0.2608\n",
      "23m 43s (- 81m 44s) (18000 22%) 0.2389\n",
      "23m 51s (- 81m 34s) (18100 22%) 0.2573\n",
      "23m 58s (- 81m 25s) (18200 22%) 0.2578\n",
      "24m 6s (- 81m 17s) (18300 22%) 0.2051\n",
      "24m 13s (- 81m 6s) (18400 23%) 0.2420\n",
      "24m 21s (- 81m 0s) (18500 23%) 0.2276\n",
      "24m 31s (- 80m 57s) (18600 23%) 0.2205\n",
      "24m 38s (- 80m 46s) (18700 23%) 0.2146\n",
      "24m 45s (- 80m 35s) (18800 23%) 0.2242\n",
      "24m 53s (- 80m 26s) (18900 23%) 0.1935\n",
      "25m 0s (- 80m 16s) (19000 23%) 0.2061\n",
      "25m 8s (- 80m 8s) (19100 23%) 0.2270\n",
      "25m 16s (- 80m 1s) (19200 24%) 0.1934\n",
      "25m 23s (- 79m 50s) (19300 24%) 0.2175\n",
      "25m 30s (- 79m 40s) (19400 24%) 0.1954\n",
      "25m 37s (- 79m 29s) (19500 24%) 0.1738\n",
      "25m 46s (- 79m 25s) (19600 24%) 0.1834\n",
      "25m 54s (- 79m 19s) (19700 24%) 0.1814\n",
      "26m 2s (- 79m 10s) (19800 24%) 0.1452\n",
      "26m 9s (- 78m 59s) (19900 24%) 0.1661\n",
      "26m 15s (- 78m 47s) (20000 25%) 0.1465\n",
      "26m 22s (- 78m 36s) (20100 25%) 0.1546\n",
      "26m 29s (- 78m 26s) (20200 25%) 0.1201\n",
      "26m 37s (- 78m 17s) (20300 25%) 0.1452\n",
      "26m 44s (- 78m 8s) (20400 25%) 0.1532\n",
      "26m 50s (- 77m 55s) (20500 25%) 0.1782\n",
      "26m 57s (- 77m 43s) (20600 25%) 0.1285\n",
      "27m 6s (- 77m 38s) (20700 25%) 0.1488\n",
      "27m 15s (- 77m 34s) (20800 26%) 0.1461\n",
      "27m 21s (- 77m 22s) (20900 26%) 0.1353\n",
      "27m 29s (- 77m 14s) (21000 26%) 0.1465\n",
      "27m 38s (- 77m 10s) (21100 26%) 0.1251\n",
      "27m 46s (- 77m 2s) (21200 26%) 0.1152\n",
      "27m 52s (- 76m 50s) (21300 26%) 0.1180\n",
      "28m 3s (- 76m 48s) (21400 26%) 0.1371\n",
      "28m 12s (- 76m 45s) (21500 26%) 0.1247\n",
      "28m 22s (- 76m 44s) (21600 27%) 0.1015\n",
      "28m 32s (- 76m 41s) (21700 27%) 0.0974\n",
      "28m 42s (- 76m 37s) (21800 27%) 0.1315\n",
      "28m 48s (- 76m 25s) (21900 27%) 0.1096\n",
      "28m 57s (- 76m 19s) (22000 27%) 0.1094\n",
      "29m 5s (- 76m 11s) (22100 27%) 0.1214\n",
      "29m 12s (- 76m 3s) (22200 27%) 0.0943\n",
      "29m 23s (- 76m 2s) (22300 27%) 0.1060\n",
      "29m 32s (- 75m 58s) (22400 28%) 0.1162\n",
      "29m 39s (- 75m 48s) (22500 28%) 0.1158\n",
      "29m 48s (- 75m 41s) (22600 28%) 0.1145\n",
      "29m 55s (- 75m 33s) (22700 28%) 0.0883\n",
      "30m 3s (- 75m 23s) (22800 28%) 0.0869\n",
      "30m 10s (- 75m 14s) (22900 28%) 0.0929\n",
      "30m 18s (- 75m 6s) (23000 28%) 0.0793\n",
      "30m 25s (- 74m 57s) (23100 28%) 0.0849\n",
      "30m 33s (- 74m 48s) (23200 28%) 0.0888\n",
      "30m 39s (- 74m 37s) (23300 29%) 0.0912\n",
      "30m 46s (- 74m 25s) (23400 29%) 0.0912\n",
      "30m 52s (- 74m 12s) (23500 29%) 0.0867\n",
      "30m 57s (- 74m 0s) (23600 29%) 0.1094\n",
      "31m 4s (- 73m 48s) (23700 29%) 0.1042\n",
      "31m 12s (- 73m 41s) (23800 29%) 0.0784\n",
      "31m 24s (- 73m 43s) (23900 29%) 0.0731\n",
      "31m 32s (- 73m 35s) (24000 30%) 0.0646\n",
      "31m 40s (- 73m 27s) (24100 30%) 0.0707\n",
      "31m 47s (- 73m 18s) (24200 30%) 0.0566\n",
      "31m 55s (- 73m 10s) (24300 30%) 0.0791\n",
      "32m 3s (- 73m 3s) (24400 30%) 0.0639\n",
      "32m 10s (- 72m 53s) (24500 30%) 0.0587\n",
      "32m 18s (- 72m 45s) (24600 30%) 0.0756\n",
      "32m 25s (- 72m 36s) (24700 30%) 0.0778\n",
      "32m 33s (- 72m 28s) (24800 31%) 0.0542\n",
      "32m 39s (- 72m 17s) (24900 31%) 0.0523\n",
      "32m 46s (- 72m 7s) (25000 31%) 0.0506\n",
      "32m 55s (- 72m 0s) (25100 31%) 0.0619\n",
      "33m 7s (- 72m 1s) (25200 31%) 0.0484\n",
      "33m 14s (- 71m 52s) (25300 31%) 0.0568\n",
      "33m 21s (- 71m 41s) (25400 31%) 0.0487\n",
      "33m 27s (- 71m 31s) (25500 31%) 0.0428\n",
      "33m 35s (- 71m 21s) (25600 32%) 0.0640\n",
      "33m 42s (- 71m 13s) (25700 32%) 0.0549\n",
      "33m 48s (- 71m 2s) (25800 32%) 0.0609\n",
      "33m 55s (- 70m 52s) (25900 32%) 0.0531\n",
      "34m 4s (- 70m 45s) (26000 32%) 0.0428\n",
      "34m 15s (- 70m 45s) (26100 32%) 0.0463\n",
      "34m 23s (- 70m 37s) (26200 32%) 0.0549\n",
      "34m 30s (- 70m 27s) (26300 32%) 0.0452\n",
      "34m 36s (- 70m 16s) (26400 33%) 0.0511\n",
      "34m 43s (- 70m 6s) (26500 33%) 0.0408\n",
      "34m 51s (- 69m 59s) (26600 33%) 0.0477\n",
      "35m 0s (- 69m 52s) (26700 33%) 0.0378\n",
      "35m 9s (- 69m 47s) (26800 33%) 0.0400\n",
      "35m 18s (- 69m 41s) (26900 33%) 0.0435\n",
      "35m 23s (- 69m 29s) (27000 33%) 0.0436\n",
      "35m 30s (- 69m 18s) (27100 33%) 0.0450\n",
      "35m 36s (- 69m 7s) (27200 34%) 0.0546\n",
      "35m 42s (- 68m 56s) (27300 34%) 0.0419\n",
      "35m 48s (- 68m 45s) (27400 34%) 0.0584\n",
      "35m 57s (- 68m 38s) (27500 34%) 0.0457\n",
      "36m 5s (- 68m 30s) (27600 34%) 0.0384\n",
      "36m 13s (- 68m 23s) (27700 34%) 0.0383\n",
      "36m 19s (- 68m 13s) (27800 34%) 0.0401\n",
      "36m 26s (- 68m 3s) (27900 34%) 0.0366\n",
      "36m 33s (- 67m 54s) (28000 35%) 0.0329\n",
      "36m 40s (- 67m 43s) (28100 35%) 0.0393\n",
      "36m 47s (- 67m 35s) (28200 35%) 0.0356\n",
      "36m 53s (- 67m 24s) (28300 35%) 0.0331\n",
      "37m 0s (- 67m 13s) (28400 35%) 0.0317\n",
      "37m 7s (- 67m 4s) (28500 35%) 0.0351\n",
      "37m 14s (- 66m 55s) (28600 35%) 0.0384\n",
      "37m 21s (- 66m 46s) (28700 35%) 0.0425\n",
      "37m 28s (- 66m 37s) (28800 36%) 0.0343\n",
      "37m 35s (- 66m 27s) (28900 36%) 0.0248\n",
      "37m 42s (- 66m 18s) (29000 36%) 0.0258\n",
      "37m 48s (- 66m 8s) (29100 36%) 0.0435\n",
      "37m 54s (- 65m 57s) (29200 36%) 0.0272\n",
      "38m 1s (- 65m 47s) (29300 36%) 0.0309\n",
      "38m 8s (- 65m 38s) (29400 36%) 0.0273\n",
      "38m 16s (- 65m 30s) (29500 36%) 0.0316\n",
      "38m 23s (- 65m 22s) (29600 37%) 0.0317\n",
      "38m 31s (- 65m 13s) (29700 37%) 0.0246\n",
      "38m 39s (- 65m 6s) (29800 37%) 0.0235\n",
      "38m 47s (- 64m 59s) (29900 37%) 0.0281\n",
      "38m 55s (- 64m 52s) (30000 37%) 0.0356\n",
      "39m 2s (- 64m 43s) (30100 37%) 0.0259\n",
      "39m 8s (- 64m 33s) (30200 37%) 0.0278\n",
      "39m 15s (- 64m 24s) (30300 37%) 0.0252\n",
      "39m 22s (- 64m 15s) (30400 38%) 0.0338\n",
      "39m 30s (- 64m 6s) (30500 38%) 0.0223\n",
      "39m 36s (- 63m 56s) (30600 38%) 0.0281\n",
      "39m 42s (- 63m 46s) (30700 38%) 0.0275\n",
      "39m 49s (- 63m 36s) (30800 38%) 0.0295\n",
      "39m 55s (- 63m 26s) (30900 38%) 0.0246\n",
      "40m 3s (- 63m 18s) (31000 38%) 0.0251\n",
      "40m 10s (- 63m 9s) (31100 38%) 0.0354\n",
      "40m 16s (- 62m 59s) (31200 39%) 0.0198\n",
      "40m 22s (- 62m 48s) (31300 39%) 0.0201\n",
      "40m 29s (- 62m 40s) (31400 39%) 0.0189\n",
      "40m 37s (- 62m 32s) (31500 39%) 0.0232\n",
      "40m 45s (- 62m 25s) (31600 39%) 0.0266\n",
      "40m 52s (- 62m 17s) (31700 39%) 0.0196\n",
      "41m 0s (- 62m 9s) (31800 39%) 0.0233\n",
      "41m 8s (- 62m 1s) (31900 39%) 0.0239\n",
      "41m 16s (- 61m 54s) (32000 40%) 0.0297\n",
      "41m 24s (- 61m 47s) (32100 40%) 0.0250\n",
      "41m 32s (- 61m 40s) (32200 40%) 0.0197\n",
      "41m 40s (- 61m 33s) (32300 40%) 0.0211\n",
      "41m 48s (- 61m 24s) (32400 40%) 0.0226\n",
      "41m 55s (- 61m 16s) (32500 40%) 0.0244\n",
      "42m 3s (- 61m 8s) (32600 40%) 0.0183\n",
      "42m 10s (- 60m 59s) (32700 40%) 0.0191\n",
      "42m 16s (- 60m 49s) (32800 41%) 0.0203\n",
      "42m 24s (- 60m 43s) (32900 41%) 0.0257\n",
      "42m 33s (- 60m 36s) (33000 41%) 0.0178\n",
      "42m 39s (- 60m 27s) (33100 41%) 0.0236\n",
      "42m 47s (- 60m 19s) (33200 41%) 0.0179\n",
      "42m 55s (- 60m 12s) (33300 41%) 0.0154\n",
      "43m 3s (- 60m 4s) (33400 41%) 0.0205\n",
      "43m 14s (- 60m 0s) (33500 41%) 0.0189\n",
      "43m 24s (- 59m 56s) (33600 42%) 0.0165\n",
      "43m 30s (- 59m 46s) (33700 42%) 0.0186\n",
      "43m 38s (- 59m 39s) (33800 42%) 0.0190\n",
      "43m 49s (- 59m 36s) (33900 42%) 0.0155\n",
      "43m 59s (- 59m 30s) (34000 42%) 0.0179\n",
      "44m 6s (- 59m 22s) (34100 42%) 0.0241\n",
      "44m 13s (- 59m 14s) (34200 42%) 0.0172\n",
      "44m 21s (- 59m 6s) (34300 42%) 0.0201\n",
      "44m 29s (- 58m 58s) (34400 43%) 0.0195\n",
      "44m 37s (- 58m 51s) (34500 43%) 0.0174\n",
      "44m 44s (- 58m 42s) (34600 43%) 0.0162\n",
      "44m 53s (- 58m 36s) (34700 43%) 0.0188\n",
      "45m 1s (- 58m 28s) (34800 43%) 0.0182\n",
      "45m 7s (- 58m 19s) (34900 43%) 0.0163\n",
      "45m 16s (- 58m 12s) (35000 43%) 0.0179\n",
      "45m 23s (- 58m 4s) (35100 43%) 0.0157\n",
      "45m 31s (- 57m 55s) (35200 44%) 0.0156\n",
      "45m 39s (- 57m 49s) (35300 44%) 0.0170\n",
      "45m 47s (- 57m 41s) (35400 44%) 0.0164\n",
      "45m 54s (- 57m 33s) (35500 44%) 0.0145\n",
      "46m 2s (- 57m 25s) (35600 44%) 0.0168\n",
      "46m 10s (- 57m 17s) (35700 44%) 0.0149\n",
      "46m 17s (- 57m 9s) (35800 44%) 0.0200\n",
      "46m 24s (- 56m 59s) (35900 44%) 0.0169\n",
      "46m 31s (- 56m 51s) (36000 45%) 0.0119\n",
      "46m 37s (- 56m 42s) (36100 45%) 0.0133\n",
      "46m 45s (- 56m 34s) (36200 45%) 0.0151\n",
      "46m 52s (- 56m 26s) (36300 45%) 0.0142\n",
      "47m 1s (- 56m 19s) (36400 45%) 0.0182\n",
      "47m 7s (- 56m 10s) (36500 45%) 0.0117\n",
      "47m 16s (- 56m 3s) (36600 45%) 0.0130\n",
      "47m 24s (- 55m 56s) (36700 45%) 0.0156\n",
      "47m 32s (- 55m 48s) (36800 46%) 0.0130\n",
      "47m 39s (- 55m 40s) (36900 46%) 0.0108\n",
      "47m 46s (- 55m 30s) (37000 46%) 0.0111\n",
      "47m 53s (- 55m 22s) (37100 46%) 0.0157\n",
      "48m 1s (- 55m 15s) (37200 46%) 0.0135\n",
      "48m 9s (- 55m 7s) (37300 46%) 0.0120\n",
      "48m 16s (- 54m 59s) (37400 46%) 0.0137\n",
      "48m 25s (- 54m 52s) (37500 46%) 0.0115\n",
      "48m 32s (- 54m 44s) (37600 47%) 0.0116\n",
      "48m 40s (- 54m 36s) (37700 47%) 0.0104\n",
      "48m 47s (- 54m 28s) (37800 47%) 0.0126\n",
      "48m 55s (- 54m 21s) (37900 47%) 0.0166\n",
      "49m 4s (- 54m 14s) (38000 47%) 0.0140\n",
      "49m 11s (- 54m 6s) (38100 47%) 0.0175\n",
      "49m 19s (- 53m 57s) (38200 47%) 0.0103\n",
      "49m 26s (- 53m 49s) (38300 47%) 0.0171\n",
      "49m 33s (- 53m 41s) (38400 48%) 0.0157\n",
      "49m 40s (- 53m 32s) (38500 48%) 0.0109\n",
      "49m 48s (- 53m 24s) (38600 48%) 0.0099\n",
      "49m 54s (- 53m 16s) (38700 48%) 0.0137\n",
      "50m 2s (- 53m 7s) (38800 48%) 0.0176\n",
      "50m 9s (- 52m 59s) (38900 48%) 0.0212\n",
      "50m 17s (- 52m 52s) (39000 48%) 0.0105\n",
      "50m 24s (- 52m 43s) (39100 48%) 0.0190\n",
      "50m 30s (- 52m 34s) (39200 49%) 0.0123\n",
      "50m 37s (- 52m 25s) (39300 49%) 0.0115\n",
      "50m 44s (- 52m 17s) (39400 49%) 0.0121\n",
      "50m 51s (- 52m 8s) (39500 49%) 0.0098\n",
      "50m 58s (- 51m 59s) (39600 49%) 0.0126\n",
      "51m 5s (- 51m 51s) (39700 49%) 0.0162\n",
      "51m 12s (- 51m 43s) (39800 49%) 0.0116\n",
      "51m 20s (- 51m 35s) (39900 49%) 0.0117\n",
      "51m 27s (- 51m 27s) (40000 50%) 0.0114\n",
      "51m 34s (- 51m 19s) (40100 50%) 0.0122\n",
      "51m 41s (- 51m 10s) (40200 50%) 0.0179\n",
      "51m 47s (- 51m 1s) (40300 50%) 0.0125\n",
      "51m 54s (- 50m 52s) (40400 50%) 0.0147\n",
      "52m 2s (- 50m 45s) (40500 50%) 0.0167\n",
      "52m 9s (- 50m 36s) (40600 50%) 0.0147\n",
      "52m 16s (- 50m 29s) (40700 50%) 0.0124\n",
      "52m 24s (- 50m 21s) (40800 51%) 0.0112\n",
      "52m 32s (- 50m 13s) (40900 51%) 0.0135\n",
      "52m 39s (- 50m 5s) (41000 51%) 0.0138\n",
      "52m 47s (- 49m 57s) (41100 51%) 0.0111\n",
      "52m 54s (- 49m 49s) (41200 51%) 0.0161\n",
      "53m 4s (- 49m 43s) (41300 51%) 0.0116\n",
      "53m 14s (- 49m 38s) (41400 51%) 0.0107\n",
      "53m 24s (- 49m 32s) (41500 51%) 0.0143\n",
      "53m 33s (- 49m 26s) (41600 52%) 0.0131\n",
      "53m 40s (- 49m 17s) (41700 52%) 0.0196\n",
      "53m 46s (- 49m 8s) (41800 52%) 0.0171\n",
      "53m 51s (- 48m 58s) (41900 52%) 0.0113\n",
      "53m 58s (- 48m 50s) (42000 52%) 0.0104\n",
      "54m 9s (- 48m 45s) (42100 52%) 0.0151\n",
      "54m 17s (- 48m 37s) (42200 52%) 0.0115\n",
      "54m 26s (- 48m 31s) (42300 52%) 0.0111\n",
      "54m 36s (- 48m 25s) (42400 53%) 0.0097\n",
      "54m 43s (- 48m 17s) (42500 53%) 0.0102\n",
      "54m 50s (- 48m 9s) (42600 53%) 0.0093\n",
      "54m 59s (- 48m 1s) (42700 53%) 0.0103\n",
      "55m 7s (- 47m 54s) (42800 53%) 0.0117\n",
      "55m 14s (- 47m 46s) (42900 53%) 0.0141\n",
      "55m 21s (- 47m 37s) (43000 53%) 0.0108\n",
      "55m 31s (- 47m 32s) (43100 53%) 0.0114\n",
      "55m 39s (- 47m 24s) (43200 54%) 0.0118\n",
      "55m 51s (- 47m 20s) (43300 54%) 0.0100\n",
      "55m 59s (- 47m 12s) (43400 54%) 0.0087\n",
      "56m 10s (- 47m 8s) (43500 54%) 0.0122\n",
      "56m 19s (- 47m 1s) (43600 54%) 0.0114\n",
      "56m 26s (- 46m 53s) (43700 54%) 0.0108\n",
      "56m 38s (- 46m 48s) (43800 54%) 0.0168\n",
      "56m 46s (- 46m 41s) (43900 54%) 0.0093\n",
      "56m 55s (- 46m 34s) (44000 55%) 0.0114\n",
      "57m 6s (- 46m 29s) (44100 55%) 0.0181\n",
      "57m 13s (- 46m 21s) (44200 55%) 0.0094\n",
      "57m 21s (- 46m 13s) (44300 55%) 0.0096\n",
      "57m 28s (- 46m 4s) (44400 55%) 0.0080\n",
      "57m 36s (- 45m 57s) (44500 55%) 0.0102\n",
      "57m 47s (- 45m 52s) (44600 55%) 0.0085\n",
      "57m 56s (- 45m 45s) (44700 55%) 0.0118\n",
      "58m 2s (- 45m 36s) (44800 56%) 0.0186\n",
      "58m 9s (- 45m 28s) (44900 56%) 0.0129\n",
      "58m 16s (- 45m 19s) (45000 56%) 0.0097\n",
      "58m 21s (- 45m 9s) (45100 56%) 0.0105\n",
      "58m 27s (- 45m 0s) (45200 56%) 0.0121\n",
      "58m 36s (- 44m 53s) (45300 56%) 0.0170\n",
      "58m 45s (- 44m 46s) (45400 56%) 0.0127\n",
      "58m 55s (- 44m 40s) (45500 56%) 0.0093\n",
      "59m 4s (- 44m 33s) (45600 56%) 0.0126\n",
      "59m 12s (- 44m 26s) (45700 57%) 0.0108\n",
      "59m 21s (- 44m 19s) (45800 57%) 0.0158\n",
      "59m 32s (- 44m 13s) (45900 57%) 0.0126\n",
      "59m 39s (- 44m 5s) (46000 57%) 0.0125\n",
      "59m 48s (- 43m 58s) (46100 57%) 0.0097\n",
      "59m 58s (- 43m 52s) (46200 57%) 0.0075\n",
      "60m 7s (- 43m 45s) (46300 57%) 0.0094\n",
      "60m 14s (- 43m 37s) (46400 57%) 0.0079\n",
      "60m 20s (- 43m 28s) (46500 58%) 0.0093\n",
      "60m 30s (- 43m 21s) (46600 58%) 0.0100\n",
      "60m 36s (- 43m 12s) (46700 58%) 0.0080\n",
      "60m 42s (- 43m 4s) (46800 58%) 0.0110\n",
      "60m 50s (- 42m 56s) (46900 58%) 0.0080\n",
      "61m 0s (- 42m 49s) (47000 58%) 0.0136\n",
      "61m 7s (- 42m 42s) (47100 58%) 0.0133\n",
      "61m 16s (- 42m 34s) (47200 59%) 0.0097\n",
      "61m 27s (- 42m 29s) (47300 59%) 0.0077\n",
      "61m 35s (- 42m 21s) (47400 59%) 0.0093\n",
      "61m 46s (- 42m 15s) (47500 59%) 0.0107\n",
      "61m 56s (- 42m 9s) (47600 59%) 0.0077\n",
      "62m 5s (- 42m 2s) (47700 59%) 0.0094\n",
      "62m 16s (- 41m 57s) (47800 59%) 0.0078\n",
      "62m 25s (- 41m 49s) (47900 59%) 0.0136\n",
      "62m 34s (- 41m 43s) (48000 60%) 0.0074\n",
      "62m 44s (- 41m 36s) (48100 60%) 0.0092\n",
      "62m 52s (- 41m 28s) (48200 60%) 0.0113\n",
      "63m 1s (- 41m 21s) (48300 60%) 0.0084\n",
      "63m 8s (- 41m 13s) (48400 60%) 0.0072\n",
      "63m 14s (- 41m 4s) (48500 60%) 0.0082\n",
      "63m 20s (- 40m 55s) (48600 60%) 0.0088\n",
      "63m 28s (- 40m 47s) (48700 60%) 0.0089\n",
      "63m 35s (- 40m 39s) (48800 61%) 0.0076\n",
      "63m 42s (- 40m 30s) (48900 61%) 0.0085\n",
      "63m 48s (- 40m 22s) (49000 61%) 0.0084\n",
      "63m 55s (- 40m 13s) (49100 61%) 0.0111\n",
      "64m 2s (- 40m 5s) (49200 61%) 0.0080\n",
      "64m 10s (- 39m 57s) (49300 61%) 0.0069\n",
      "64m 19s (- 39m 50s) (49400 61%) 0.0069\n",
      "64m 26s (- 39m 42s) (49500 61%) 0.0072\n",
      "64m 33s (- 39m 34s) (49600 62%) 0.0112\n",
      "64m 42s (- 39m 26s) (49700 62%) 0.0101\n",
      "64m 51s (- 39m 19s) (49800 62%) 0.0108\n",
      "65m 0s (- 39m 12s) (49900 62%) 0.0090\n",
      "65m 8s (- 39m 4s) (50000 62%) 0.0100\n",
      "65m 18s (- 38m 58s) (50100 62%) 0.0066\n",
      "65m 27s (- 38m 51s) (50200 62%) 0.0143\n",
      "65m 35s (- 38m 43s) (50300 62%) 0.0065\n",
      "65m 42s (- 38m 35s) (50400 63%) 0.0107\n",
      "65m 50s (- 38m 27s) (50500 63%) 0.0070\n",
      "65m 57s (- 38m 19s) (50600 63%) 0.0101\n",
      "66m 5s (- 38m 11s) (50700 63%) 0.0072\n",
      "66m 13s (- 38m 3s) (50800 63%) 0.0075\n",
      "66m 22s (- 37m 56s) (50900 63%) 0.0095\n",
      "66m 31s (- 37m 49s) (51000 63%) 0.0099\n",
      "66m 40s (- 37m 42s) (51100 63%) 0.0111\n",
      "66m 50s (- 37m 35s) (51200 64%) 0.0070\n",
      "66m 58s (- 37m 28s) (51300 64%) 0.0097\n",
      "67m 6s (- 37m 20s) (51400 64%) 0.0079\n",
      "67m 16s (- 37m 13s) (51500 64%) 0.0066\n",
      "67m 27s (- 37m 7s) (51600 64%) 0.0067\n",
      "67m 36s (- 37m 0s) (51700 64%) 0.0076\n",
      "67m 45s (- 36m 53s) (51800 64%) 0.0116\n",
      "67m 52s (- 36m 45s) (51900 64%) 0.0099\n",
      "68m 1s (- 36m 37s) (52000 65%) 0.0058\n",
      "68m 9s (- 36m 29s) (52100 65%) 0.0090\n",
      "68m 16s (- 36m 21s) (52200 65%) 0.0086\n",
      "68m 23s (- 36m 13s) (52300 65%) 0.0063\n",
      "68m 29s (- 36m 4s) (52400 65%) 0.0117\n",
      "68m 37s (- 35m 56s) (52500 65%) 0.0087\n",
      "68m 46s (- 35m 49s) (52600 65%) 0.0068\n",
      "68m 52s (- 35m 40s) (52700 65%) 0.0130\n",
      "68m 58s (- 35m 32s) (52800 66%) 0.0107\n",
      "69m 5s (- 35m 23s) (52900 66%) 0.0060\n",
      "69m 13s (- 35m 16s) (53000 66%) 0.0094\n",
      "69m 21s (- 35m 8s) (53100 66%) 0.0095\n",
      "69m 30s (- 35m 0s) (53200 66%) 0.0089\n",
      "69m 38s (- 34m 52s) (53300 66%) 0.0062\n",
      "69m 45s (- 34m 45s) (53400 66%) 0.0059\n",
      "69m 56s (- 34m 38s) (53500 66%) 0.0063\n",
      "70m 6s (- 34m 31s) (53600 67%) 0.0084\n",
      "70m 11s (- 34m 22s) (53700 67%) 0.0087\n",
      "70m 19s (- 34m 14s) (53800 67%) 0.0098\n",
      "70m 27s (- 34m 7s) (53900 67%) 0.0060\n",
      "70m 33s (- 33m 58s) (54000 67%) 0.0089\n",
      "70m 40s (- 33m 49s) (54100 67%) 0.0065\n",
      "70m 46s (- 33m 41s) (54200 67%) 0.0078\n",
      "70m 54s (- 33m 33s) (54300 67%) 0.0083\n",
      "HyperBorad agent failed to append: record is already deleted!\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/liwb/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2889: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "encoder1 = EncoderRNN(inputlang.n_words, hidden_size)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, outputlang.n_words,\n",
    "                               1, dropout_p=0.1)\n",
    "\n",
    "if use_cuda:\n",
    "    encoder1 = encoder1.cuda()\n",
    "    attn_decoder1 = attn_decoder1.cuda()\n",
    "\n",
    "trainIters(encoder1, attn_decoder1, inputlang, outputlang, 80000, print_every=100, save_model_every=2000)\n",
    "\n",
    "######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "######################################################################\n",
    "# Evaluation\n",
    "# ==========\n",
    "#\n",
    "# Evaluation is mostly the same as training, but there are no targets so\n",
    "# we simply feed the decoder's predictions back to itself for each step.\n",
    "# Every time it predicts a word we add it to the output string, and if it\n",
    "# predicts the EOS token we stop there. We also store the decoder's\n",
    "# attention outputs for display later.\n",
    "#\n",
    "\n",
    "def evaluate(encoder, decoder, sentence, input_lang, output_lang, max_length=MAX_LENGTH):\n",
    "    input_variable = variableFromSentence(input_lang, sentence)\n",
    "    input_length = input_variable.size()[0]\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_outputs = Variable(torch.zeros(max_length, encoder.hidden_size))\n",
    "    encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_variable[ei],\n",
    "                                                 encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_outputs[ei] + encoder_output[0][0]\n",
    "\n",
    "    decoder_input = Variable(torch.LongTensor([[SOS_token]]))  # SOS\n",
    "    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    decoded_words = []\n",
    "    decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "    for di in range(max_length):#注意这里跟训练的时候不一样，训练的时候用的是target_length。这里因为要输出句子，而代码限定了句子的最大长度。\n",
    "        decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "            decoder_input, decoder_hidden, encoder_output, encoder_outputs)\n",
    "        decoder_attentions[di] = decoder_attention.data\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        ni = topi[0][0]\n",
    "        if ni == EOS_token:\n",
    "            decoded_words.append('<EOS>')  #检测到结束符就停止\n",
    "            break\n",
    "        else:\n",
    "            decoded_words.append(output_lang.index2word[ni])\n",
    "\n",
    "        decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "        decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "    return decoded_words, decoder_attentions[:di + 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# We can evaluate random sentences from the training set and print out the\n",
    "# input, target, and output to make some subjective quality judgements:\n",
    "#\n",
    "\n",
    "def evaluateRandomly(encoder, decoder, inputlang, outputlang, n=100):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0].decode('utf-8'))\n",
    "        print('=', pair[1].decode('gb2312'))\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0].decode('utf-8'),inputlang, outputlang)\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> a slight form of turbulence .\n",
      "=  一 小 团 空 气 流 。 \n",
      "<  一 小 团 空 气 流 。  <EOS>\n",
      "\n",
      "> a letter for you madam .\n",
      "=  一 封 你 的 信 ， 女 士 。 \n",
      "<  一 封 你 的 信 ， 女 士 。  <EOS>\n",
      "\n",
      "> i want tact ops on site within the hour .\n",
      "=  一 小 时 内 建 好 战 略 指 挥 部 。 \n",
      "<  一 小 时 内 建 好 战 略 指 挥 部 。  <EOS>\n",
      "\n",
      "> one couple was white the other black .\n",
      "=  一 对 是 白 人 一 对 是 黑 人 。 \n",
      "<  一 对 是 白 人 一 对 是 黑 人 。  <EOS>\n",
      "\n",
      "> i ll have your warrant in an hour .\n",
      "=  一 小 时 之 内 给 你 搜 查 令 。 \n",
      "<  一 小 时 之 内 给 你 搜 查 令 。  <EOS>\n",
      "\n",
      "> four inches of rain in under an hour .\n",
      "=  一 小 时 之 内 下 了 4 寸 的 雨 量 \n",
      "<  一 小 时 之 内 下 了 4 寸 的 雨 量  <EOS>\n",
      "\n",
      "> couple comes by .\n",
      "=  一 对 情 侣 经 过 。 \n",
      "<  一 对 情 侣 经 过 。  <EOS>\n",
      "\n",
      "> your father passed away an hour ago .\n",
      "=  一 小 时 前 你 父 亲 过 世 了 。 \n",
      "<  一 小 时 前 你 父 亲 过 世 了 。  <EOS>\n",
      "\n",
      "> some newlyweds was killed . some more of it he said .\n",
      "=  一 对 新 婚 夫 妇 被 杀 了 。 他 说 还 有 更 多 会 被 杀 。 \n",
      "<  一 对 新 婚 夫 妇 被 杀 了 。 他 说 还 有 更 多 会 被 杀 。  <EOS>\n",
      "\n",
      "> a piece of cake isn t gonna hurt those .\n",
      "=  一 小 块 蛋 糕 没 问 题 的 。 \n",
      "<  一 小 块 蛋 糕 没 问 题 的 。  <EOS>\n",
      "\n",
      "> a ietter to myseif to be sent in a year .\n",
      "=  一 封 寄 给 我 自 己 的 信 将 在 一 年 内 寄 出 。 \n",
      "<  一 封 寄 给 我 自 己 的 信 将 在 一 年 内 寄 出 。  <EOS>\n",
      "\n",
      "> a few people in their party\n",
      "=  一 小 撮 人 指 定 的 \n",
      "<  一 小 撮 人 指 定 的  <EOS>\n",
      "\n",
      "> male and female male with a dislocated leg .\n",
      "=  一 对 夫 妇 前 来 就 诊 男 的 腿 部 脱 臼 。 \n",
      "<  一 对 夫 妇 前 来 就 诊 男 的 腿 部 脱 臼 。  <EOS>\n",
      "\n",
      "> an hour and a quarter ? yes\n",
      "=  一 小 时 1 5 分 钟 ？ 是 的 \n",
      "<  一 小 时 1 5 分 钟 ？ 是 的  <EOS>\n",
      "\n",
      "> well she wasn t an hour ago .\n",
      "=  一 小 时 前 她 可 不 是 。 \n",
      "<  一 小 时 前 她 可 不 是 。  <EOS>\n",
      "\n",
      "> within an hour i ll have you hounded by reporters .\n",
      "=  一 小 时 内 ， 你 将 会 被 记 者 包 围 。 \n",
      "<  一 小 时 内 ， 你 将 会 被 记 者 包 围 。  <EOS>\n",
      "\n",
      "> a honeymooning couple .\n",
      "=  一 对 正 在 度 蜜 月 的 夫 妻 。 \n",
      "<  一 对 正 在 度 蜜 月 的 夫 妻 。  <EOS>\n",
      "\n",
      "> couple that couldn t be more mismatched . i m gonna guess that this guy s loaded .\n",
      "=  一 对 史 上 最 不 般 配 的 情 侣 。 我 猜 这 个 男 人 很 富 有 。 \n",
      "<  一 对 史 上 最 不 般 配 的 情 侣 。 我 猜 这 个 男 人 很 富 有 。  <EOS>\n",
      "\n",
      "> it won t take long .\n",
      "=  一 小 会 儿 就 好 。 \n",
      "<  一 小 会 儿 就 好 。  <EOS>\n",
      "\n",
      "> a married couple who like puzzles .\n",
      "=  一 对 喜 欢 字 谜 的 夫 妇 。 \n",
      "<  一 对 喜 欢 字 谜 的 夫 妇 。  <EOS>\n",
      "\n",
      "> at . an hour and all those nice benefits\n",
      "=  一 小 时 8 . 7 5 美 元 还 有 福 利 待 遇 。 \n",
      "<  一 小 时 8 . 7 5 美 元 还 有 福 利 待 遇 。  <EOS>\n",
      "\n",
      "> well for started an hour ago you say maybe we should kill someone and then someone gets killed .\n",
      "=  一 小 时 以 前 你 说 或 许 我 们 应 该 杀 死 个 人 然 后 就 有 人 死 了 。 \n",
      "<  一 小 时 以 前 你 说 或 许 我 们 应 该 杀 死 个 人 然 后 就 有 人 死 了 。  <EOS>\n",
      "\n",
      "> a little tiny piece of his brain . seemed a waste .\n",
      "=  一 小 块 脑 子 都 没 用 了 。 \n",
      "<  一 小 块 脑 子 都 没 用 了 。  <EOS>\n",
      "\n",
      "> you won t see again in a hour maybe .\n",
      "=  一 小 时 之 后 ， 你 的 视 觉 会 恢 复 ， 也 许 。 \n",
      "<  一 小 时 之 后 ， 你 的 视 觉 会 恢 复 ， 也 许 。  <EOS>\n",
      "\n",
      "> got a conference call with him in an hour\n",
      "=  一 小 时 之 后 跟 他 电 话 会 谈 \n",
      "<  一 小 时 之 后 跟 他 电 话 会 谈  <EOS>\n",
      "\n",
      "> she saw andie and charlie\n",
      "=  一 小 时 前 她   看 见 安 迪 和 查 利 \n",
      "<  一 小 时 前 她   看 见 安 迪 和 查 利  <EOS>\n",
      "\n",
      "> over a sip of water from a radioactive puddle \n",
      "=  一 小 口 受 到 核 污 染 的 水 而 打 得 你 死 我 活 时 ， \n",
      "<  一 小 口 受 到 核 污 染 的 水 而 打 得 你 死 我 活 时 ，  <EOS>\n",
      "\n",
      "> let s try in an hour okay .\n",
      "=  一 小 时 之 内 吧 ， 好 的 。 \n",
      "<  一 小 时 之 内 吧 ， 好 的 。  <EOS>\n",
      "\n",
      "> i dropped henry at your office an hour ago .\n",
      "=  一 小 时 前 我 开 车 送 海 瑞 去 了 你 办 公 室 。 \n",
      "<  一 小 时 前 我 开 车 送 海 瑞 去 了 你 办 公 室 。  <EOS>\n",
      "\n",
      "> had this fixed an hour ago . i m just double checking the riggings .\n",
      "=  一 小 时 前 就 修 好 了 。 我 只 想 再 检 查 一 下 。 \n",
      "<  一 小 时 前 就 修 好 了 。 我 只 想 再 检 查 一 下 。  <EOS>\n",
      "\n",
      "> a couple of murders ?\n",
      "=  一 对 杀 人 犯 ？ \n",
      "<  一 对 杀 人 犯 ？  <EOS>\n",
      "\n",
      "> got a paternal match .\n",
      "=  一 对 父 女 关 系 。 \n",
      "<  一 对 父 女 关 系 。  <EOS>\n",
      "\n",
      "> i ordered a state of emergency an hour ago .\n",
      "=  一 小 时 前 我 就 下 令 进 入 紧 急 状 态 了 。 \n",
      "<  一 小 时 前 我 就 下 令 进 入 紧 急 状 态 了 。  <EOS>\n",
      "\n",
      "> a word of caution .\n",
      "=  一 封 谴 责 信 。 \n",
      "<  一 封 谴 责 信 。  <EOS>\n",
      "\n",
      ">  cent an hour .\n",
      "=  一 小 时 三 毛 五 。 \n",
      "<  一 小 时 三 毛 五 。  <EOS>\n",
      "\n",
      "> within the hour .\n",
      "=  一 小 时   。 \n",
      "<  一 小 时   。  <EOS>\n",
      "\n",
      "> a letter ? why do you need it ?\n",
      "=  一 封 信 ？ 你 要 来 干 嘛 ？ \n",
      "<  一 封 信 ？ 你 要 来 干 嘛 ？  <EOS>\n",
      "\n",
      "> technically so i could give you a tour an hour ago .\n",
      "=  一 小 时 前 我 就 该 带 你 到 处 转 转 了 。 \n",
      "<  一 小 时 前 我 就 该 带 你 到 处 转 转 了 。  <EOS>\n",
      "\n",
      "> last call came in an hour ago and the man on the end said\n",
      "=  一 小 时 前 ， 我 们 接 到 电 话 电 话 那 边 的 人 说 \n",
      "<  一 小 时 前 ， 我 们 接 到 电 话 电 话 那 边 的 人 说  <EOS>\n",
      "\n",
      "> i ll have my helicopter ready for you in one hour .\n",
      "=  一 小 时 内 我 准 备 好 直 升 机 。 \n",
      "<  一 小 时 内 我 准 备 好 直 升 机 。  <EOS>\n",
      "\n",
      "> one for you and one for him .\n",
      "=  一 封 给 你 另 一 封 给 他 。 \n",
      "<  一 封 给 你 另 一 封 给 他 。  <EOS>\n",
      "\n",
      "> within the hour the governor has called in all sources for a manhunt .\n",
      "=  一 小 时 内 ， 政 府 已 发 动 一 切 力 量 进 行 搜 索 。 \n",
      "<  一 小 时 内 ， 政 府 已 发 动 一 切 力 量 进 行 搜 索 。  <EOS>\n",
      "\n",
      "> one for my stepmother two for my stepsisters .\n",
      "=  一 封 是 给 继 母 的 ， 两 封 是 给 姐 姐 的 。 \n",
      "<  一 封 是 给 继 母 的 ， 两 封 是 给 姐 姐 的 。  <EOS>\n",
      "\n",
      "> an hour ago i was here asleep \n",
      "=  一 小 时 前 我 在 这 里 睡 觉 ， \n",
      "<  一 小 时 前 我 在 这 里 睡 觉 ，  <EOS>\n",
      "\n",
      "> your father passed away an hour ago .\n",
      "=  一 小 时 前 你 父 亲 过 世 了 。 \n",
      "<  一 小 时 前 你 父 亲 过 世 了 。  <EOS>\n",
      "\n",
      "> bought my plane ticket an hour ago .\n",
      "=  一 小 时 前 买 的 票 。 \n",
      "<  一 小 时 前 买 的 票 。  <EOS>\n",
      "\n",
      "> you have a doctor s appointment in an hour .\n",
      "=  一 小 时 内 你 去 预 约 医 生 。 \n",
      "<  一 小 时 内 你 去 预 约 医 生 。  <EOS>\n",
      "\n",
      "> not one hour from this very door we were accosted by .\n",
      "=  一 小 时 前 在 大 门 那 边 。 \n",
      "<  一 小 时 前 在 大 门 那 边 。  <EOS>\n",
      "\n",
      "> eternity\n",
      "=  一 小 会 儿 就 是 永 恒 \n",
      "<  一 小 会 儿 就 是 永 恒  <EOS>\n",
      "\n",
      "> within the hour .\n",
      "=  一 小 时   。 \n",
      "<  一 小 时   。  <EOS>\n",
      "\n",
      "> and i ll be back in an hour with another update .\n",
      "=  一 小 时 内 我 会 再 来 汇 报 。 \n",
      "<  一 小 时 内 我 会 再 来 汇 报 。  <EOS>\n",
      "\n",
      "> for one small mite one small mite .\n",
      "=  一 小 口 ，   一 小 口 。 \n",
      "<  一 小 口 ，   一 小 口 。  <EOS>\n",
      "\n",
      "> a little hit ?\n",
      "=  一 小 口 ？ \n",
      "<  一 小 口 ？  <EOS>\n",
      "\n",
      "> an hour ago the man on your tablets\n",
      "=  一 小 时 前 ， 平 板 上 的 此 人 \n",
      "<  一 小 时 前 ， 平 板 上 的 此 人  <EOS>\n",
      "\n",
      "> the chancellor s unreachable for another hour .\n",
      "=  一 小 时 内 恐 怕 是 联 系 不 上 。 \n",
      "<  一 小 时 内 恐 怕 是 联 系 不 上 。  <EOS>\n",
      "\n",
      "> a couple holding hands \n",
      "=  一 对 牵 手 的 情 侣 ， \n",
      "<  一 对 牵 手 的 情 侣 ，  <EOS>\n",
      "\n",
      "> an obscene letter .\n",
      "=  一 封 很 猥 琐 的 信 。 \n",
      "<  一 封 很 猥 琐 的 信 。  <EOS>\n",
      "\n",
      "> siblings . adult siblings .\n",
      "=  一 对 兄 妹 。 一 对 成 年 的 兄 妹 。 \n",
      "<  一 对 兄 妹 。 一 对 成 年 的 兄 妹 。  <EOS>\n",
      "\n",
      "> a couple holding hands an old man in a ball cap and a blue car parked .\n",
      "=  一 对 情 侣 牵 着 手 ， 一 个 老 人 带 着 橄 榄 球 帽 ， 还 停 车 一 辆 蓝 色 的 车 。 \n",
      "<  一 对 情 侣 牵 着 手 ， 一 个 老 人 带 着 橄 榄 球 帽 ， 还 停 车 一 辆 蓝 色 的 车 。\n",
      "\n",
      "> the dynamic duo will seal the deal tonight .\n",
      "=  一 对 俪 人 今 天 晚 上 就 会 敲 定 了 。 \n",
      "<  一 对 俪 人 今 天 晚 上 就 会 敲 定 了 。  <EOS>\n",
      "\n",
      "> a pair of snow leopards is stalking you \n",
      "=  一 对 雪 豹 正 尾 随 着 你 ， \n",
      "<  一 对 雪 豹 正 尾 随 着 你 ，  <EOS>\n",
      "\n",
      "> even a single unaccounted for piece of the endoskeleton\n",
      "=  一 小 块 不 知 名 的 骨 骼 甚 至 \n",
      "<  一 小 块 不 知 名 的 骨 骼 甚 至  <EOS>\n",
      "\n",
      "> we put them down like an hour ago .\n",
      "=  一 小 时 前 我 们 安 顿 他 们 睡 下 了 。 \n",
      "<  一 小 时 前 我 们 安 顿 他 们 睡 下 了 。  <EOS>\n",
      "\n",
      "> one small bite to drag her down\n",
      "=  一 小 口 她 就 会 倒 下 \n",
      "<  一 小 口 她 就 会 倒 下  <EOS>\n",
      "\n",
      "> a small disciplined militia can not only hold out against a larger force but drive it back\n",
      "=  一 小 支 训 练 有 素 的 民 兵 不 仅 可 以 抵 挡 强 大 的 军 队 还 可 以 击 退 它 \n",
      "<  一 小 支 训 练 有 素 的 民 兵 不 仅 可 以 抵 挡 强 大 的 军 队 还 可 以 击 退 它  <EOS>\n",
      "\n",
      "> a pair of crows had come to nest on our roof as if they had come for lhamo .\n",
      "=  一 对 乌 鸦 飞 到 我 们 屋 顶 上 的 巢 里 ， 它 们 好 像 专 门 为 拉 木 而 来 的 。 \n",
      "<  一 对 乌 鸦 飞 到 我 们 屋 顶 上 的 巢 里 ， 它 们 好 像 专 门 为 拉 木 而 来 的 。 \n",
      "\n",
      "> couple comes by .\n",
      "=  一 对 情 侣 经 过 。 \n",
      "<  一 对 情 侣 经 过 。  <EOS>\n",
      "\n",
      "> two lovers . intimate yet visceral .\n",
      "=  一 对 真 心 相 爱 的 恋 人 。 \n",
      "<  一 对 真 心 相 爱 的 恋 人 。  <EOS>\n",
      "\n",
      "> the herald just finished it about an hour ago .\n",
      "=  一 小 时 前 刚 弄 好 。   \n",
      "<  一 小 时 前 刚 弄 好 。    <EOS>\n",
      "\n",
      "> get it on the air in the next hour . you got it .\n",
      "=  一 小 时 内 你 去 办 好 。 \n",
      "<  一 小 时 内 你 去 办 好 。  <EOS>\n",
      "\n",
      "> a brother and sister lived here .\n",
      "=  一 对 姐 弟 曾 住 在 这 儿 。 \n",
      "<  一 对 姐 弟 曾 住 在 这 儿 。  <EOS>\n",
      "\n",
      "> a pair of red crowned cranes have staked out their nesting territory\n",
      "=  一 对 丹 顶 鹤 正 监 视 着 它 们 的 筑 巢 领 地 \n",
      "<  一 对 丹 顶 鹤 正 监 视 着 它 们 的 筑 巢 领 地  <EOS>\n",
      "\n",
      "> life never seems as grim after a couple of fried eggs .\n",
      "=  一 对 煎 蛋 从 来 会 让 生 活 变 得 美 好 点 。 \n",
      "<  一 对 煎 蛋 从 来 会 让 生 活 变 得 美 好 点 。  <EOS>\n",
      "\n",
      "> it took an hour for someone to finally notice\n",
      "=  一 小 时 之 后 才 有 人 发 现 \n",
      "<  一 小 时 之 后 才 有 人 发 现  <EOS>\n",
      "\n",
      "> pair of contact lenses . and fifteen quid .\n",
      "=  一 对 隐 形 眼 镜 。 1 5 镑 。 \n",
      "<  一 对 隐 形 眼 镜 。 1 5 镑 。  <EOS>\n",
      "\n",
      "> i was rescued in an hour .\n",
      "=  一 小 时 以 后 我 获 救 了 。 \n",
      "<  一 小 时 以 后 我 获 救 了 。  <EOS>\n",
      "\n",
      "> a little hit ?\n",
      "=  一 小 口 ？ \n",
      "<  一 小 口 ？  <EOS>\n",
      "\n",
      "> even a couple of the neighbors tried helping me .\n",
      "=  一 对 邻 居 夫 妇 都 来 帮 我 找 了 。 \n",
      "<  一 对 邻 居 夫 妇 都 来 帮 我 找 了 。  <EOS>\n",
      "\n",
      "> an hour ago the man on your tablets\n",
      "=  一 小 时 前 ， 平 板 上 的 此 人 \n",
      "<  一 小 时 前 ， 平 板 上 的 此 人  <EOS>\n",
      "\n",
      "> an email went to people\n",
      "=  一 封 电 邮 发 给 了 1 7 . 8 万 人 \n",
      "<  一 封 电 邮 发 给 了 1 7 . 8 万 人  <EOS>\n",
      "\n",
      "> a letter for you from hungary\n",
      "=  一 封 你 的 信 ， 从 匈 牙 利 寄 来 的 \n",
      "<  一 封 你 的 信 ， 从 匈 牙 利 寄 来 的  <EOS>\n",
      "\n",
      "> into a convincing married couple .\n",
      "=  一 对 令 人 信 服 的 已 婚 夫 妇 。 \n",
      "<  一 对 令 人 信 服 的 已 婚 夫 妇 。  <EOS>\n",
      "\n",
      "> a couple of lights and some chair fabric\n",
      "=  一 对 灯 泡 和 沙 发 面 料 \n",
      "<  一 对 灯 泡 和 沙 发 面 料  <EOS>\n",
      "\n",
      "> i ll bill you for an hour .\n",
      "=  一 小 时 内 我 会 给 你 账 单 。 \n",
      "<  一 小 时 内 我 会 给 你 账 单 。  <EOS>\n",
      "\n",
      "> a normal couple .\n",
      "=  一 对 正 常 的 情 侣 。 \n",
      "<  一 对 正 常 的 情 侣 。  <EOS>\n",
      "\n",
      "> outside memphis hours ago .\n",
      "=  一 小 时 前 在 孟 斐 斯 近 郊 被 发 现 死 于 自 杀 。 \n",
      "<  一 小 时 前 在 孟 斐 斯 近 郊 被 发 现 死 于 自 杀 。  <EOS>\n",
      "\n",
      "> a breeding pair .\n",
      "=  一 对 能 繁 殖 的 浣 熊 。 \n",
      "<  一 对 能 繁 殖 的 浣 熊 。  <EOS>\n",
      "\n",
      "> i had a little drink about an hour ago\n",
      "=  一 小 时 前 喝 的 酒 \n",
      "<  一 小 时 前 喝 的 酒  <EOS>\n",
      "\n",
      "> owned by a french couple .\n",
      "=  一 对 法 国 夫 妻 所 拥 有 。 \n",
      "<  一 对 法 国 夫 妻 所 拥 有 。  <EOS>\n",
      "\n",
      "> a simple letter seems so pointless .\n",
      "=  一 封 简 单 的 信 似 乎 没 什 么 意 义 了 。 \n",
      "<  一 封 简 单 的 信 似 乎 没 什 么 意 义 了 。  <EOS>\n",
      "\n",
      "> couple of people ? no a couple of antelope .\n",
      "=  一 对 夫 妇 ？ 不 ， 是 一 对 羚 羊 。 \n",
      "<  一 对 夫 妇 ？ 不 ， 是 一 对 羚 羊 。  <EOS>\n",
      "\n",
      "> you have lunch in an hour with people\n",
      "=  一 小 时 之 后 你 就 可 以 和 别 人 吃 饭 \n",
      "<  一 小 时 之 后 你 就 可 以 和 别 人 吃 饭  <EOS>\n",
      "\n",
      "> you must copulate within the hour .\n",
      "=  一 小 时 之 内 你 必 须 享 尽 云 雨 之 欢 。 \n",
      "<  一 小 时 之 内 你 必 须 享 尽 云 雨 之 欢 。  <EOS>\n",
      "\n",
      "> a pair of top of the line breast implants .\n",
      "=  一 对 非 比 寻 常 的 胸 部 整 形 。 \n",
      "<  一 对 非 比 寻 常 的 胸 部 整 形 。  <EOS>\n",
      "\n",
      "> did manage to uh hook a good sized trout about an hour ago .\n",
      "=  一 小 时 前 成 功 钩 起 来 一 条 不 小 的 鲑 鱼 。 \n",
      "<  一 小 时 前 成 功 钩 起 来 一 条 不 小 的 鲑 鱼 。  <EOS>\n",
      "\n",
      "> like an hour ago ? at the library .\n",
      "=  一 小 时 前 , 在 图 书 馆 。 \n",
      "<  一 小 时 前 , 在 图 书 馆 。  <EOS>\n",
      "\n",
      "> it was hard to watch an hour ago .\n",
      "=  一 小 时 前 她 就 知 道 了 。 \n",
      "<  一 小 时 前 她 就 知 道 了 。  <EOS>\n",
      "\n",
      "> it s a complaint form larry .\n",
      "=  一 封 投 诉 信 ， 拉 里 。 \n",
      "<  一 封 投 诉 信 ， 拉 里 。  <EOS>\n",
      "\n",
      "> one tiny fragment of an ice age giant\n",
      "=  一 小 块 冰 河 时 期 史 前 巨 兽 的 残 骸 \n",
      "<  一 小 块 冰 河 时 期 史 前 巨 兽 的 残 骸  <EOS>\n",
      "\n",
      "> a few people in their party\n",
      "=  一 小 撮 人 指 定 的 \n",
      "<  一 小 撮 人 指 定 的  <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder1,attn_decoder1,inputlang,outputlang)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
